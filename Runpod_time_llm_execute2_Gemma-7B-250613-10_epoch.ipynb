{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1UItzpXIgm2J",
    "outputId": "9f88d41b-816b-4845-95ec-8942047549c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Time-LLM' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/KimMeen/Time-LLM.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FkhiSKcUilna",
    "outputId": "b9e6307a-8802-4d61-f959-8c5a37958a7e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.9.0)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
      "Collecting tqdm (from gdown)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: tqdm, PySocks, gdown\n",
      "Successfully installed PySocks-1.7.1 gdown-5.2.0 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1NF7VEefXCmXuWNbnNe858WvQAkJ_7wuP\n",
      "From (redirected): https://drive.google.com/uc?id=1NF7VEefXCmXuWNbnNe858WvQAkJ_7wuP&confirm=t&uuid=fd06f087-5520-44f8-a716-cbde530b6886\n",
      "To: /workspace/dataset.zip\n",
      "100%|████████████████████████████████████████| 355M/355M [00:05<00:00, 63.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!gdown --id 1NF7VEefXCmXuWNbnNe858WvQAkJ_7wuP -O dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]      \u001b[0m\n",
      "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]3m\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]        \u001b[0m\u001b[33m\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2984 kB]\n",
      "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.8 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1246 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4476 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
      "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB][0m\u001b[33m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1765 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]    \u001b[0m\u001b[33m\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3295 kB]\u001b[33m\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1553 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4630 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]33m\u001b[33m\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Fetched 40.6 MB in 3s (15.1 MB/s)[33m                      \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "143 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"
     ]
    }
   ],
   "source": [
    "!apt update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Suggested packages:\n",
      "  zip\n",
      "The following NEW packages will be installed:\n",
      "  unzip\n",
      "0 upgraded, 1 newly installed, 0 to remove and 143 not upgraded.\n",
      "Need to get 175 kB of archives.\n",
      "After this operation, 386 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 unzip amd64 6.0-26ubuntu3.2 [175 kB]\n",
      "Fetched 175 kB in 0s (545 kB/s)[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package unzip.\n",
      "(Reading database ... 20729 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-26ubuntu3.2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking unzip (6.0-26ubuntu3.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up unzip (6.0-26ubuntu3.2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt install -y unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zRNo_MINj6XQ"
   },
   "outputs": [],
   "source": [
    "!unzip -q dataset.zip -d ./dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iAYKTZeNi2Vp"
   },
   "outputs": [],
   "source": [
    "!mkdir -p /workspace/Time-LLM/dataset/ETT-small\n",
    "!cp /workspace/dataset/dataset/ETT-small/ETTh1.csv /workspace/Time-LLM/dataset/ETT-small/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4Q_YSRcxioaH",
    "outputId": "834fc2c4-fe64-41a5-f7b4-af9f8521187b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.2.2 (from -r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting accelerate==0.28.0 (from -r /workspace/Time-LLM/requirements.txt (line 2))\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting einops==0.7.0 (from -r /workspace/Time-LLM/requirements.txt (line 3))\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting matplotlib==3.7.0 (from -r /workspace/Time-LLM/requirements.txt (line 4))\n",
      "  Downloading matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting numpy==1.23.5 (from -r /workspace/Time-LLM/requirements.txt (line 5))\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting pandas==1.5.3 (from -r /workspace/Time-LLM/requirements.txt (line 6))\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scikit_learn==1.2.2 (from -r /workspace/Time-LLM/requirements.txt (line 7))\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy==1.12.0 (from -r /workspace/Time-LLM/requirements.txt (line 8))\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm==4.65.0 (from -r /workspace/Time-LLM/requirements.txt (line 9))\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft==0.4.0 (from -r /workspace/Time-LLM/requirements.txt (line 10))\n",
      "  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting transformers==4.31.0 (from -r /workspace/Time-LLM/requirements.txt (line 11))\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting deepspeed==0.14.0 (from -r /workspace/Time-LLM/requirements.txt (line 12))\n",
      "  Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentencepiece==0.2.0 (from -r /workspace/Time-LLM/requirements.txt (line 13))\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1)) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1)) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1)) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0->-r /workspace/Time-LLM/requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0->-r /workspace/Time-LLM/requirements.txt (line 2)) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.28.0->-r /workspace/Time-LLM/requirements.txt (line 2)) (6.0.1)\n",
      "Collecting huggingface-hub (from accelerate==0.28.0->-r /workspace/Time-LLM/requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors>=0.3.1 (from accelerate==0.28.0->-r /workspace/Time-LLM/requirements.txt (line 2))\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.7.0->-r /workspace/Time-LLM/requirements.txt (line 4))\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.7.0->-r /workspace/Time-LLM/requirements.txt (line 4))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.7.0->-r /workspace/Time-LLM/requirements.txt (line 4))\n",
      "  Downloading fonttools-4.58.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib==3.7.0->-r /workspace/Time-LLM/requirements.txt (line 4))\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.0->-r /workspace/Time-LLM/requirements.txt (line 4)) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib==3.7.0->-r /workspace/Time-LLM/requirements.txt (line 4)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.0->-r /workspace/Time-LLM/requirements.txt (line 4)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas==1.5.3->-r /workspace/Time-LLM/requirements.txt (line 6))\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting joblib>=1.1.1 (from scikit_learn==1.2.2->-r /workspace/Time-LLM/requirements.txt (line 7))\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit_learn==1.2.2->-r /workspace/Time-LLM/requirements.txt (line 7))\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.31.0->-r /workspace/Time-LLM/requirements.txt (line 11))\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0->-r /workspace/Time-LLM/requirements.txt (line 11)) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0->-r /workspace/Time-LLM/requirements.txt (line 11))\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting hjson (from deepspeed==0.14.0->-r /workspace/Time-LLM/requirements.txt (line 12))\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting ninja (from deepspeed==0.14.0->-r /workspace/Time-LLM/requirements.txt (line 12))\n",
      "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting py-cpuinfo (from deepspeed==0.14.0->-r /workspace/Time-LLM/requirements.txt (line 12))\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pydantic (from deepspeed==0.14.0->-r /workspace/Time-LLM/requirements.txt (line 12))\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pynvml (from deepspeed==0.14.0->-r /workspace/Time-LLM/requirements.txt (line 12))\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting fsspec (from torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1))\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub->accelerate==0.28.0->-r /workspace/Time-LLM/requirements.txt (line 2))\n",
      "  Downloading hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.0->-r /workspace/Time-LLM/requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1)) (2.1.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic->deepspeed==0.14.0->-r /workspace/Time-LLM/requirements.txt (line 12))\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic->deepspeed==0.14.0->-r /workspace/Time-LLM/requirements.txt (line 12))\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic->deepspeed==0.14.0->-r /workspace/Time-LLM/requirements.txt (line 12))\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml->deepspeed==0.14.0->-r /workspace/Time-LLM/requirements.txt (line 12))\n",
      "  Downloading nvidia_ml_py-12.575.51-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0->-r /workspace/Time-LLM/requirements.txt (line 11)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0->-r /workspace/Time-LLM/requirements.txt (line 11)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0->-r /workspace/Time-LLM/requirements.txt (line 11)) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0->-r /workspace/Time-LLM/requirements.txt (line 11)) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.2->-r /workspace/Time-LLM/requirements.txt (line 1)) (1.3.0)\n",
      "Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.2/444.2 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400399 sha256=e3ff978848989d37bbeaf7d20e1d709caffb2a32940c32f2ee11385d494b563f\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/96/24/bab20c3b4e2af15e195b339afaec373eca7072cf90620432e5\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: tokenizers, sentencepiece, pytz, py-cpuinfo, nvidia-ml-py, hjson, typing-extensions, triton, tqdm, threadpoolctl, safetensors, regex, pynvml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, kiwisolver, joblib, hf-xet, fsspec, fonttools, einops, cycler, annotated-types, typing-inspection, scipy, pydantic-core, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, contourpy, transformers, scikit_learn, pydantic, nvidia-cusolver-cu12, matplotlib, torch, deepspeed, accelerate, peft\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.2.2 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.28.0 annotated-types-0.7.0 contourpy-1.3.2 cycler-0.12.1 deepspeed-0.14.0 einops-0.7.0 fonttools-4.58.2 fsspec-2025.5.1 hf-xet-1.1.3 hjson-3.1.0 huggingface-hub-0.33.0 joblib-1.5.1 kiwisolver-1.4.8 matplotlib-3.7.0 ninja-1.11.1.4 numpy-1.23.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py-12.575.51 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pandas-1.5.3 peft-0.4.0 py-cpuinfo-9.0.0 pydantic-2.11.5 pydantic-core-2.33.2 pynvml-12.0.0 pytz-2025.2 regex-2024.11.6 safetensors-0.5.3 scikit_learn-1.2.2 scipy-1.12.0 sentencepiece-0.2.0 threadpoolctl-3.6.0 tokenizers-0.13.3 torch-2.2.2 tqdm-4.65.0 transformers-4.31.0 triton-2.2.0 typing-extensions-4.14.0 typing-inspection-0.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: deepspeed in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
      "Requirement already satisfied: hjson in /usr/local/lib/python3.10/dist-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.11.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.6)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.11.5)\n",
      "Requirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from deepspeed) (12.0.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.65.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed) (4.14.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed) (0.4.1)\n",
      "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pynvml->deepspeed) (12.575.51)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /workspace/Time-LLM/requirements.txt\n",
    "!pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-25.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vi7fEyBqk-wp",
    "outputId": "b23d017c-4e02-47f1-fa59-0fb1364ee24a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.39.2\n",
      "  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.2) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.2) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.2) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.2) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.2)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.2) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.2) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.2) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.2) (1.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.2) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.2) (2022.12.7)\n",
      "Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.13.3\n",
      "\u001b[2K    Uninstalling tokenizers-0.13.3:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.13.3\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.31.0\n",
      "\u001b[2K    Uninstalling transformers-4.31.0:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.31.0━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed tokenizers-0.15.2 transformers-4.39.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.39.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "id": "JVBJpRk4msND",
    "outputId": "3280c3ac-c9f0-4ddd-82d7-dc81ba202e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.4\n",
      "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping jax as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping jaxlib as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.24.4\n",
    "!pip uninstall -y jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "byzYiy7jnS3T"
   },
   "outputs": [],
   "source": [
    "ds_config = \"\"\"\n",
    "{\n",
    "  \"train_batch_size\": 8,\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\"\n",
    "    },\n",
    "    \"allgather_partitions\": true,\n",
    "    \"allgather_bucket_size\": 200000000,\n",
    "    \"overlap_comm\": true,\n",
    "    \"reduce_scatter\": true,\n",
    "    \"reduce_bucket_size\": 200000000\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"ds_config_zero2.json\", \"w\") as f:\n",
    "    f.write(ds_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy InRelease [270 kB]                \u001b[0m\n",
      "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]3m\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1776 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2986 kB]\n",
      "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.8 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1246 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4476 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]       \u001b[0m\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]     \u001b[0m\u001b[33m\u001b[33m\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy/restricted amd64 Packages [164 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/multiverse amd64 Packages [266 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/universe amd64 Packages [17.5 MB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 Packages [1792 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4630 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1553 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3296 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
      "Fetched 40.6 MB in 3s (13.1 MB/s)[33m                       \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "143 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  autoconf automake autotools-dev file gfortran gfortran-11 ibverbs-providers\n",
      "  javascript-common libcaf-openmpi-3 libcoarrays-dev libcoarrays-openmpi-dev\n",
      "  libevent-2.1-7 libevent-core-2.1-7 libevent-dev libevent-extra-2.1-7\n",
      "  libevent-openssl-2.1-7 libevent-pthreads-2.1-7 libfabric1 libgfortran-11-dev\n",
      "  libgfortran5 libhwloc-dev libhwloc-plugins libhwloc15 libibverbs-dev\n",
      "  libibverbs1 libjs-jquery libjs-jquery-ui libltdl-dev libltdl7 libmagic-mgc\n",
      "  libmagic1 libnl-3-200 libnl-3-dev libnl-route-3-200 libnl-route-3-dev\n",
      "  libnuma-dev libnuma1 libopenmpi3 libpmix-dev libpmix2 libpsm-infinipath1\n",
      "  libpsm2-2 librdmacm1 libsigsegv2 libtool libucx0 libxnvctrl0 m4\n",
      "  ocl-icd-libopencl1 openmpi-bin openmpi-common\n",
      "Suggested packages:\n",
      "  autoconf-archive gnu-standards autoconf-doc gettext gfortran-multilib\n",
      "  gfortran-doc gfortran-11-multilib gfortran-11-doc libhwloc-contrib-plugins\n",
      "  libjs-jquery-ui-docs libtool-doc openmpi-doc gcj-jdk m4-doc opencl-icd\n",
      "The following NEW packages will be installed:\n",
      "  autoconf automake autotools-dev file gfortran gfortran-11 ibverbs-providers\n",
      "  javascript-common libcaf-openmpi-3 libcoarrays-dev libcoarrays-openmpi-dev\n",
      "  libevent-2.1-7 libevent-core-2.1-7 libevent-dev libevent-extra-2.1-7\n",
      "  libevent-openssl-2.1-7 libevent-pthreads-2.1-7 libfabric1 libgfortran-11-dev\n",
      "  libgfortran5 libhwloc-dev libhwloc-plugins libhwloc15 libibverbs-dev\n",
      "  libibverbs1 libjs-jquery libjs-jquery-ui libltdl-dev libltdl7 libmagic-mgc\n",
      "  libmagic1 libnl-3-200 libnl-3-dev libnl-route-3-200 libnl-route-3-dev\n",
      "  libnuma-dev libnuma1 libopenmpi-dev libopenmpi3 libpmix-dev libpmix2\n",
      "  libpsm-infinipath1 libpsm2-2 librdmacm1 libsigsegv2 libtool libucx0\n",
      "  libxnvctrl0 m4 ocl-icd-libopencl1 openmpi-bin openmpi-common\n",
      "0 upgraded, 52 newly installed, 0 to remove and 143 not upgraded.\n",
      "Need to get 25.7 MB of archives.\n",
      "After this operation, 107 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmagic-mgc amd64 1:5.41-3ubuntu0.1 [257 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libmagic1 amd64 1:5.41-3ubuntu0.1 [87.2 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 file amd64 1:5.41-3ubuntu0.1 [21.5 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnuma1 amd64 2.0.14-3ubuntu2 [22.5 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsigsegv2 amd64 2.13-1ubuntu3 [14.6 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 m4 amd64 1.4.18-5ubuntu2 [199 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 autoconf all 2.71-2 [338 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 autotools-dev all 20220109.1 [44.9 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 automake all 1:1.16.5-1.3 [558 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgfortran5 amd64 12.3.0-1ubuntu1~22.04 [879 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgfortran-11-dev amd64 11.4.0-1ubuntu1~22.04 [842 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gfortran-11 amd64 11.4.0-1ubuntu1~22.04 [11.2 MB]\n",
      "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libxnvctrl0 575.57.08-0ubuntu1 [11.5 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 gfortran amd64 4:11.2.0-1ubuntu1 [1182 B]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-3-200 amd64 3.5.0-0.1 [59.1 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-route-3-200 amd64 3.5.0-0.1 [180 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libibverbs1 amd64 39.0-1 [69.3 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 ibverbs-providers amd64 39.0-1 [341 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 javascript-common all 11+nmu1 [5936 B]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-core-2.1-7 amd64 2.1.12-stable-1build3 [93.9 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-pthreads-2.1-7 amd64 2.1.12-stable-1build3 [7642 B]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpsm-infinipath1 amd64 3.3+20.604758e7-6.1 [170 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpsm2-2 amd64 11.2.185-1 [182 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 librdmacm1 amd64 39.0-1 [71.2 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfabric1 amd64 1.11.0-3 [558 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libhwloc15 amd64 2.7.0-2ubuntu1 [159 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ocl-icd-libopencl1 amd64 2.2.14-3 [39.1 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libhwloc-plugins amd64 2.7.0-2ubuntu1 [15.6 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpmix2 amd64 4.1.2-2ubuntu1 [604 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libucx0 amd64 1.12.1~rc2-1 [891 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenmpi3 amd64 4.1.2-2ubuntu1 [2594 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcaf-openmpi-3 amd64 2.9.2-3 [36.5 kB]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcoarrays-dev amd64 2.9.2-3 [40.5 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 openmpi-common all 4.1.2-2ubuntu1 [162 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 openmpi-bin amd64 4.1.2-2ubuntu1 [116 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcoarrays-openmpi-dev amd64 2.9.2-3 [452 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-2.1-7 amd64 2.1.12-stable-1build3 [148 kB]\n",
      "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-extra-2.1-7 amd64 2.1.12-stable-1build3 [65.4 kB]\n",
      "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-openssl-2.1-7 amd64 2.1.12-stable-1build3 [15.8 kB]\n",
      "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevent-dev amd64 2.1.12-stable-1build3 [278 kB]\n",
      "Get:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjs-jquery all 3.6.0+dfsg+~3.5.13-1 [321 kB]\n",
      "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-jquery-ui all 1.13.1+dfsg-1 [253 kB]\n",
      "Get:43 http://archive.ubuntu.com/ubuntu jammy/main amd64 libltdl7 amd64 2.4.6-15build2 [39.6 kB]\n",
      "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 libltdl-dev amd64 2.4.6-15build2 [169 kB]\n",
      "Get:45 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-3-dev amd64 3.5.0-0.1 [101 kB]\n",
      "Get:46 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-route-3-dev amd64 3.5.0-0.1 [202 kB]\n",
      "Get:47 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnuma-dev amd64 2.0.14-3ubuntu2 [35.9 kB]\n",
      "Get:48 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libhwloc-dev amd64 2.7.0-2ubuntu1 [256 kB]\n",
      "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpmix-dev amd64 4.1.2-2ubuntu1 [805 kB]\n",
      "Get:50 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtool all 2.4.6-15build2 [164 kB]\n",
      "Get:51 http://archive.ubuntu.com/ubuntu jammy/main amd64 libibverbs-dev amd64 39.0-1 [628 kB]\n",
      "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopenmpi-dev amd64 4.1.2-2ubuntu1 [867 kB]\n",
      "Fetched 25.7 MB in 3s (9691 kB/s)        \u001b[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libmagic-mgc.\n",
      "(Reading database ... 20729 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libmagic-mgc_1%3a5.41-3ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libmagic-mgc (1:5.41-3ubuntu0.1) ...\n",
      "Selecting previously unselected package libmagic1:amd64.\n",
      "Preparing to unpack .../01-libmagic1_1%3a5.41-3ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  1%]\u001b[49m\u001b[39m [..........................................................] \u001b8Unpacking libmagic1:amd64 (1:5.41-3ubuntu0.1) ...\n",
      "Selecting previously unselected package file.\n",
      "Preparing to unpack .../02-file_1%3a5.41-3ubuntu0.1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  2%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking file (1:5.41-3ubuntu0.1) ...\n",
      "Selecting previously unselected package libnuma1:amd64.\n",
      "Preparing to unpack .../03-libnuma1_2.0.14-3ubuntu2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  3%]\u001b[49m\u001b[39m [#.........................................................] \u001b8Unpacking libnuma1:amd64 (2.0.14-3ubuntu2) ...\n",
      "Selecting previously unselected package libsigsegv2:amd64.\n",
      "Preparing to unpack .../04-libsigsegv2_2.13-1ubuntu3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  4%]\u001b[49m\u001b[39m [##........................................................] \u001b8Unpacking libsigsegv2:amd64 (2.13-1ubuntu3) ...\n",
      "Selecting previously unselected package m4.\n",
      "Preparing to unpack .../05-m4_1.4.18-5ubuntu2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  5%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking m4 (1.4.18-5ubuntu2) ...\n",
      "Selecting previously unselected package autoconf.\n",
      "Preparing to unpack .../06-autoconf_2.71-2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking autoconf (2.71-2) ...\n",
      "Selecting previously unselected package autotools-dev.\n",
      "Preparing to unpack .../07-autotools-dev_20220109.1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  7%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking autotools-dev (20220109.1) ...\n",
      "Selecting previously unselected package automake.\n",
      "Preparing to unpack .../08-automake_1%3a1.16.5-1.3_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  8%]\u001b[49m\u001b[39m [####......................................................] \u001b8Unpacking automake (1:1.16.5-1.3) ...\n",
      "Selecting previously unselected package libgfortran5:amd64.\n",
      "Preparing to unpack .../09-libgfortran5_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  9%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking libgfortran5:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package libgfortran-11-dev:amd64.\n",
      "Preparing to unpack .../10-libgfortran-11-dev_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 10%]\u001b[49m\u001b[39m [#####.....................................................] \u001b8Unpacking libgfortran-11-dev:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package gfortran-11.\n",
      "Preparing to unpack .../11-gfortran-11_11.4.0-1ubuntu1~22.04_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 11%]\u001b[49m\u001b[39m [######....................................................] \u001b8Unpacking gfortran-11 (11.4.0-1ubuntu1~22.04) ...\n",
      "Selecting previously unselected package gfortran.\n",
      "Preparing to unpack .../12-gfortran_4%3a11.2.0-1ubuntu1_amd64.deb ...\n",
      "Unpacking gfortran (4:11.2.0-1ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package libnl-3-200:amd64.\n",
      "Preparing to unpack .../13-libnl-3-200_3.5.0-0.1_amd64.deb ...\n",
      "Unpacking libnl-3-200:amd64 (3.5.0-0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 13%]\u001b[49m\u001b[39m [#######...................................................] \u001b8Selecting previously unselected package libnl-route-3-200:amd64.\n",
      "Preparing to unpack .../14-libnl-route-3-200_3.5.0-0.1_amd64.deb ...\n",
      "Unpacking libnl-route-3-200:amd64 (3.5.0-0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 14%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package libibverbs1:amd64.\n",
      "Preparing to unpack .../15-libibverbs1_39.0-1_amd64.deb ...\n",
      "Unpacking libibverbs1:amd64 (39.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 15%]\u001b[49m\u001b[39m [########..................................................] \u001b8Selecting previously unselected package ibverbs-providers:amd64.\n",
      "Preparing to unpack .../16-ibverbs-providers_39.0-1_amd64.deb ...\n",
      "Unpacking ibverbs-providers:amd64 (39.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package javascript-common.\n",
      "Preparing to unpack .../17-javascript-common_11+nmu1_all.deb ...\n",
      "Unpacking javascript-common (11+nmu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Selecting previously unselected package libevent-core-2.1-7:amd64.\n",
      "Preparing to unpack .../18-libevent-core-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-core-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libevent-pthreads-2.1-7:amd64.\n",
      "Preparing to unpack .../19-libevent-pthreads-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-pthreads-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 19%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libpsm-infinipath1.\n",
      "Preparing to unpack .../20-libpsm-infinipath1_3.3+20.604758e7-6.1_amd64.deb ...\n",
      "Unpacking libpsm-infinipath1 (3.3+20.604758e7-6.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Selecting previously unselected package libpsm2-2.\n",
      "Preparing to unpack .../21-libpsm2-2_11.2.185-1_amd64.deb ...\n",
      "Unpacking libpsm2-2 (11.2.185-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 21%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package librdmacm1:amd64.\n",
      "Preparing to unpack .../22-librdmacm1_39.0-1_amd64.deb ...\n",
      "Unpacking librdmacm1:amd64 (39.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 22%]\u001b[49m\u001b[39m [############..............................................] \u001b8Selecting previously unselected package libfabric1:amd64.\n",
      "Preparing to unpack .../23-libfabric1_1.11.0-3_amd64.deb ...\n",
      "Unpacking libfabric1:amd64 (1.11.0-3) ...\n",
      "Selecting previously unselected package libhwloc15:amd64.\n",
      "Preparing to unpack .../24-libhwloc15_2.7.0-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 23%]\u001b[49m\u001b[39m [#############.............................................] \u001b8Unpacking libhwloc15:amd64 (2.7.0-2ubuntu1) ...\n",
      "Selecting previously unselected package libxnvctrl0:amd64.\n",
      "Preparing to unpack .../25-libxnvctrl0_575.57.08-0ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 24%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libxnvctrl0:amd64 (575.57.08-0ubuntu1) ...\n",
      "Selecting previously unselected package ocl-icd-libopencl1:amd64.\n",
      "Preparing to unpack .../26-ocl-icd-libopencl1_2.2.14-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking ocl-icd-libopencl1:amd64 (2.2.14-3) ...\n",
      "Selecting previously unselected package libhwloc-plugins:amd64.\n",
      "Preparing to unpack .../27-libhwloc-plugins_2.7.0-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 26%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking libhwloc-plugins:amd64 (2.7.0-2ubuntu1) ...\n",
      "Selecting previously unselected package libpmix2:amd64.\n",
      "Preparing to unpack .../28-libpmix2_4.1.2-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 27%]\u001b[49m\u001b[39m [###############...........................................] \u001b8Unpacking libpmix2:amd64 (4.1.2-2ubuntu1) ...\n",
      "Selecting previously unselected package libucx0:amd64.\n",
      "Preparing to unpack .../29-libucx0_1.12.1~rc2-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 28%]\u001b[49m\u001b[39m [################..........................................] \u001b8Unpacking libucx0:amd64 (1.12.1~rc2-1) ...\n",
      "Selecting previously unselected package libopenmpi3:amd64.\n",
      "Preparing to unpack .../30-libopenmpi3_4.1.2-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 29%]\u001b[49m\u001b[39m [################..........................................] \u001b8Unpacking libopenmpi3:amd64 (4.1.2-2ubuntu1) ...\n",
      "Selecting previously unselected package libcaf-openmpi-3:amd64.\n",
      "Preparing to unpack .../31-libcaf-openmpi-3_2.9.2-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 30%]\u001b[49m\u001b[39m [#################.........................................] \u001b8Unpacking libcaf-openmpi-3:amd64 (2.9.2-3) ...\n",
      "Selecting previously unselected package libcoarrays-dev:amd64.\n",
      "Preparing to unpack .../32-libcoarrays-dev_2.9.2-3_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking libcoarrays-dev:amd64 (2.9.2-3) ...\n",
      "Selecting previously unselected package openmpi-common.\n",
      "Preparing to unpack .../33-openmpi-common_4.1.2-2ubuntu1_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 32%]\u001b[49m\u001b[39m [##################........................................] \u001b8Unpacking openmpi-common (4.1.2-2ubuntu1) ...\n",
      "Selecting previously unselected package openmpi-bin.\n",
      "Preparing to unpack .../34-openmpi-bin_4.1.2-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Unpacking openmpi-bin (4.1.2-2ubuntu1) ...\n",
      "Selecting previously unselected package libcoarrays-openmpi-dev:amd64.\n",
      "Preparing to unpack .../35-libcoarrays-openmpi-dev_2.9.2-3_amd64.deb ...\n",
      "Unpacking libcoarrays-openmpi-dev:amd64 (2.9.2-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 34%]\u001b[49m\u001b[39m [###################.......................................] \u001b8Selecting previously unselected package libevent-2.1-7:amd64.\n",
      "Preparing to unpack .../36-libevent-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 35%]\u001b[49m\u001b[39m [####################......................................] \u001b8Selecting previously unselected package libevent-extra-2.1-7:amd64.\n",
      "Preparing to unpack .../37-libevent-extra-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-extra-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 36%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libevent-openssl-2.1-7:amd64.\n",
      "Preparing to unpack .../38-libevent-openssl-2.1-7_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-openssl-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libevent-dev.\n",
      "Preparing to unpack .../39-libevent-dev_2.1.12-stable-1build3_amd64.deb ...\n",
      "Unpacking libevent-dev (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 38%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libjs-jquery.\n",
      "Preparing to unpack .../40-libjs-jquery_3.6.0+dfsg+~3.5.13-1_all.deb ...\n",
      "Unpacking libjs-jquery (3.6.0+dfsg+~3.5.13-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 39%]\u001b[49m\u001b[39m [######################....................................] \u001b8Selecting previously unselected package libjs-jquery-ui.\n",
      "Preparing to unpack .../41-libjs-jquery-ui_1.13.1+dfsg-1_all.deb ...\n",
      "Unpacking libjs-jquery-ui (1.13.1+dfsg-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package libltdl7:amd64.\n",
      "Preparing to unpack .../42-libltdl7_2.4.6-15build2_amd64.deb ...\n",
      "Unpacking libltdl7:amd64 (2.4.6-15build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 41%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Selecting previously unselected package libltdl-dev:amd64.\n",
      "Preparing to unpack .../43-libltdl-dev_2.4.6-15build2_amd64.deb ...\n",
      "Unpacking libltdl-dev:amd64 (2.4.6-15build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 42%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package libnl-3-dev:amd64.\n",
      "Preparing to unpack .../44-libnl-3-dev_3.5.0-0.1_amd64.deb ...\n",
      "Unpacking libnl-3-dev:amd64 (3.5.0-0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [########################..................................] \u001b8Selecting previously unselected package libnl-route-3-dev:amd64.\n",
      "Preparing to unpack .../45-libnl-route-3-dev_3.5.0-0.1_amd64.deb ...\n",
      "Unpacking libnl-route-3-dev:amd64 (3.5.0-0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 44%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Selecting previously unselected package libnuma-dev:amd64.\n",
      "Preparing to unpack .../46-libnuma-dev_2.0.14-3ubuntu2_amd64.deb ...\n",
      "Unpacking libnuma-dev:amd64 (2.0.14-3ubuntu2) ...\n",
      "Selecting previously unselected package libhwloc-dev:amd64.\n",
      "Preparing to unpack .../47-libhwloc-dev_2.7.0-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 45%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking libhwloc-dev:amd64 (2.7.0-2ubuntu1) ...\n",
      "Selecting previously unselected package libpmix-dev:amd64.\n",
      "Preparing to unpack .../48-libpmix-dev_4.1.2-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 46%]\u001b[49m\u001b[39m [##########################................................] \u001b8Unpacking libpmix-dev:amd64 (4.1.2-2ubuntu1) ...\n",
      "Selecting previously unselected package libtool.\n",
      "Preparing to unpack .../49-libtool_2.4.6-15build2_all.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 47%]\u001b[49m\u001b[39m [###########################...............................] \u001b8Unpacking libtool (2.4.6-15build2) ...\n",
      "Selecting previously unselected package libibverbs-dev:amd64.\n",
      "Preparing to unpack .../50-libibverbs-dev_39.0-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 48%]\u001b[49m\u001b[39m [############################..............................] \u001b8Unpacking libibverbs-dev:amd64 (39.0-1) ...\n",
      "Selecting previously unselected package libopenmpi-dev:amd64.\n",
      "Preparing to unpack .../51-libopenmpi-dev_4.1.2-2ubuntu1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 49%]\u001b[49m\u001b[39m [############################..............................] \u001b8Unpacking libopenmpi-dev:amd64 (4.1.2-2ubuntu1) ...\n",
      "Setting up javascript-common (11+nmu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libmagic-mgc (1:5.41-3ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 51%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libmagic1:amd64 (1:5.41-3ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 52%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up file (1:5.41-3ubuntu0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 53%]\u001b[49m\u001b[39m [##############################............................] \u001b8Setting up libxnvctrl0:amd64 (575.57.08-0ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 54%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up autotools-dev (20220109.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 55%]\u001b[49m\u001b[39m [###############################...........................] \u001b8Setting up libsigsegv2:amd64 (2.13-1ubuntu3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libhwloc15:amd64 (2.7.0-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 57%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libevent-core-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 58%]\u001b[49m\u001b[39m [#################################.........................] \u001b8Setting up libevent-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 59%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libltdl7:amd64 (2.4.6-15build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8Setting up libgfortran5:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 61%]\u001b[49m\u001b[39m [###################################.......................] \u001b8Setting up libnuma1:amd64 (2.0.14-3ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up ocl-icd-libopencl1:amd64 (2.2.14-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 63%]\u001b[49m\u001b[39m [####################################......................] \u001b8Setting up libnl-3-200:amd64 (3.5.0-0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 64%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up libpsm2-2 (11.2.185-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 65%]\u001b[49m\u001b[39m [#####################################.....................] \u001b8Setting up openmpi-common (4.1.2-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [######################################....................] \u001b8Setting up libpsm-infinipath1 (3.3+20.604758e7-6.1) ...\n",
      "update-alternatives: using /usr/lib/libpsm1/libpsm_infinipath.so.1.16 to provide /usr/lib/x86_64-linux-gnu/libpsm_infinipath.so.1 (libpsm_infinipath.so.1) in auto mode\n",
      "Setting up libjs-jquery (3.6.0+dfsg+~3.5.13-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libevent-pthreads-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libevent-extra-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 69%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libtool (2.4.6-15build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 70%]\u001b[49m\u001b[39m [########################################..................] \u001b8Setting up libgfortran-11-dev:amd64 (11.4.0-1ubuntu1~22.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 71%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up libevent-openssl-2.1-7:amd64 (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 72%]\u001b[49m\u001b[39m [#########################################.................] \u001b8Setting up m4 (1.4.18-5ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 73%]\u001b[49m\u001b[39m [##########################################................] \u001b8Setting up libhwloc-plugins:amd64 (2.7.0-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 74%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libnuma-dev:amd64 (2.0.14-3ubuntu2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8Setting up libnl-route-3-200:amd64 (3.5.0-0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 76%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libjs-jquery-ui (1.13.1+dfsg-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 77%]\u001b[49m\u001b[39m [############################################..............] \u001b8Setting up libevent-dev (2.1.12-stable-1build3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 78%]\u001b[49m\u001b[39m [#############################################.............] \u001b8Setting up gfortran-11 (11.4.0-1ubuntu1~22.04) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 79%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up autoconf (2.71-2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Setting up libnl-3-dev:amd64 (3.5.0-0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up automake (1:1.16.5-1.3) ...\n",
      "update-alternatives: using /usr/bin/automake-1.16 to provide /usr/bin/automake (automake) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/automake.1.gz because associated file /usr/share/man/man1/automake-1.16.1.gz (of link group automake) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/aclocal.1.gz because associated file /usr/share/man/man1/aclocal-1.16.1.gz (of link group automake) doesn't exist\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 82%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libibverbs1:amd64 (39.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up libpmix2:amd64 (4.1.2-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 84%]\u001b[49m\u001b[39m [################################################..........] \u001b8Setting up ibverbs-providers:amd64 (39.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 85%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up gfortran (4:11.2.0-1ubuntu1) ...\n",
      "update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f95 (f95) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/f95.1.gz because associated file /usr/share/man/man1/gfortran.1.gz (of link group f95) doesn't exist\n",
      "update-alternatives: using /usr/bin/gfortran to provide /usr/bin/f77 (f77) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/f77.1.gz because associated file /usr/share/man/man1/gfortran.1.gz (of link group f77) doesn't exist\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 86%]\u001b[49m\u001b[39m [#################################################.........] \u001b8Setting up libnl-route-3-dev:amd64 (3.5.0-0.1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8Setting up libltdl-dev:amd64 (2.4.6-15build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 88%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up libhwloc-dev:amd64 (2.7.0-2ubuntu1) ...\n",
      "Setting up libpmix-dev:amd64 (4.1.2-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 89%]\u001b[49m\u001b[39m [###################################################.......] \u001b8Setting up librdmacm1:amd64 (39.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 90%]\u001b[49m\u001b[39m [####################################################......] \u001b8Setting up libucx0:amd64 (1.12.1~rc2-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 91%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up libcoarrays-dev:amd64 (2.9.2-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 92%]\u001b[49m\u001b[39m [#####################################################.....] \u001b8Setting up libibverbs-dev:amd64 (39.0-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up libfabric1:amd64 (1.11.0-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 94%]\u001b[49m\u001b[39m [######################################################....] \u001b8Setting up libopenmpi3:amd64 (4.1.2-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 95%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up libcaf-openmpi-3:amd64 (2.9.2-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 96%]\u001b[49m\u001b[39m [#######################################################...] \u001b8Setting up openmpi-bin (4.1.2-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 97%]\u001b[49m\u001b[39m [########################################################..] \u001b8update-alternatives: using /usr/bin/mpirun.openmpi to provide /usr/bin/mpirun (mpirun) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpirun.1.gz because associated file /usr/share/man/man1/mpirun.openmpi.1.gz (of link group mpirun) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpiexec.1.gz because associated file /usr/share/man/man1/mpiexec.openmpi.1.gz (of link group mpirun) doesn't exist\n",
      "update-alternatives: using /usr/bin/mpicc.openmpi to provide /usr/bin/mpicc (mpi) in auto mode\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpicc.1.gz because associated file /usr/share/man/man1/mpicc.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpic++.1.gz because associated file /usr/share/man/man1/mpic++.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpicxx.1.gz because associated file /usr/share/man/man1/mpicxx.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpiCC.1.gz because associated file /usr/share/man/man1/mpiCC.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpif77.1.gz because associated file /usr/share/man/man1/mpif77.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpif90.1.gz because associated file /usr/share/man/man1/mpif90.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "update-alternatives: warning: skip creation of /usr/share/man/man1/mpifort.1.gz because associated file /usr/share/man/man1/mpifort.openmpi.1.gz (of link group mpi) doesn't exist\n",
      "Setting up libcoarrays-openmpi-dev:amd64 (2.9.2-3) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 98%]\u001b[49m\u001b[39m [########################################################..] \u001b8update-alternatives: using /usr/lib/x86_64-linux-gnu/open-coarrays/openmpi/bin/caf to provide /usr/bin/caf.openmpi (caf-openmpi) in auto mode\n",
      "update-alternatives: using /usr/bin/caf.openmpi to provide /usr/bin/caf (caf) in auto mode\n",
      "Setting up libopenmpi-dev:amd64 (4.1.2-2ubuntu1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 99%]\u001b[49m\u001b[39m [#########################################################.] \u001b8update-alternatives: using /usr/lib/x86_64-linux-gnu/openmpi/include to provide /usr/include/x86_64-linux-gnu/mpi (mpi-x86_64-linux-gnu) in auto mode\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt update\n",
    "!apt install -y libopenmpi-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "build-essential set to manually installed.\n",
      "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
      "openmpi-bin is already the newest version (4.1.2-2ubuntu1).\n",
      "openmpi-bin set to manually installed.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 143 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt install -y build-essential openmpi-bin libopenmpi-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjVC3ZR6oQ0q",
    "outputId": "cab24475-d4b8-498c-d86e-1cc0a78c6a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mpi4py\n",
      "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: mpi4py\n",
      "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp310-cp310-linux_x86_64.whl size=4255570 sha256=3c00b68b8e99c4cbed4bae46ce0631e2af59907726c9d30bd925a657184c24ac\n",
      "  Stored in directory: /root/.cache/pip/wheels/94/57/cc/2e34c76a8690ce2a90b200702473f579b23f8224b5058b6a6d\n",
      "Successfully built mpi4py\n",
      "Installing collected packages: mpi4py\n",
      "Successfully installed mpi4py-4.0.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExrQ9a7uoHZW",
    "outputId": "03ec0a8d-f851-4c96-c968-6a3b14d51d9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Time-LLM\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/Time-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DB2Pic2Tr-um",
    "outputId": "128d90c5-9c03-4b16-852b-052c95fd3c3f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 5, in <module>\n",
      "    from accelerate.commands.accelerate_cli import main\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 19, in <module>\n",
      "    from accelerate.commands.estimate import estimate_command_parser\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/estimate.py\", line 34, in <module>\n",
      "    import timm\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/__init__.py\", line 2, in <module>\n",
      "    from .layers import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/__init__.py\", line 8, in <module>\n",
      "    from .classifier import create_classifier, ClassifierHead, NormMlpClassifierHead, ClNormMlpClassifierHead\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/classifier.py\", line 15, in <module>\n",
      "    from .create_norm import get_norm_layer\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/create_norm.py\", line 14, in <module>\n",
      "    from torchvision.ops.misc import FrozenBatchNorm2d\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
      "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\", line 163, in <module>\n",
      "    @torch.library.register_fake(\"torchvision::nms\")\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'torch.library' has no attribute 'register_fake'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 5, in <module>\n",
      "    from accelerate.commands.accelerate_cli import main\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 19, in <module>\n",
      "    from accelerate.commands.estimate import estimate_command_parser\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/estimate.py\", line 34, in <module>\n",
      "    import timm\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/__init__.py\", line 2, in <module>\n",
      "    from .layers import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/__init__.py\", line 8, in <module>\n",
      "    from .classifier import create_classifier, ClassifierHead, NormMlpClassifierHead, ClNormMlpClassifierHead\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/classifier.py\", line 15, in <module>\n",
      "    from .create_norm import get_norm_layer\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/create_norm.py\", line 14, in <module>\n",
      "    from torchvision.ops.misc import FrozenBatchNorm2d\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
      "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\", line 163, in <module>\n",
      "    @torch.library.register_fake(\"torchvision::nms\")\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'torch.library' has no attribute 'register_fake'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 5, in <module>\n",
      "    from accelerate.commands.accelerate_cli import main\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 19, in <module>\n",
      "    from accelerate.commands.estimate import estimate_command_parser\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/estimate.py\", line 34, in <module>\n",
      "    import timm\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/__init__.py\", line 2, in <module>\n",
      "    from .layers import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/__init__.py\", line 8, in <module>\n",
      "    from .classifier import create_classifier, ClassifierHead, NormMlpClassifierHead, ClNormMlpClassifierHead\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/classifier.py\", line 15, in <module>\n",
      "    from .create_norm import get_norm_layer\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/create_norm.py\", line 14, in <module>\n",
      "    from torchvision.ops.misc import FrozenBatchNorm2d\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
      "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\", line 163, in <module>\n",
      "    @torch.library.register_fake(\"torchvision::nms\")\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'torch.library' has no attribute 'register_fake'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 5, in <module>\n",
      "    from accelerate.commands.accelerate_cli import main\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 19, in <module>\n",
      "    from accelerate.commands.estimate import estimate_command_parser\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/estimate.py\", line 34, in <module>\n",
      "    import timm\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/__init__.py\", line 2, in <module>\n",
      "    from .layers import (\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/__init__.py\", line 8, in <module>\n",
      "    from .classifier import create_classifier, ClassifierHead, NormMlpClassifierHead, ClNormMlpClassifierHead\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/classifier.py\", line 15, in <module>\n",
      "    from .create_norm import get_norm_layer\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/timm/layers/create_norm.py\", line 14, in <module>\n",
      "    from torchvision.ops.misc import FrozenBatchNorm2d\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\", line 10, in <module>\n",
      "    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/_meta_registrations.py\", line 163, in <module>\n",
      "    @torch.library.register_fake(\"torchvision::nms\")\n",
      "     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: module 'torch.library' has no attribute 'register_fake'\n"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/TimeLLM_ETTh1.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "K-OKptz2gplS",
    "outputId": "8a15de8a-a4a5-4ad9-9f80-a4eaeaaa0452",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victor-250612-Choosing 2B/7B\n",
      "\n",
      "--- Starting Gemma 7B Loading Process ---\n",
      "Loading tokenizer for 'google/gemma-7b'...\n",
      "Tokenizer loaded successfully.\n",
      "Loading model 'google/gemma-7b'... This may take a few minutes and download several gigabytes.\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.19it/s]\n",
      "Model loaded successfully and moved to the appropriate device.\n",
      "[2025-06-13 01:58:48,086] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-06-13 01:58:48,222] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2025-06-13 01:58:48,222] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2025-06-13 01:58:48,222] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2025-06-13 01:58:48,976] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.24.0.2, master_port=29500\n",
      "[2025-06-13 01:58:48,977] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-06-13 01:58:50,390] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-06-13 01:58:50,391] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-06-13 01:58:50,391] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-06-13 01:58:50,392] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2025-06-13 01:58:50,392] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2025-06-13 01:58:50,392] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2025-06-13 01:58:50,392] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2025-06-13 01:58:50,392] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2025-06-13 01:58:50,392] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2025-06-13 01:58:50,392] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2025-06-13 01:58:51,431] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2025-06-13 01:58:51,432] [INFO] [utils.py:801:see_memory_usage] MA 17.35 GB         Max_MA 17.83 GB         CA 17.83 GB         Max_CA 18 GB \n",
      "[2025-06-13 01:58:51,432] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 61.51 GB, percent = 4.1%\n",
      "[2025-06-13 01:58:51,499] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2025-06-13 01:58:51,500] [INFO] [utils.py:801:see_memory_usage] MA 17.35 GB         Max_MA 18.31 GB         CA 18.79 GB         Max_CA 19 GB \n",
      "[2025-06-13 01:58:51,500] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 61.51 GB, percent = 4.1%\n",
      "[2025-06-13 01:58:51,500] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2025-06-13 01:58:51,564] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-06-13 01:58:51,564] [INFO] [utils.py:801:see_memory_usage] MA 17.35 GB         Max_MA 17.35 GB         CA 18.79 GB         Max_CA 19 GB \n",
      "[2025-06-13 01:58:51,564] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 61.51 GB, percent = 4.1%\n",
      "[2025-06-13 01:58:51,565] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2025-06-13 01:58:51,565] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2025-06-13 01:58:51,565] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-06-13 01:58:51,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4.000000000000002e-06], mom=[(0.95, 0.999)]\n",
      "[2025-06-13 01:58:51,566] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2025-06-13 01:58:51,566] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-06-13 01:58:51,566] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2025-06-13 01:58:51,566] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2025-06-13 01:58:51,566] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ad7993679a0>\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-06-13 01:58:51,567] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2025-06-13 01:58:51,568] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   train_batch_size ............. 16\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  16\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2025-06-13 01:58:51,569] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2025-06-13 01:58:51,570] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2025-06-13 01:58:51,570] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2025-06-13 01:58:51,570] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2025-06-13 01:58:51,570] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2025-06-13 01:58:51,570] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-06-13 01:58:51,570] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2025-06-13 01:58:51,570] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-06-13 01:58:51,570] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2025-06-13 01:58:51,570] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 16, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "99it [00:30,  3.43it/s]\titers: 100, epoch: 1 | loss: 0.5389749\n",
      "\tspeed: 0.3381s/iter; left time: 12464.2269s\n",
      "199it [00:59,  3.28it/s]\titers: 200, epoch: 1 | loss: 0.3905720\n",
      "\tspeed: 0.2970s/iter; left time: 10916.9413s\n",
      "299it [01:29,  3.38it/s]\titers: 300, epoch: 1 | loss: 0.3540888\n",
      "\tspeed: 0.2978s/iter; left time: 10916.8022s\n",
      "399it [01:59,  3.33it/s]\titers: 400, epoch: 1 | loss: 0.5273830\n",
      "\tspeed: 0.2986s/iter; left time: 10916.1188s\n",
      "499it [02:32,  3.08it/s]\titers: 500, epoch: 1 | loss: 0.5445503\n",
      "\tspeed: 0.3307s/iter; left time: 12057.4631s\n",
      "599it [03:01,  3.44it/s]\titers: 600, epoch: 1 | loss: 0.5437137\n",
      "\tspeed: 0.2905s/iter; left time: 10563.0920s\n",
      "699it [03:30,  3.46it/s]\titers: 700, epoch: 1 | loss: 0.4068151\n",
      "\tspeed: 0.2903s/iter; left time: 10526.5603s\n",
      "799it [04:00,  3.35it/s]\titers: 800, epoch: 1 | loss: 0.4848845\n",
      "\tspeed: 0.2992s/iter; left time: 10818.1797s\n",
      "899it [04:30,  3.16it/s]\titers: 900, epoch: 1 | loss: 0.4945395\n",
      "\tspeed: 0.3024s/iter; left time: 10906.1582s\n",
      "999it [05:03,  3.45it/s]\titers: 1000, epoch: 1 | loss: 0.5638670\n",
      "\tspeed: 0.3238s/iter; left time: 11643.0855s\n",
      "1099it [05:32,  3.46it/s]\titers: 1100, epoch: 1 | loss: 0.7409131\n",
      "\tspeed: 0.2906s/iter; left time: 10421.6643s\n",
      "1199it [06:02,  3.29it/s]\titers: 1200, epoch: 1 | loss: 0.4459667\n",
      "\tspeed: 0.2994s/iter; left time: 10705.6333s\n",
      "1299it [06:32,  3.34it/s]\titers: 1300, epoch: 1 | loss: 0.3205299\n",
      "\tspeed: 0.3000s/iter; left time: 10699.4294s\n",
      "1399it [07:01,  3.40it/s]\titers: 1400, epoch: 1 | loss: 0.8572370\n",
      "\tspeed: 0.2970s/iter; left time: 10561.1040s\n",
      "1499it [07:31,  3.34it/s]\titers: 1500, epoch: 1 | loss: 0.7862072\n",
      "\tspeed: 0.2983s/iter; left time: 10578.8789s\n",
      "1599it [08:01,  3.35it/s]\titers: 1600, epoch: 1 | loss: 0.6155725\n",
      "\tspeed: 0.2946s/iter; left time: 10418.0500s\n",
      "1699it [08:32,  3.08it/s]\titers: 1700, epoch: 1 | loss: 0.3865549\n",
      "\tspeed: 0.3119s/iter; left time: 10996.8779s\n",
      "1799it [09:03,  3.27it/s]\titers: 1800, epoch: 1 | loss: 0.8548797\n",
      "\tspeed: 0.3084s/iter; left time: 10843.2923s\n",
      "1899it [09:34,  3.31it/s]\titers: 1900, epoch: 1 | loss: 0.3632402\n",
      "\tspeed: 0.3150s/iter; left time: 11042.8644s\n",
      "1999it [10:06,  3.11it/s]\titers: 2000, epoch: 1 | loss: 0.4556250\n",
      "\tspeed: 0.3200s/iter; left time: 11188.4550s\n",
      "2099it [10:36,  3.41it/s]\titers: 2100, epoch: 1 | loss: 0.3097339\n",
      "\tspeed: 0.2940s/iter; left time: 10250.4470s\n",
      "2199it [11:05,  3.40it/s]\titers: 2200, epoch: 1 | loss: 0.4226862\n",
      "\tspeed: 0.2951s/iter; left time: 10256.8474s\n",
      "2299it [11:35,  3.35it/s]\titers: 2300, epoch: 1 | loss: 0.3631898\n",
      "\tspeed: 0.2989s/iter; left time: 10360.2521s\n",
      "2399it [12:05,  3.24it/s]\titers: 2400, epoch: 1 | loss: 0.3115807\n",
      "\tspeed: 0.2996s/iter; left time: 10355.2675s\n",
      "2499it [12:35,  3.31it/s]\titers: 2500, epoch: 1 | loss: 0.3299899\n",
      "\tspeed: 0.3019s/iter; left time: 10402.5628s\n",
      "2599it [13:05,  3.40it/s]\titers: 2600, epoch: 1 | loss: 0.4992775\n",
      "\tspeed: 0.3001s/iter; left time: 10312.9807s\n",
      "2699it [13:35,  3.35it/s]\titers: 2700, epoch: 1 | loss: 0.3553942\n",
      "\tspeed: 0.2949s/iter; left time: 10103.5059s\n",
      "2799it [14:04,  3.43it/s]\titers: 2800, epoch: 1 | loss: 0.3713297\n",
      "\tspeed: 0.2956s/iter; left time: 10097.1787s\n",
      "2899it [14:34,  3.42it/s]\titers: 2900, epoch: 1 | loss: 0.4022743\n",
      "\tspeed: 0.2964s/iter; left time: 10097.2970s\n",
      "2999it [15:03,  3.46it/s]\titers: 3000, epoch: 1 | loss: 0.3745370\n",
      "\tspeed: 0.2962s/iter; left time: 10057.7343s\n",
      "3099it [15:32,  3.46it/s]\titers: 3100, epoch: 1 | loss: 0.5132129\n",
      "\tspeed: 0.2886s/iter; left time: 9773.6529s\n",
      "3199it [16:02,  3.39it/s]\titers: 3200, epoch: 1 | loss: 0.5147268\n",
      "\tspeed: 0.2925s/iter; left time: 9875.0927s\n",
      "3299it [16:31,  3.43it/s]\titers: 3300, epoch: 1 | loss: 0.2960162\n",
      "\tspeed: 0.2915s/iter; left time: 9812.8865s\n",
      "3399it [17:01,  3.01it/s]\titers: 3400, epoch: 1 | loss: 0.6551767\n",
      "\tspeed: 0.3039s/iter; left time: 10200.0031s\n",
      "3499it [17:34,  3.07it/s]\titers: 3500, epoch: 1 | loss: 0.6553926\n",
      "\tspeed: 0.3299s/iter; left time: 11038.6467s\n",
      "3599it [18:08,  2.73it/s]\titers: 3600, epoch: 1 | loss: 0.4890575\n",
      "\tspeed: 0.3350s/iter; left time: 11176.8605s\n",
      "3696it [18:41,  3.30it/s]\n",
      "Epoch: 1 cost time: 1121.168063402176\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:44,  7.40it/s]\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:23,  8.50it/s]\n",
      "Epoch: 1 | Train Loss: 0.5238241 Vali Loss: 0.8573977 Test Loss: 0.4912486 MAE Loss: 0.4668450\n",
      "lr = 0.0000040000\n",
      "Updating learning rate to 4.000000000000002e-06\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "99it [00:29,  3.45it/s]\titers: 100, epoch: 2 | loss: 0.6643361\n",
      "\tspeed: 3.8351s/iter; left time: 127189.4686s\n",
      "199it [00:58,  3.48it/s]\titers: 200, epoch: 2 | loss: 0.2846662\n",
      "\tspeed: 0.2900s/iter; left time: 9587.6119s\n",
      "299it [01:27,  3.48it/s]\titers: 300, epoch: 2 | loss: 0.5838317\n",
      "\tspeed: 0.2882s/iter; left time: 9499.4366s\n",
      "399it [01:56,  3.48it/s]\titers: 400, epoch: 2 | loss: 0.5738685\n",
      "\tspeed: 0.2883s/iter; left time: 9474.7234s\n",
      "499it [02:24,  3.46it/s]\titers: 500, epoch: 2 | loss: 0.5374463\n",
      "\tspeed: 0.2884s/iter; left time: 9448.9382s\n",
      "599it [02:53,  3.49it/s]\titers: 600, epoch: 2 | loss: 0.3683378\n",
      "\tspeed: 0.2880s/iter; left time: 9408.1750s\n",
      "699it [03:22,  3.44it/s]\titers: 700, epoch: 2 | loss: 0.5480833\n",
      "\tspeed: 0.2889s/iter; left time: 9409.2011s\n",
      "799it [03:51,  3.45it/s]\titers: 800, epoch: 2 | loss: 0.4456811\n",
      "\tspeed: 0.2907s/iter; left time: 9438.0590s\n",
      "899it [04:20,  3.43it/s]\titers: 900, epoch: 2 | loss: 0.1710064\n",
      "\tspeed: 0.2907s/iter; left time: 9407.0248s\n",
      "999it [04:49,  3.43it/s]\titers: 1000, epoch: 2 | loss: 0.5435949\n",
      "\tspeed: 0.2886s/iter; left time: 9312.4679s\n",
      "1099it [05:18,  3.44it/s]\titers: 1100, epoch: 2 | loss: 0.4234154\n",
      "\tspeed: 0.2887s/iter; left time: 9287.3305s\n",
      "1199it [05:47,  3.41it/s]\titers: 1200, epoch: 2 | loss: 0.6164606\n",
      "\tspeed: 0.2917s/iter; left time: 9354.4104s\n",
      "1299it [06:16,  3.44it/s]\titers: 1300, epoch: 2 | loss: 0.2930855\n",
      "\tspeed: 0.2901s/iter; left time: 9272.5543s\n",
      "1399it [06:46,  3.48it/s]\titers: 1400, epoch: 2 | loss: 0.2871781\n",
      "\tspeed: 0.3016s/iter; left time: 9609.4225s\n",
      "1499it [07:15,  3.45it/s]\titers: 1500, epoch: 2 | loss: 0.2704344\n",
      "\tspeed: 0.2875s/iter; left time: 9132.9997s\n",
      "1599it [07:44,  3.45it/s]\titers: 1600, epoch: 2 | loss: 0.7558002\n",
      "\tspeed: 0.2907s/iter; left time: 9206.5226s\n",
      "1699it [08:13,  3.44it/s]\titers: 1700, epoch: 2 | loss: 0.8290588\n",
      "\tspeed: 0.2908s/iter; left time: 9179.0573s\n",
      "1799it [08:42,  3.46it/s]\titers: 1800, epoch: 2 | loss: 0.2325925\n",
      "\tspeed: 0.2901s/iter; left time: 9127.2217s\n",
      "1899it [09:11,  3.46it/s]\titers: 1900, epoch: 2 | loss: 0.3346842\n",
      "\tspeed: 0.2896s/iter; left time: 9084.1460s\n",
      "1999it [09:40,  3.48it/s]\titers: 2000, epoch: 2 | loss: 0.3423248\n",
      "\tspeed: 0.2891s/iter; left time: 9039.9392s\n",
      "2099it [10:11,  2.58it/s]\titers: 2100, epoch: 2 | loss: 0.2892722\n",
      "\tspeed: 0.3132s/iter; left time: 9759.4275s\n",
      "2199it [10:47,  3.47it/s]\titers: 2200, epoch: 2 | loss: 0.5321702\n",
      "\tspeed: 0.3566s/iter; left time: 11078.6465s\n",
      "2299it [11:16,  3.49it/s]\titers: 2300, epoch: 2 | loss: 0.3307129\n",
      "\tspeed: 0.2868s/iter; left time: 8880.2404s\n",
      "2399it [11:45,  3.44it/s]\titers: 2400, epoch: 2 | loss: 0.4788206\n",
      "\tspeed: 0.2904s/iter; left time: 8964.4948s\n",
      "2499it [12:14,  3.43it/s]\titers: 2500, epoch: 2 | loss: 0.4831259\n",
      "\tspeed: 0.2898s/iter; left time: 8915.2807s\n",
      "2599it [12:43,  3.48it/s]\titers: 2600, epoch: 2 | loss: 0.3231266\n",
      "\tspeed: 0.2933s/iter; left time: 8993.7003s\n",
      "2699it [13:12,  3.46it/s]\titers: 2700, epoch: 2 | loss: 0.2304187\n",
      "\tspeed: 0.2881s/iter; left time: 8806.9660s\n",
      "2799it [13:41,  3.48it/s]\titers: 2800, epoch: 2 | loss: 0.4437835\n",
      "\tspeed: 0.2889s/iter; left time: 8800.7337s\n",
      "2899it [14:10,  3.42it/s]\titers: 2900, epoch: 2 | loss: 0.6011753\n",
      "\tspeed: 0.2903s/iter; left time: 8815.9161s\n",
      "2999it [14:39,  3.45it/s]\titers: 3000, epoch: 2 | loss: 0.4034798\n",
      "\tspeed: 0.2898s/iter; left time: 8771.3966s\n",
      "3099it [15:08,  3.46it/s]\titers: 3100, epoch: 2 | loss: 0.5375018\n",
      "\tspeed: 0.2895s/iter; left time: 8733.7095s\n",
      "3199it [15:37,  3.43it/s]\titers: 3200, epoch: 2 | loss: 0.3775550\n",
      "\tspeed: 0.2881s/iter; left time: 8660.2776s\n",
      "3299it [16:06,  3.47it/s]\titers: 3300, epoch: 2 | loss: 0.4709841\n",
      "\tspeed: 0.2904s/iter; left time: 8701.1366s\n",
      "3399it [16:35,  3.47it/s]\titers: 3400, epoch: 2 | loss: 0.4131920\n",
      "\tspeed: 0.2898s/iter; left time: 8656.1065s\n",
      "3499it [17:04,  3.35it/s]\titers: 3500, epoch: 2 | loss: 0.2934077\n",
      "\tspeed: 0.2893s/iter; left time: 8610.6713s\n",
      "3599it [17:33,  3.48it/s]\titers: 3600, epoch: 2 | loss: 0.4881082\n",
      "\tspeed: 0.2950s/iter; left time: 8750.6776s\n",
      "3696it [18:01,  3.42it/s]\n",
      "Epoch: 2 cost time: 1081.9829478263855\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:25,  8.39it/s]\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:22,  8.54it/s]\n",
      "Epoch: 2 | Train Loss: 0.4292832 Vali Loss: 0.7862104 Test Loss: 0.4409170 MAE Loss: 0.4388770\n",
      "Updating learning rate to 2.000000000000001e-06\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "99it [00:31,  3.32it/s]\titers: 100, epoch: 3 | loss: 0.5167665\n",
      "\tspeed: 3.6761s/iter; left time: 108330.8710s\n",
      "199it [01:01,  3.29it/s]\titers: 200, epoch: 3 | loss: 0.3223612\n",
      "\tspeed: 0.3027s/iter; left time: 8889.2260s\n",
      "299it [01:32,  3.32it/s]\titers: 300, epoch: 3 | loss: 0.4549427\n",
      "\tspeed: 0.3020s/iter; left time: 8839.5850s\n",
      "399it [02:02,  3.34it/s]\titers: 400, epoch: 3 | loss: 0.2301953\n",
      "\tspeed: 0.3038s/iter; left time: 8861.6100s\n",
      "499it [02:32,  3.31it/s]\titers: 500, epoch: 3 | loss: 0.5878772\n",
      "\tspeed: 0.3024s/iter; left time: 8791.1560s\n",
      "599it [03:02,  3.34it/s]\titers: 600, epoch: 3 | loss: 0.4042051\n",
      "\tspeed: 0.3031s/iter; left time: 8779.6908s\n",
      "699it [03:33,  3.34it/s]\titers: 700, epoch: 3 | loss: 0.3304503\n",
      "\tspeed: 0.3009s/iter; left time: 8686.2437s\n",
      "799it [04:03,  3.29it/s]\titers: 800, epoch: 3 | loss: 0.3292032\n",
      "\tspeed: 0.3020s/iter; left time: 8689.4677s\n",
      "899it [04:33,  3.33it/s]\titers: 900, epoch: 3 | loss: 0.4045283\n",
      "\tspeed: 0.3017s/iter; left time: 8648.2728s\n",
      "999it [05:03,  3.31it/s]\titers: 1000, epoch: 3 | loss: 0.7422041\n",
      "\tspeed: 0.3037s/iter; left time: 8676.8742s\n",
      "1099it [05:34,  3.27it/s]\titers: 1100, epoch: 3 | loss: 0.3153823\n",
      "\tspeed: 0.3024s/iter; left time: 8608.9213s\n",
      "1199it [06:04,  3.31it/s]\titers: 1200, epoch: 3 | loss: 0.3245410\n",
      "\tspeed: 0.3021s/iter; left time: 8569.2314s\n",
      "1299it [06:34,  3.33it/s]\titers: 1300, epoch: 3 | loss: 0.2516571\n",
      "\tspeed: 0.3014s/iter; left time: 8521.5511s\n",
      "1399it [07:04,  3.33it/s]\titers: 1400, epoch: 3 | loss: 0.6026422\n",
      "\tspeed: 0.3010s/iter; left time: 8479.6494s\n",
      "1499it [07:34,  3.35it/s]\titers: 1500, epoch: 3 | loss: 0.2744479\n",
      "\tspeed: 0.3003s/iter; left time: 8428.9316s\n",
      "1599it [08:04,  3.31it/s]\titers: 1600, epoch: 3 | loss: 0.3917826\n",
      "\tspeed: 0.3024s/iter; left time: 8459.0613s\n",
      "1699it [08:34,  3.34it/s]\titers: 1700, epoch: 3 | loss: 0.2827588\n",
      "\tspeed: 0.3018s/iter; left time: 8412.0433s\n",
      "1799it [09:04,  3.32it/s]\titers: 1800, epoch: 3 | loss: 0.5580993\n",
      "\tspeed: 0.3006s/iter; left time: 8347.3348s\n",
      "1899it [09:35,  3.35it/s]\titers: 1900, epoch: 3 | loss: 0.3090303\n",
      "\tspeed: 0.3042s/iter; left time: 8415.7235s\n",
      "1999it [10:05,  3.32it/s]\titers: 2000, epoch: 3 | loss: 0.3394394\n",
      "\tspeed: 0.3005s/iter; left time: 8284.0957s\n",
      "2099it [10:35,  3.32it/s]\titers: 2100, epoch: 3 | loss: 0.3770523\n",
      "\tspeed: 0.3008s/iter; left time: 8261.3671s\n",
      "2199it [11:05,  3.32it/s]\titers: 2200, epoch: 3 | loss: 0.7069287\n",
      "\tspeed: 0.3024s/iter; left time: 8275.4679s\n",
      "2299it [11:35,  3.34it/s]\titers: 2300, epoch: 3 | loss: 0.3561049\n",
      "\tspeed: 0.3017s/iter; left time: 8226.7529s\n",
      "2399it [12:05,  3.31it/s]\titers: 2400, epoch: 3 | loss: 0.3736746\n",
      "\tspeed: 0.2981s/iter; left time: 8099.8660s\n",
      "2499it [12:35,  3.33it/s]\titers: 2500, epoch: 3 | loss: 0.6480546\n",
      "\tspeed: 0.3024s/iter; left time: 8184.3221s\n",
      "2599it [13:05,  3.37it/s]\titers: 2600, epoch: 3 | loss: 0.3247092\n",
      "\tspeed: 0.2992s/iter; left time: 8068.1824s\n",
      "2699it [13:35,  3.33it/s]\titers: 2700, epoch: 3 | loss: 0.2574255\n",
      "\tspeed: 0.2985s/iter; left time: 8021.3134s\n",
      "2799it [14:05,  3.30it/s]\titers: 2800, epoch: 3 | loss: 0.3283206\n",
      "\tspeed: 0.2988s/iter; left time: 7997.5953s\n",
      "2899it [14:35,  3.33it/s]\titers: 2900, epoch: 3 | loss: 0.3037384\n",
      "\tspeed: 0.3001s/iter; left time: 8003.4005s\n",
      "2999it [15:05,  3.34it/s]\titers: 3000, epoch: 3 | loss: 0.4580655\n",
      "\tspeed: 0.2993s/iter; left time: 7951.7443s\n",
      "3099it [15:35,  3.20it/s]\titers: 3100, epoch: 3 | loss: 0.5683362\n",
      "\tspeed: 0.2988s/iter; left time: 7909.6108s\n",
      "3199it [16:05,  3.33it/s]\titers: 3200, epoch: 3 | loss: 0.3294116\n",
      "\tspeed: 0.2982s/iter; left time: 7863.6721s\n",
      "3299it [16:35,  3.32it/s]\titers: 3300, epoch: 3 | loss: 0.2959902\n",
      "\tspeed: 0.3003s/iter; left time: 7887.9425s\n",
      "3399it [17:05,  3.25it/s]\titers: 3400, epoch: 3 | loss: 0.4608560\n",
      "\tspeed: 0.3007s/iter; left time: 7869.5964s\n",
      "3499it [17:36,  3.28it/s]\titers: 3500, epoch: 3 | loss: 0.4421138\n",
      "\tspeed: 0.3074s/iter; left time: 8014.1857s\n",
      "3599it [18:06,  3.32it/s]\titers: 3600, epoch: 3 | loss: 0.4569879\n",
      "\tspeed: 0.3025s/iter; left time: 7855.9928s\n",
      "3696it [18:36,  3.31it/s]\n",
      "Epoch: 3 cost time: 1116.2490074634552\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:29,  8.16it/s]\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:36,  7.78it/s]\n",
      "Epoch: 3 | Train Loss: 0.4085354 Vali Loss: 0.7430226 Test Loss: 0.4186530 MAE Loss: 0.4255587\n",
      "Updating learning rate to 1.0000000000000006e-06\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "99it [00:30,  3.23it/s]\titers: 100, epoch: 4 | loss: 0.2465249\n",
      "\tspeed: 3.8671s/iter; left time: 99666.0544s\n",
      "199it [01:00,  3.33it/s]\titers: 200, epoch: 4 | loss: 0.3350238\n",
      "\tspeed: 0.3019s/iter; left time: 7751.4088s\n",
      "299it [01:30,  3.30it/s]\titers: 300, epoch: 4 | loss: 0.4936987\n",
      "\tspeed: 0.3008s/iter; left time: 7692.9455s\n",
      "399it [02:00,  3.39it/s]\titers: 400, epoch: 4 | loss: 0.3637270\n",
      "\tspeed: 0.2992s/iter; left time: 7620.8545s\n",
      "499it [02:30,  3.44it/s]\titers: 500, epoch: 4 | loss: 0.5892675\n",
      "\tspeed: 0.2968s/iter; left time: 7530.9157s\n",
      "599it [02:59,  3.39it/s]\titers: 600, epoch: 4 | loss: 0.4300954\n",
      "\tspeed: 0.2914s/iter; left time: 7363.7675s\n",
      "699it [03:28,  3.47it/s]\titers: 700, epoch: 4 | loss: 0.5322685\n",
      "\tspeed: 0.2932s/iter; left time: 7381.1577s\n",
      "799it [03:58,  3.39it/s]\titers: 800, epoch: 4 | loss: 0.3521198\n",
      "\tspeed: 0.2949s/iter; left time: 7393.9715s\n",
      "899it [04:27,  3.39it/s]\titers: 900, epoch: 4 | loss: 0.3119884\n",
      "\tspeed: 0.2970s/iter; left time: 7416.2539s\n",
      "999it [04:57,  3.40it/s]\titers: 1000, epoch: 4 | loss: 0.3068696\n",
      "\tspeed: 0.2943s/iter; left time: 7319.9890s\n",
      "1099it [05:26,  3.48it/s]\titers: 1100, epoch: 4 | loss: 0.3466788\n",
      "\tspeed: 0.2925s/iter; left time: 7246.9633s\n",
      "1199it [05:56,  3.42it/s]\titers: 1200, epoch: 4 | loss: 0.4348427\n",
      "\tspeed: 0.2965s/iter; left time: 7316.4160s\n",
      "1299it [06:25,  3.41it/s]\titers: 1300, epoch: 4 | loss: 0.3242353\n",
      "\tspeed: 0.2927s/iter; left time: 7191.8042s\n",
      "1399it [06:55,  3.37it/s]\titers: 1400, epoch: 4 | loss: 0.2089174\n",
      "\tspeed: 0.2946s/iter; left time: 7208.6740s\n",
      "1499it [07:24,  3.38it/s]\titers: 1500, epoch: 4 | loss: 0.3483659\n",
      "\tspeed: 0.2943s/iter; left time: 7174.0243s\n",
      "1599it [07:53,  3.41it/s]\titers: 1600, epoch: 4 | loss: 0.4420883\n",
      "\tspeed: 0.2940s/iter; left time: 7135.6216s\n",
      "1699it [08:23,  3.38it/s]\titers: 1700, epoch: 4 | loss: 0.3223463\n",
      "\tspeed: 0.2961s/iter; left time: 7156.7610s\n",
      "1799it [08:52,  3.36it/s]\titers: 1800, epoch: 4 | loss: 0.2448355\n",
      "\tspeed: 0.2942s/iter; left time: 7082.0773s\n",
      "1899it [09:22,  3.31it/s]\titers: 1900, epoch: 4 | loss: 0.3738885\n",
      "\tspeed: 0.2931s/iter; left time: 7026.9542s\n",
      "1999it [09:51,  3.34it/s]\titers: 2000, epoch: 4 | loss: 0.3723390\n",
      "\tspeed: 0.2973s/iter; left time: 7098.3416s\n",
      "2099it [10:21,  3.37it/s]\titers: 2100, epoch: 4 | loss: 0.4102765\n",
      "\tspeed: 0.2997s/iter; left time: 7125.8833s\n",
      "2199it [10:51,  3.46it/s]\titers: 2200, epoch: 4 | loss: 0.4813679\n",
      "\tspeed: 0.2919s/iter; left time: 6909.4072s\n",
      "2299it [11:20,  3.42it/s]\titers: 2300, epoch: 4 | loss: 0.4951153\n",
      "\tspeed: 0.2923s/iter; left time: 6891.5164s\n",
      "2399it [11:50,  3.32it/s]\titers: 2400, epoch: 4 | loss: 0.7173523\n",
      "\tspeed: 0.2999s/iter; left time: 7040.2395s\n",
      "2499it [12:19,  3.43it/s]\titers: 2500, epoch: 4 | loss: 0.3120330\n",
      "\tspeed: 0.2954s/iter; left time: 6904.2265s\n",
      "2599it [12:49,  3.37it/s]\titers: 2600, epoch: 4 | loss: 0.6296433\n",
      "\tspeed: 0.2939s/iter; left time: 6839.2421s\n",
      "2699it [13:18,  3.41it/s]\titers: 2700, epoch: 4 | loss: 0.4606722\n",
      "\tspeed: 0.2950s/iter; left time: 6837.0631s\n",
      "2799it [13:48,  3.37it/s]\titers: 2800, epoch: 4 | loss: 0.1789999\n",
      "\tspeed: 0.2927s/iter; left time: 6752.6673s\n",
      "2899it [14:17,  3.35it/s]\titers: 2900, epoch: 4 | loss: 0.3508358\n",
      "\tspeed: 0.2967s/iter; left time: 6817.1219s\n",
      "2999it [14:47,  3.48it/s]\titers: 3000, epoch: 4 | loss: 0.2592826\n",
      "\tspeed: 0.2983s/iter; left time: 6824.0638s\n",
      "3099it [15:16,  3.47it/s]\titers: 3100, epoch: 4 | loss: 0.4341192\n",
      "\tspeed: 0.2918s/iter; left time: 6644.5414s\n",
      "3199it [15:46,  3.40it/s]\titers: 3200, epoch: 4 | loss: 0.4806348\n",
      "\tspeed: 0.2935s/iter; left time: 6653.4991s\n",
      "3299it [16:15,  3.41it/s]\titers: 3300, epoch: 4 | loss: 0.3919269\n",
      "\tspeed: 0.2935s/iter; left time: 6625.9017s\n",
      "3399it [16:44,  3.37it/s]\titers: 3400, epoch: 4 | loss: 0.5851271\n",
      "\tspeed: 0.2948s/iter; left time: 6624.3284s\n",
      "3499it [17:14,  3.48it/s]\titers: 3500, epoch: 4 | loss: 0.4687200\n",
      "\tspeed: 0.2926s/iter; left time: 6546.1286s\n",
      "3599it [17:43,  3.33it/s]\titers: 3600, epoch: 4 | loss: 0.3585695\n",
      "\tspeed: 0.2938s/iter; left time: 6544.1704s\n",
      "3696it [18:12,  3.38it/s]\n",
      "Epoch: 4 cost time: 1092.9582121372223\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:23,  8.50it/s]\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:23,  8.51it/s]\n",
      "Epoch: 4 | Train Loss: 0.4017689 Vali Loss: 0.7453512 Test Loss: 0.4144848 MAE Loss: 0.4230592\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 5.000000000000003e-07\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "99it [00:30,  3.40it/s]\titers: 100, epoch: 5 | loss: 0.6209991\n",
      "\tspeed: 3.4593s/iter; left time: 76370.2490s\n",
      "199it [00:59,  3.46it/s]\titers: 200, epoch: 5 | loss: 0.2954062\n",
      "\tspeed: 0.2927s/iter; left time: 6433.2529s\n",
      "299it [01:28,  3.36it/s]\titers: 300, epoch: 5 | loss: 0.2738439\n",
      "\tspeed: 0.2941s/iter; left time: 6434.0525s\n",
      "399it [01:58,  3.39it/s]\titers: 400, epoch: 5 | loss: 0.5073515\n",
      "\tspeed: 0.2936s/iter; left time: 6393.3343s\n",
      "499it [02:27,  3.47it/s]\titers: 500, epoch: 5 | loss: 0.3526617\n",
      "\tspeed: 0.2924s/iter; left time: 6337.9555s\n",
      "599it [02:56,  3.44it/s]\titers: 600, epoch: 5 | loss: 0.6899012\n",
      "\tspeed: 0.2927s/iter; left time: 6314.5670s\n",
      "699it [03:25,  3.43it/s]\titers: 700, epoch: 5 | loss: 0.3673239\n",
      "\tspeed: 0.2933s/iter; left time: 6299.5674s\n",
      "799it [03:55,  3.45it/s]\titers: 800, epoch: 5 | loss: 0.3816351\n",
      "\tspeed: 0.2930s/iter; left time: 6263.8447s\n",
      "899it [04:24,  3.36it/s]\titers: 900, epoch: 5 | loss: 0.4481589\n",
      "\tspeed: 0.2961s/iter; left time: 6299.9473s\n",
      "999it [04:54,  3.32it/s]\titers: 1000, epoch: 5 | loss: 0.6298306\n",
      "\tspeed: 0.2996s/iter; left time: 6343.7820s\n",
      "1099it [05:24,  3.50it/s]\titers: 1100, epoch: 5 | loss: 0.3467148\n",
      "\tspeed: 0.2931s/iter; left time: 6177.3989s\n",
      "1199it [05:53,  3.43it/s]\titers: 1200, epoch: 5 | loss: 0.3897614\n",
      "\tspeed: 0.2918s/iter; left time: 6120.2196s\n",
      "1299it [06:22,  3.40it/s]\titers: 1300, epoch: 5 | loss: 0.4302413\n",
      "\tspeed: 0.2916s/iter; left time: 6088.5257s\n",
      "1399it [06:51,  3.41it/s]\titers: 1400, epoch: 5 | loss: 0.2937129\n",
      "\tspeed: 0.2882s/iter; left time: 5987.4121s\n",
      "1499it [07:20,  3.40it/s]\titers: 1500, epoch: 5 | loss: 0.3857278\n",
      "\tspeed: 0.2934s/iter; left time: 6066.2627s\n",
      "1599it [07:49,  3.41it/s]\titers: 1600, epoch: 5 | loss: 0.2630356\n",
      "\tspeed: 0.2936s/iter; left time: 6041.8046s\n",
      "1699it [08:19,  3.41it/s]\titers: 1700, epoch: 5 | loss: 0.3442020\n",
      "\tspeed: 0.2934s/iter; left time: 6007.4945s\n",
      "1799it [08:48,  3.49it/s]\titers: 1800, epoch: 5 | loss: 0.4048681\n",
      "\tspeed: 0.2884s/iter; left time: 5877.7149s\n",
      "1899it [09:17,  3.48it/s]\titers: 1900, epoch: 5 | loss: 0.4496201\n",
      "\tspeed: 0.2886s/iter; left time: 5851.8710s\n",
      "1999it [09:45,  3.47it/s]\titers: 2000, epoch: 5 | loss: 0.3479216\n",
      "\tspeed: 0.2878s/iter; left time: 5806.9376s\n",
      "2099it [10:14,  3.45it/s]\titers: 2100, epoch: 5 | loss: 0.3740781\n",
      "\tspeed: 0.2887s/iter; left time: 5795.8876s\n",
      "2199it [10:43,  3.37it/s]\titers: 2200, epoch: 5 | loss: 0.3394291\n",
      "\tspeed: 0.2905s/iter; left time: 5803.6927s\n",
      "2299it [11:13,  3.39it/s]\titers: 2300, epoch: 5 | loss: 0.4064813\n",
      "\tspeed: 0.2946s/iter; left time: 5855.6240s\n",
      "2399it [11:42,  3.42it/s]\titers: 2400, epoch: 5 | loss: 0.3873379\n",
      "\tspeed: 0.2940s/iter; left time: 5815.0878s\n",
      "2499it [12:11,  3.40it/s]\titers: 2500, epoch: 5 | loss: 0.3810628\n",
      "\tspeed: 0.2932s/iter; left time: 5769.7528s\n",
      "2599it [12:41,  3.42it/s]\titers: 2600, epoch: 5 | loss: 0.2944186\n",
      "\tspeed: 0.2928s/iter; left time: 5732.0129s\n",
      "2699it [13:10,  3.43it/s]\titers: 2700, epoch: 5 | loss: 0.3558396\n",
      "\tspeed: 0.2950s/iter; left time: 5745.0043s\n",
      "2799it [13:40,  3.42it/s]\titers: 2800, epoch: 5 | loss: 0.3572732\n",
      "\tspeed: 0.2941s/iter; left time: 5699.1730s\n",
      "2899it [14:09,  3.42it/s]\titers: 2900, epoch: 5 | loss: 0.4306317\n",
      "\tspeed: 0.2947s/iter; left time: 5680.9500s\n",
      "2999it [14:39,  3.41it/s]\titers: 3000, epoch: 5 | loss: 0.3622509\n",
      "\tspeed: 0.2942s/iter; left time: 5641.9318s\n",
      "3099it [15:08,  3.45it/s]\titers: 3100, epoch: 5 | loss: 0.2476974\n",
      "\tspeed: 0.2954s/iter; left time: 5636.2210s\n",
      "3199it [15:37,  3.34it/s]\titers: 3200, epoch: 5 | loss: 0.3394744\n",
      "\tspeed: 0.2931s/iter; left time: 5561.8606s\n",
      "3299it [16:07,  3.42it/s]\titers: 3300, epoch: 5 | loss: 0.4363036\n",
      "\tspeed: 0.2948s/iter; left time: 5565.5819s\n",
      "3399it [16:36,  3.42it/s]\titers: 3400, epoch: 5 | loss: 0.3475056\n",
      "\tspeed: 0.2945s/iter; left time: 5529.6131s\n",
      "3499it [17:06,  3.40it/s]\titers: 3500, epoch: 5 | loss: 0.4897146\n",
      "\tspeed: 0.2929s/iter; left time: 5470.0543s\n",
      "3599it [17:35,  3.42it/s]\titers: 3600, epoch: 5 | loss: 0.3552352\n",
      "\tspeed: 0.2927s/iter; left time: 5438.3540s\n",
      "3696it [18:04,  3.41it/s]\n",
      "Epoch: 5 cost time: 1084.188597202301\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:22,  8.55it/s]\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:26,  8.31it/s]\n",
      "Epoch: 5 | Train Loss: 0.3993753 Vali Loss: 0.7429949 Test Loss: 0.4131282 MAE Loss: 0.4218396\n",
      "Updating learning rate to 2.5000000000000015e-07\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "99it [00:30,  3.41it/s]\titers: 100, epoch: 6 | loss: 0.3157395\n",
      "\tspeed: 3.7050s/iter; left time: 68101.5102s\n",
      "199it [00:58,  3.49it/s]\titers: 200, epoch: 6 | loss: 0.5382771\n",
      "\tspeed: 0.2891s/iter; left time: 5284.2690s\n",
      "299it [01:28,  3.45it/s]\titers: 300, epoch: 6 | loss: 0.3670421\n",
      "\tspeed: 0.2904s/iter; left time: 5279.6602s\n",
      "399it [01:57,  3.40it/s]\titers: 400, epoch: 6 | loss: 0.3077852\n",
      "\tspeed: 0.2944s/iter; left time: 5323.6753s\n",
      "499it [02:26,  3.45it/s]\titers: 500, epoch: 6 | loss: 0.2530380\n",
      "\tspeed: 0.2953s/iter; left time: 5309.0652s\n",
      "599it [02:56,  3.40it/s]\titers: 600, epoch: 6 | loss: 0.4161499\n",
      "\tspeed: 0.2948s/iter; left time: 5270.6894s\n",
      "699it [03:26,  3.31it/s]\titers: 700, epoch: 6 | loss: 0.2846807\n",
      "\tspeed: 0.2971s/iter; left time: 5282.5965s\n",
      "799it [03:55,  3.38it/s]\titers: 800, epoch: 6 | loss: 0.2713862\n",
      "\tspeed: 0.2948s/iter; left time: 5212.6372s\n",
      "899it [04:25,  3.43it/s]\titers: 900, epoch: 6 | loss: 0.6110048\n",
      "\tspeed: 0.2978s/iter; left time: 5236.1597s\n",
      "999it [04:54,  3.42it/s]\titers: 1000, epoch: 6 | loss: 0.3600900\n",
      "\tspeed: 0.2936s/iter; left time: 5132.9493s\n",
      "1099it [05:24,  3.34it/s]\titers: 1100, epoch: 6 | loss: 0.2446388\n",
      "\tspeed: 0.2983s/iter; left time: 5184.6572s\n",
      "1199it [05:54,  3.43it/s]\titers: 1200, epoch: 6 | loss: 0.2501777\n",
      "\tspeed: 0.2952s/iter; left time: 5101.2800s\n",
      "1299it [06:23,  3.28it/s]\titers: 1300, epoch: 6 | loss: 0.2316504\n",
      "\tspeed: 0.2971s/iter; left time: 5104.8891s\n",
      "1399it [06:53,  3.34it/s]\titers: 1400, epoch: 6 | loss: 0.5636105\n",
      "\tspeed: 0.2999s/iter; left time: 5122.6854s\n",
      "1499it [07:23,  3.45it/s]\titers: 1500, epoch: 6 | loss: 0.4258219\n",
      "\tspeed: 0.2990s/iter; left time: 5078.0168s\n",
      "1599it [07:53,  3.35it/s]\titers: 1600, epoch: 6 | loss: 0.3493955\n",
      "\tspeed: 0.2985s/iter; left time: 5039.4266s\n",
      "1699it [08:23,  3.38it/s]\titers: 1700, epoch: 6 | loss: 0.3650035\n",
      "\tspeed: 0.2998s/iter; left time: 5030.6292s\n",
      "1799it [08:53,  3.35it/s]\titers: 1800, epoch: 6 | loss: 0.3664434\n",
      "\tspeed: 0.3005s/iter; left time: 5012.8942s\n",
      "1899it [09:23,  3.37it/s]\titers: 1900, epoch: 6 | loss: 0.2660965\n",
      "\tspeed: 0.3004s/iter; left time: 4981.4531s\n",
      "1999it [09:53,  3.46it/s]\titers: 2000, epoch: 6 | loss: 0.4566249\n",
      "\tspeed: 0.2974s/iter; left time: 4901.9272s\n",
      "2099it [10:23,  3.29it/s]\titers: 2100, epoch: 6 | loss: 0.2394730\n",
      "\tspeed: 0.2993s/iter; left time: 4902.4561s\n",
      "2199it [10:53,  3.35it/s]\titers: 2200, epoch: 6 | loss: 0.6284758\n",
      "\tspeed: 0.3000s/iter; left time: 4884.9494s\n",
      "2299it [11:23,  3.28it/s]\titers: 2300, epoch: 6 | loss: 0.3014846\n",
      "\tspeed: 0.3004s/iter; left time: 4860.8455s\n",
      "2399it [11:53,  3.33it/s]\titers: 2400, epoch: 6 | loss: 0.4358779\n",
      "\tspeed: 0.3003s/iter; left time: 4829.8611s\n",
      "2499it [12:23,  3.28it/s]\titers: 2500, epoch: 6 | loss: 0.2177922\n",
      "\tspeed: 0.3008s/iter; left time: 4807.8519s\n",
      "2599it [12:53,  3.36it/s]\titers: 2600, epoch: 6 | loss: 0.3447635\n",
      "\tspeed: 0.2972s/iter; left time: 4720.6112s\n",
      "2699it [13:23,  3.33it/s]\titers: 2700, epoch: 6 | loss: 0.5108984\n",
      "\tspeed: 0.2982s/iter; left time: 4706.6300s\n",
      "2799it [13:53,  3.32it/s]\titers: 2800, epoch: 6 | loss: 0.3568596\n",
      "\tspeed: 0.3011s/iter; left time: 4721.0238s\n",
      "2899it [14:23,  3.35it/s]\titers: 2900, epoch: 6 | loss: 0.4512796\n",
      "\tspeed: 0.3005s/iter; left time: 4682.0288s\n",
      "2999it [14:53,  3.35it/s]\titers: 3000, epoch: 6 | loss: 0.3099474\n",
      "\tspeed: 0.3007s/iter; left time: 4655.6914s\n",
      "3099it [15:23,  3.23it/s]\titers: 3100, epoch: 6 | loss: 0.4965122\n",
      "\tspeed: 0.3008s/iter; left time: 4626.6862s\n",
      "3199it [15:54,  3.26it/s]\titers: 3200, epoch: 6 | loss: 0.4687258\n",
      "\tspeed: 0.3063s/iter; left time: 4680.5641s\n",
      "3299it [16:24,  3.22it/s]\titers: 3300, epoch: 6 | loss: 0.2242674\n",
      "\tspeed: 0.3043s/iter; left time: 4620.1694s\n",
      "3399it [16:55,  3.20it/s]\titers: 3400, epoch: 6 | loss: 0.9264178\n",
      "\tspeed: 0.3123s/iter; left time: 4709.8211s\n",
      "3499it [17:26,  3.33it/s]\titers: 3500, epoch: 6 | loss: 0.2716328\n",
      "\tspeed: 0.3069s/iter; left time: 4597.9134s\n",
      "3599it [17:56,  3.43it/s]\titers: 3600, epoch: 6 | loss: 0.5224289\n",
      "\tspeed: 0.3008s/iter; left time: 4475.9362s\n",
      "3696it [18:25,  3.34it/s]\n",
      "Epoch: 6 cost time: 1105.40536403656\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:22,  8.56it/s]\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:24,  8.43it/s]\n",
      "Epoch: 6 | Train Loss: 0.3979378 Vali Loss: 0.7307933 Test Loss: 0.4103508 MAE Loss: 0.4200651\n",
      "Updating learning rate to 1.2500000000000007e-07\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "99it [00:30,  3.27it/s]\titers: 100, epoch: 7 | loss: 0.5923057\n",
      "\tspeed: 3.6109s/iter; left time: 53026.6128s\n",
      "199it [00:59,  3.42it/s]\titers: 200, epoch: 7 | loss: 0.2901328\n",
      "\tspeed: 0.2931s/iter; left time: 4274.7027s\n",
      "299it [01:29,  3.43it/s]\titers: 300, epoch: 7 | loss: 0.3301842\n",
      "\tspeed: 0.2947s/iter; left time: 4268.1267s\n",
      "399it [01:58,  3.41it/s]\titers: 400, epoch: 7 | loss: 0.4742263\n",
      "\tspeed: 0.2927s/iter; left time: 4210.0363s\n",
      "499it [02:28,  3.42it/s]\titers: 500, epoch: 7 | loss: 0.2384150\n",
      "\tspeed: 0.2942s/iter; left time: 4202.6814s\n",
      "599it [02:57,  3.42it/s]\titers: 600, epoch: 7 | loss: 0.5855042\n",
      "\tspeed: 0.2954s/iter; left time: 4189.7840s\n",
      "699it [03:27,  3.41it/s]\titers: 700, epoch: 7 | loss: 0.1950468\n",
      "\tspeed: 0.2944s/iter; left time: 4146.8737s\n",
      "799it [03:56,  3.42it/s]\titers: 800, epoch: 7 | loss: 0.5418211\n",
      "\tspeed: 0.2937s/iter; left time: 4107.6334s\n",
      "899it [04:26,  3.40it/s]\titers: 900, epoch: 7 | loss: 0.7012662\n",
      "\tspeed: 0.2960s/iter; left time: 4110.3419s\n",
      "999it [04:55,  3.38it/s]\titers: 1000, epoch: 7 | loss: 0.5783727\n",
      "\tspeed: 0.2980s/iter; left time: 4108.4371s\n",
      "1099it [05:26,  3.40it/s]\titers: 1100, epoch: 7 | loss: 0.4097408\n",
      "\tspeed: 0.3041s/iter; left time: 4160.9640s\n",
      "1199it [05:55,  3.40it/s]\titers: 1200, epoch: 7 | loss: 0.4426219\n",
      "\tspeed: 0.2928s/iter; left time: 3978.0797s\n",
      "1299it [06:25,  3.38it/s]\titers: 1300, epoch: 7 | loss: 0.5334916\n",
      "\tspeed: 0.3022s/iter; left time: 4075.8010s\n",
      "1399it [06:55,  3.42it/s]\titers: 1400, epoch: 7 | loss: 0.3874390\n",
      "\tspeed: 0.2960s/iter; left time: 3962.5154s\n",
      "1499it [07:25,  3.39it/s]\titers: 1500, epoch: 7 | loss: 0.2887952\n",
      "\tspeed: 0.3004s/iter; left time: 3991.0965s\n",
      "1599it [07:55,  3.39it/s]\titers: 1600, epoch: 7 | loss: 0.5741284\n",
      "\tspeed: 0.2969s/iter; left time: 3914.3311s\n",
      "1699it [08:24,  3.42it/s]\titers: 1700, epoch: 7 | loss: 0.2870657\n",
      "\tspeed: 0.2947s/iter; left time: 3855.6484s\n",
      "1799it [08:53,  3.42it/s]\titers: 1800, epoch: 7 | loss: 0.2393513\n",
      "\tspeed: 0.2932s/iter; left time: 3807.1932s\n",
      "1899it [09:23,  3.42it/s]\titers: 1900, epoch: 7 | loss: 0.3653481\n",
      "\tspeed: 0.2928s/iter; left time: 3772.4164s\n",
      "1999it [09:52,  3.41it/s]\titers: 2000, epoch: 7 | loss: 0.4814088\n",
      "\tspeed: 0.2939s/iter; left time: 3756.9293s\n",
      "2099it [10:22,  3.43it/s]\titers: 2100, epoch: 7 | loss: 0.5428428\n",
      "\tspeed: 0.2961s/iter; left time: 3755.5943s\n",
      "2199it [10:51,  3.32it/s]\titers: 2200, epoch: 7 | loss: 0.7189175\n",
      "\tspeed: 0.2966s/iter; left time: 3732.5643s\n",
      "2299it [11:22,  3.27it/s]\titers: 2300, epoch: 7 | loss: 0.4681046\n",
      "\tspeed: 0.3051s/iter; left time: 3808.7615s\n",
      "2399it [11:52,  3.30it/s]\titers: 2400, epoch: 7 | loss: 0.3176191\n",
      "\tspeed: 0.3050s/iter; left time: 3777.9262s\n",
      "2499it [12:23,  3.26it/s]\titers: 2500, epoch: 7 | loss: 0.4979253\n",
      "\tspeed: 0.3067s/iter; left time: 3768.1621s\n",
      "2599it [12:55,  3.31it/s]\titers: 2600, epoch: 7 | loss: 0.3690926\n",
      "\tspeed: 0.3246s/iter; left time: 3955.3223s\n",
      "2699it [13:25,  3.42it/s]\titers: 2700, epoch: 7 | loss: 0.3509083\n",
      "\tspeed: 0.2988s/iter; left time: 3610.9828s\n",
      "2799it [13:55,  3.28it/s]\titers: 2800, epoch: 7 | loss: 0.6098956\n",
      "\tspeed: 0.2989s/iter; left time: 3582.1492s\n",
      "2899it [14:26,  3.21it/s]\titers: 2900, epoch: 7 | loss: 0.2018496\n",
      "\tspeed: 0.3039s/iter; left time: 3612.0518s\n",
      "2999it [14:56,  3.40it/s]\titers: 3000, epoch: 7 | loss: 0.3559167\n",
      "\tspeed: 0.3006s/iter; left time: 3542.7239s\n",
      "3099it [15:25,  3.32it/s]\titers: 3100, epoch: 7 | loss: 0.2895813\n",
      "\tspeed: 0.2960s/iter; left time: 3458.8949s\n",
      "3199it [15:56,  3.31it/s]\titers: 3200, epoch: 7 | loss: 0.5643123\n",
      "\tspeed: 0.3031s/iter; left time: 3511.2711s\n",
      "3299it [16:26,  3.31it/s]\titers: 3300, epoch: 7 | loss: 0.2273063\n",
      "\tspeed: 0.3028s/iter; left time: 3477.7061s\n",
      "3399it [16:56,  3.34it/s]\titers: 3400, epoch: 7 | loss: 0.3944189\n",
      "\tspeed: 0.3017s/iter; left time: 3434.5601s\n",
      "3499it [17:26,  3.29it/s]\titers: 3500, epoch: 7 | loss: 0.4687324\n",
      "\tspeed: 0.3047s/iter; left time: 3437.9967s\n",
      "3599it [17:57,  3.21it/s]\titers: 3600, epoch: 7 | loss: 0.4075987\n",
      "\tspeed: 0.3078s/iter; left time: 3443.0859s\n",
      "3696it [18:27,  3.34it/s]\n",
      "Epoch: 7 cost time: 1107.4692313671112\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:21,  8.61it/s]\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:21,  8.60it/s]\n",
      "Epoch: 7 | Train Loss: 0.3977759 Vali Loss: 0.7344983 Test Loss: 0.4103906 MAE Loss: 0.4202882\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 6.250000000000004e-08\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "99it [00:30,  3.39it/s]\titers: 100, epoch: 8 | loss: 0.3846644\n",
      "\tspeed: 3.4313s/iter; left time: 37706.8593s\n",
      "199it [00:59,  3.33it/s]\titers: 200, epoch: 8 | loss: 0.2964869\n",
      "\tspeed: 0.2966s/iter; left time: 3230.1654s\n",
      "299it [01:29,  3.33it/s]\titers: 300, epoch: 8 | loss: 0.2273122\n",
      "\tspeed: 0.3006s/iter; left time: 3243.3233s\n",
      "399it [01:59,  3.46it/s]\titers: 400, epoch: 8 | loss: 0.4295090\n",
      "\tspeed: 0.2946s/iter; left time: 3149.1059s\n",
      "499it [02:28,  3.41it/s]\titers: 500, epoch: 8 | loss: 0.2334808\n",
      "\tspeed: 0.2927s/iter; left time: 3099.2930s\n",
      "599it [02:58,  3.38it/s]\titers: 600, epoch: 8 | loss: 0.4489692\n",
      "\tspeed: 0.2952s/iter; left time: 3096.0340s\n",
      "699it [03:27,  3.36it/s]\titers: 700, epoch: 8 | loss: 0.3165450\n",
      "\tspeed: 0.2945s/iter; left time: 3059.1975s\n",
      "799it [03:57,  3.34it/s]\titers: 800, epoch: 8 | loss: 0.3648021\n",
      "\tspeed: 0.2994s/iter; left time: 3080.7393s\n",
      "899it [04:27,  3.32it/s]\titers: 900, epoch: 8 | loss: 0.2975907\n",
      "\tspeed: 0.3004s/iter; left time: 3060.3283s\n",
      "999it [04:57,  3.33it/s]\titers: 1000, epoch: 8 | loss: 0.4598352\n",
      "\tspeed: 0.3017s/iter; left time: 3043.5031s\n",
      "1099it [05:27,  3.33it/s]\titers: 1100, epoch: 8 | loss: 0.3856477\n",
      "\tspeed: 0.3019s/iter; left time: 3015.5441s\n",
      "1199it [05:58,  3.29it/s]\titers: 1200, epoch: 8 | loss: 0.3140411\n",
      "\tspeed: 0.3038s/iter; left time: 3004.1866s\n",
      "1299it [06:28,  3.42it/s]\titers: 1300, epoch: 8 | loss: 0.2957899\n",
      "\tspeed: 0.2970s/iter; left time: 2907.0658s\n",
      "1399it [06:57,  3.40it/s]\titers: 1400, epoch: 8 | loss: 0.6299727\n",
      "\tspeed: 0.2950s/iter; left time: 2858.6390s\n",
      "1499it [07:26,  3.31it/s]\titers: 1500, epoch: 8 | loss: 0.3445222\n",
      "\tspeed: 0.2935s/iter; left time: 2813.9182s\n",
      "1599it [07:56,  3.43it/s]\titers: 1600, epoch: 8 | loss: 0.3298544\n",
      "\tspeed: 0.2944s/iter; left time: 2793.8939s\n",
      "1699it [08:25,  3.40it/s]\titers: 1700, epoch: 8 | loss: 0.3496334\n",
      "\tspeed: 0.2937s/iter; left time: 2757.5360s\n",
      "1799it [08:55,  3.38it/s]\titers: 1800, epoch: 8 | loss: 0.3796625\n",
      "\tspeed: 0.2939s/iter; left time: 2730.4579s\n",
      "1899it [09:24,  3.38it/s]\titers: 1900, epoch: 8 | loss: 0.4724604\n",
      "\tspeed: 0.2929s/iter; left time: 2691.3322s\n",
      "1999it [09:54,  3.47it/s]\titers: 2000, epoch: 8 | loss: 0.2811897\n",
      "\tspeed: 0.2960s/iter; left time: 2689.9600s\n",
      "2099it [10:22,  3.47it/s]\titers: 2100, epoch: 8 | loss: 0.4212179\n",
      "\tspeed: 0.2888s/iter; left time: 2595.7753s\n",
      "2199it [10:51,  3.49it/s]\titers: 2200, epoch: 8 | loss: 0.2835036\n",
      "\tspeed: 0.2881s/iter; left time: 2560.6698s\n",
      "2299it [11:20,  3.44it/s]\titers: 2300, epoch: 8 | loss: 0.2581424\n",
      "\tspeed: 0.2883s/iter; left time: 2534.1515s\n",
      "2399it [11:49,  3.37it/s]\titers: 2400, epoch: 8 | loss: 0.2538091\n",
      "\tspeed: 0.2896s/iter; left time: 2516.5276s\n",
      "2499it [12:19,  3.33it/s]\titers: 2500, epoch: 8 | loss: 0.3260711\n",
      "\tspeed: 0.2997s/iter; left time: 2574.1677s\n",
      "2599it [12:49,  3.34it/s]\titers: 2600, epoch: 8 | loss: 0.3781174\n",
      "\tspeed: 0.3006s/iter; left time: 2551.5773s\n",
      "2699it [13:19,  3.32it/s]\titers: 2700, epoch: 8 | loss: 0.4109162\n",
      "\tspeed: 0.2993s/iter; left time: 2510.9920s\n",
      "2799it [13:49,  3.34it/s]\titers: 2800, epoch: 8 | loss: 0.3749569\n",
      "\tspeed: 0.3001s/iter; left time: 2487.7268s\n",
      "2899it [14:19,  3.32it/s]\titers: 2900, epoch: 8 | loss: 0.5022954\n",
      "\tspeed: 0.3001s/iter; left time: 2457.3431s\n",
      "2999it [14:49,  3.41it/s]\titers: 3000, epoch: 8 | loss: 0.3370352\n",
      "\tspeed: 0.2953s/iter; left time: 2388.6683s\n",
      "3099it [15:19,  3.32it/s]\titers: 3100, epoch: 8 | loss: 0.2542665\n",
      "\tspeed: 0.3005s/iter; left time: 2400.9294s\n",
      "3199it [15:49,  3.31it/s]\titers: 3200, epoch: 8 | loss: 0.4129049\n",
      "\tspeed: 0.3060s/iter; left time: 2413.9033s\n",
      "3299it [16:19,  3.32it/s]\titers: 3300, epoch: 8 | loss: 0.7004137\n",
      "\tspeed: 0.3003s/iter; left time: 2339.2828s\n",
      "3399it [16:49,  3.41it/s]\titers: 3400, epoch: 8 | loss: 0.4634564\n",
      "\tspeed: 0.2966s/iter; left time: 2280.4678s\n",
      "3499it [17:18,  3.41it/s]\titers: 3500, epoch: 8 | loss: 0.4618967\n",
      "\tspeed: 0.2925s/iter; left time: 2219.7900s\n",
      "3599it [17:47,  3.43it/s]\titers: 3600, epoch: 8 | loss: 0.5794383\n",
      "\tspeed: 0.2927s/iter; left time: 2191.7129s\n",
      "3696it [18:16,  3.37it/s]\n",
      "Epoch: 8 cost time: 1096.5251607894897\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:25,  8.37it/s]\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:25,  8.38it/s]\n",
      "Epoch: 8 | Train Loss: 0.3975044 Vali Loss: 0.7352212 Test Loss: 0.4104254 MAE Loss: 0.4202669\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Updating learning rate to 3.125000000000002e-08\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "99it [00:30,  3.31it/s]\titers: 100, epoch: 9 | loss: 0.5267907\n",
      "\tspeed: 3.5054s/iter; left time: 25564.9983s\n",
      "199it [01:00,  3.35it/s]\titers: 200, epoch: 9 | loss: 0.2417669\n",
      "\tspeed: 0.3001s/iter; left time: 2158.3607s\n",
      "299it [01:30,  3.34it/s]\titers: 300, epoch: 9 | loss: 0.5160916\n",
      "\tspeed: 0.3005s/iter; left time: 2131.5681s\n",
      "399it [02:00,  3.33it/s]\titers: 400, epoch: 9 | loss: 0.5255128\n",
      "\tspeed: 0.3001s/iter; left time: 2098.8897s\n",
      "499it [02:30,  3.36it/s]\titers: 500, epoch: 9 | loss: 0.3745037\n",
      "\tspeed: 0.2999s/iter; left time: 2067.0903s\n",
      "599it [03:00,  3.29it/s]\titers: 600, epoch: 9 | loss: 0.2886486\n",
      "\tspeed: 0.2995s/iter; left time: 2034.3101s\n",
      "699it [03:31,  3.32it/s]\titers: 700, epoch: 9 | loss: 0.6657383\n",
      "\tspeed: 0.3040s/iter; left time: 2034.4521s\n",
      "799it [04:01,  3.33it/s]\titers: 800, epoch: 9 | loss: 0.4314496\n",
      "\tspeed: 0.2973s/iter; left time: 1960.3254s\n",
      "899it [04:32,  3.24it/s]\titers: 900, epoch: 9 | loss: 0.5838470\n",
      "\tspeed: 0.3101s/iter; left time: 2013.2298s\n",
      "999it [05:01,  3.32it/s]\titers: 1000, epoch: 9 | loss: 0.5935739\n",
      "\tspeed: 0.2995s/iter; left time: 1914.7748s\n",
      "1099it [05:32,  3.33it/s]\titers: 1100, epoch: 9 | loss: 0.3039352\n",
      "\tspeed: 0.3019s/iter; left time: 1899.7587s\n",
      "1199it [06:02,  3.31it/s]\titers: 1200, epoch: 9 | loss: 0.2852997\n",
      "\tspeed: 0.3007s/iter; left time: 1862.3782s\n",
      "1299it [06:32,  3.18it/s]\titers: 1300, epoch: 9 | loss: 0.3034100\n",
      "\tspeed: 0.2997s/iter; left time: 1825.8494s\n",
      "1399it [07:03,  3.33it/s]\titers: 1400, epoch: 9 | loss: 0.3420649\n",
      "\tspeed: 0.3090s/iter; left time: 1851.6433s\n",
      "1499it [07:33,  3.35it/s]\titers: 1500, epoch: 9 | loss: 0.2757820\n",
      "\tspeed: 0.3003s/iter; left time: 1769.7281s\n",
      "1599it [08:03,  3.34it/s]\titers: 1600, epoch: 9 | loss: 0.6537561\n",
      "\tspeed: 0.3004s/iter; left time: 1740.4265s\n",
      "1699it [08:33,  3.34it/s]\titers: 1700, epoch: 9 | loss: 0.3320953\n",
      "\tspeed: 0.3008s/iter; left time: 1712.6303s\n",
      "1799it [09:02,  3.47it/s]\titers: 1800, epoch: 9 | loss: 0.5561955\n",
      "\tspeed: 0.2968s/iter; left time: 1659.8237s\n",
      "1899it [09:32,  3.31it/s]\titers: 1900, epoch: 9 | loss: 0.2902057\n",
      "\tspeed: 0.2976s/iter; left time: 1634.7788s\n",
      "1999it [10:02,  3.43it/s]\titers: 2000, epoch: 9 | loss: 0.6015102\n",
      "\tspeed: 0.2938s/iter; left time: 1584.2770s\n",
      "2099it [10:32,  3.46it/s]\titers: 2100, epoch: 9 | loss: 0.3751157\n",
      "\tspeed: 0.3004s/iter; left time: 1590.0787s\n",
      "2199it [11:01,  3.36it/s]\titers: 2200, epoch: 9 | loss: 0.3135453\n",
      "\tspeed: 0.2939s/iter; left time: 1526.4467s\n",
      "2299it [11:31,  3.33it/s]\titers: 2300, epoch: 9 | loss: 0.3675324\n",
      "\tspeed: 0.2977s/iter; left time: 1516.4270s\n",
      "2399it [12:01,  3.33it/s]\titers: 2400, epoch: 9 | loss: 0.2612816\n",
      "\tspeed: 0.3008s/iter; left time: 1501.6874s\n",
      "2499it [12:31,  3.50it/s]\titers: 2500, epoch: 9 | loss: 0.3647166\n",
      "\tspeed: 0.2975s/iter; left time: 1455.8444s\n",
      "2599it [13:00,  3.33it/s]\titers: 2600, epoch: 9 | loss: 0.3499140\n",
      "\tspeed: 0.2975s/iter; left time: 1426.1221s\n",
      "2699it [13:30,  3.36it/s]\titers: 2700, epoch: 9 | loss: 0.3965107\n",
      "\tspeed: 0.2953s/iter; left time: 1385.7951s\n",
      "2799it [14:00,  3.33it/s]\titers: 2800, epoch: 9 | loss: 0.3450828\n",
      "\tspeed: 0.2999s/iter; left time: 1377.2972s\n",
      "2899it [14:29,  3.36it/s]\titers: 2900, epoch: 9 | loss: 0.4333178\n",
      "\tspeed: 0.2944s/iter; left time: 1322.5159s\n",
      "2999it [14:59,  3.47it/s]\titers: 3000, epoch: 9 | loss: 0.2200059\n",
      "\tspeed: 0.2988s/iter; left time: 1312.8255s\n",
      "3099it [15:29,  3.32it/s]\titers: 3100, epoch: 9 | loss: 0.3089241\n",
      "\tspeed: 0.2978s/iter; left time: 1278.4042s\n",
      "3199it [15:59,  3.31it/s]\titers: 3200, epoch: 9 | loss: 0.3668992\n",
      "\tspeed: 0.3007s/iter; left time: 1260.6467s\n",
      "3299it [16:29,  3.36it/s]\titers: 3300, epoch: 9 | loss: 0.2891819\n",
      "\tspeed: 0.2974s/iter; left time: 1217.1052s\n",
      "3399it [16:59,  3.38it/s]\titers: 3400, epoch: 9 | loss: 0.6342349\n",
      "\tspeed: 0.2974s/iter; left time: 1187.3780s\n",
      "3499it [17:30,  2.73it/s]\titers: 3500, epoch: 9 | loss: 0.3373721\n",
      "\tspeed: 0.3133s/iter; left time: 1219.8069s\n",
      "3599it [18:00,  3.30it/s]\titers: 3600, epoch: 9 | loss: 0.2651136\n",
      "\tspeed: 0.3050s/iter; left time: 1156.9759s\n",
      "3696it [18:30,  3.33it/s]\n",
      "Epoch: 9 cost time: 1110.4083552360535\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:28,  8.23it/s]\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:22,  8.52it/s]\n",
      "Epoch: 9 | Train Loss: 0.3971528 Vali Loss: 0.7349202 Test Loss: 0.4103983 MAE Loss: 0.4203134\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Updating learning rate to 1.562500000000001e-08\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "99it [00:30,  3.37it/s]\titers: 100, epoch: 10 | loss: 0.5155102\n",
      "\tspeed: 3.5103s/iter; left time: 12626.3847s\n",
      "199it [00:59,  3.42it/s]\titers: 200, epoch: 10 | loss: 0.3472772\n",
      "\tspeed: 0.2930s/iter; left time: 1024.6350s\n",
      "299it [01:29,  3.37it/s]\titers: 300, epoch: 10 | loss: 0.3714704\n",
      "\tspeed: 0.2938s/iter; left time: 998.1431s\n",
      "399it [01:59,  3.34it/s]\titers: 400, epoch: 10 | loss: 0.3812822\n",
      "\tspeed: 0.3006s/iter; left time: 990.9780s\n",
      "499it [02:29,  3.33it/s]\titers: 500, epoch: 10 | loss: 0.6638126\n",
      "\tspeed: 0.3004s/iter; left time: 960.3511s\n",
      "599it [02:58,  3.32it/s]\titers: 600, epoch: 10 | loss: 0.4836461\n",
      "\tspeed: 0.2970s/iter; left time: 919.8517s\n",
      "699it [03:29,  3.33it/s]\titers: 700, epoch: 10 | loss: 0.4012577\n",
      "\tspeed: 0.3008s/iter; left time: 901.6032s\n",
      "799it [03:58,  3.48it/s]\titers: 800, epoch: 10 | loss: 0.4418771\n",
      "\tspeed: 0.2974s/iter; left time: 861.4474s\n",
      "899it [04:28,  3.34it/s]\titers: 900, epoch: 10 | loss: 0.3481673\n",
      "\tspeed: 0.2954s/iter; left time: 826.1882s\n",
      "999it [04:58,  3.29it/s]\titers: 1000, epoch: 10 | loss: 0.3518224\n",
      "\tspeed: 0.3006s/iter; left time: 810.6466s\n",
      "1099it [05:28,  3.35it/s]\titers: 1100, epoch: 10 | loss: 0.2798786\n",
      "\tspeed: 0.3010s/iter; left time: 781.5823s\n",
      "1199it [05:58,  3.35it/s]\titers: 1200, epoch: 10 | loss: 0.4712234\n",
      "\tspeed: 0.3004s/iter; left time: 750.1049s\n",
      "1299it [06:28,  3.35it/s]\titers: 1300, epoch: 10 | loss: 0.4958041\n",
      "\tspeed: 0.2983s/iter; left time: 715.0016s\n",
      "1399it [06:58,  3.31it/s]\titers: 1400, epoch: 10 | loss: 0.3670802\n",
      "\tspeed: 0.3007s/iter; left time: 690.6614s\n",
      "1499it [07:28,  3.34it/s]\titers: 1500, epoch: 10 | loss: 0.2260005\n",
      "\tspeed: 0.3004s/iter; left time: 660.0400s\n",
      "1599it [07:58,  3.31it/s]\titers: 1600, epoch: 10 | loss: 0.5490409\n",
      "\tspeed: 0.2984s/iter; left time: 625.7413s\n",
      "1699it [08:28,  3.34it/s]\titers: 1700, epoch: 10 | loss: 0.3824348\n",
      "\tspeed: 0.3006s/iter; left time: 600.2133s\n",
      "1799it [08:58,  3.25it/s]\titers: 1800, epoch: 10 | loss: 0.3454630\n",
      "\tspeed: 0.3037s/iter; left time: 576.1618s\n",
      "1899it [09:28,  3.35it/s]\titers: 1900, epoch: 10 | loss: 0.4552744\n",
      "\tspeed: 0.3027s/iter; left time: 543.9904s\n",
      "1999it [09:58,  3.33it/s]\titers: 2000, epoch: 10 | loss: 0.2119606\n",
      "\tspeed: 0.2989s/iter; left time: 507.2422s\n",
      "2099it [10:28,  3.34it/s]\titers: 2100, epoch: 10 | loss: 0.3688361\n",
      "\tspeed: 0.3000s/iter; left time: 479.1240s\n",
      "2199it [10:58,  3.31it/s]\titers: 2200, epoch: 10 | loss: 0.3957047\n",
      "\tspeed: 0.2997s/iter; left time: 448.5910s\n",
      "2299it [11:29,  3.34it/s]\titers: 2300, epoch: 10 | loss: 0.3804989\n",
      "\tspeed: 0.3026s/iter; left time: 422.7372s\n",
      "2399it [11:59,  3.31it/s]\titers: 2400, epoch: 10 | loss: 0.5669200\n",
      "\tspeed: 0.3013s/iter; left time: 390.7595s\n",
      "2499it [12:29,  3.32it/s]\titers: 2500, epoch: 10 | loss: 0.5698070\n",
      "\tspeed: 0.2999s/iter; left time: 358.9523s\n",
      "2599it [12:59,  3.36it/s]\titers: 2600, epoch: 10 | loss: 0.2803713\n",
      "\tspeed: 0.3012s/iter; left time: 330.4136s\n",
      "2699it [13:29,  3.33it/s]\titers: 2700, epoch: 10 | loss: 0.2971319\n",
      "\tspeed: 0.3007s/iter; left time: 299.7948s\n",
      "2799it [13:59,  3.32it/s]\titers: 2800, epoch: 10 | loss: 0.4575007\n",
      "\tspeed: 0.2981s/iter; left time: 267.3978s\n",
      "2899it [14:29,  3.33it/s]\titers: 2900, epoch: 10 | loss: 0.7248253\n",
      "\tspeed: 0.2993s/iter; left time: 238.5061s\n",
      "2999it [14:59,  3.33it/s]\titers: 3000, epoch: 10 | loss: 0.3236105\n",
      "\tspeed: 0.2991s/iter; left time: 208.4742s\n",
      "3099it [15:28,  3.35it/s]\titers: 3100, epoch: 10 | loss: 0.2432559\n",
      "\tspeed: 0.2992s/iter; left time: 178.6074s\n",
      "3199it [15:58,  3.43it/s]\titers: 3200, epoch: 10 | loss: 0.3531762\n",
      "\tspeed: 0.3000s/iter; left time: 149.0902s\n",
      "3299it [16:29,  3.31it/s]\titers: 3300, epoch: 10 | loss: 0.2538635\n",
      "\tspeed: 0.3005s/iter; left time: 119.2801s\n",
      "3399it [16:59,  3.34it/s]\titers: 3400, epoch: 10 | loss: 0.2897813\n",
      "\tspeed: 0.3007s/iter; left time: 89.3194s\n",
      "3499it [17:28,  3.34it/s]\titers: 3500, epoch: 10 | loss: 0.2652676\n",
      "\tspeed: 0.2980s/iter; left time: 58.7072s\n",
      "3599it [17:58,  3.33it/s]\titers: 3600, epoch: 10 | loss: 0.2379589\n",
      "\tspeed: 0.2997s/iter; left time: 29.0695s\n",
      "3696it [18:28,  3.33it/s]\n",
      "Epoch: 10 cost time: 1108.5299944877625\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:25,  8.38it/s]\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "1218it [02:28,  8.21it/s]\n",
      "Epoch: 10 | Train Loss: 0.3970765 Vali Loss: 0.7334584 Test Loss: 0.4099353 MAE Loss: 0.4199401\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Updating learning rate to 7.812500000000005e-09\n",
      "success delete checkpoints\n"
     ]
    }
   ],
   "source": [
    "!python /workspace/Time-LLM/run_main.py \\\n",
    "  --task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --model_id ETTh1_96_96 \\\n",
    "  --model_comment \"colab test\" \\\n",
    "  --model TimeLLM \\\n",
    "  --data ETTh1 \\\n",
    "  --root_path /workspace/dataset/dataset/ETT-small/\\\n",
    "  --data_path ETTh1.csv \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 7 \\\n",
    "  --dec_in 7 \\\n",
    "  --c_out 7 \\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \\\n",
    "  --llm_model Gemma7b \\\n",
    "  --llm_dim 3072 \\\n",
    "  --batch_size 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
