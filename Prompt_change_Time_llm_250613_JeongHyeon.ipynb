{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88HX-9StZuPa","executionInfo":{"status":"ok","timestamp":1749775637240,"user_tz":-540,"elapsed":2688,"user":{"displayName":"이정현","userId":"05894115669961020676"}},"outputId":"ae37e103-b19d-4f10-fd6c-058c8437ea14"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","\n","mydrive_path = '25-1/Time-LLM-main'\n","\n","gg_drive_path = os.path.join('/content/drive/MyDrive', mydrive_path)\n","print(sorted(os.listdir(gg_drive_path)))\n","\n","# 작업 디렉토리 변경\n","os.chdir(gg_drive_path)\n","\n","# 현재 디렉토리 확인\n","print(f\"현재 작업 디렉토리: {os.getcwd()}\")\n","print(f\"requirements.txt 존재 여부: {os.path.exists('requirements.txt')}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kS8AS_DkZveD","executionInfo":{"status":"ok","timestamp":1749775637291,"user_tz":-540,"elapsed":47,"user":{"displayName":"이정현","userId":"05894115669961020676"}},"outputId":"154ca065-80bb-4b04-b6ab-1ac7f32716ee"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['LEGAL.md', 'LICENSE', 'README.md', '__MACOSX', 'data_provider', 'data_provider_pretrain', 'dataset', 'ds_config_zero2.json', 'figures', 'layers', 'models', 'requirements.txt', 'run_m4.py', 'run_main.py', 'run_pretrain.py', 'scripts', 'time_llm_text_250612.ipynb', 'utils']\n","현재 작업 디렉토리: /content/drive/MyDrive/25-1/Time-LLM-main\n","requirements.txt 존재 여부: True\n"]}]},{"cell_type":"code","source":["!pip install -r requirements.txt\n","!pip install transformers datasets accelerate bitsandbytes\n","!pip install numpy==1.24.4\n","!pip uninstall -y jax jaxlib"],"metadata":{"id":"4Q_YSRcxioaH","colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"outputId":"d62a64e0-3323-4d13-cbc5-07dd851bdbbb","executionInfo":{"status":"ok","timestamp":1749774775535,"user_tz":-540,"elapsed":201762,"user":{"displayName":"이정현","userId":"05894115669961020676"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==2.2.2 (from -r requirements.txt (line 1))\n","  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n","Collecting accelerate==0.28.0 (from -r requirements.txt (line 2))\n","  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n","Collecting einops==0.7.0 (from -r requirements.txt (line 3))\n","  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n","Collecting matplotlib==3.7.0 (from -r requirements.txt (line 4))\n","  Downloading matplotlib-3.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Collecting numpy==1.23.5 (from -r requirements.txt (line 5))\n","  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n","Collecting pandas==1.5.3 (from -r requirements.txt (line 6))\n","  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting scikit_learn==1.2.2 (from -r requirements.txt (line 7))\n","  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting scipy==1.12.0 (from -r requirements.txt (line 8))\n","  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tqdm==4.65.0 (from -r requirements.txt (line 9))\n","  Downloading tqdm-4.65.0-py3-none-any.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting peft==0.4.0 (from -r requirements.txt (line 10))\n","  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n","Collecting transformers==4.31.0 (from -r requirements.txt (line 11))\n","  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting deepspeed==0.14.0 (from -r requirements.txt (line 12))\n","  Downloading deepspeed-0.14.0.tar.gz (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->-r requirements.txt (line 1)) (4.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->-r requirements.txt (line 1)) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->-r requirements.txt (line 1)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->-r requirements.txt (line 1)) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.2.0 (from torch==2.2.2->-r requirements.txt (line 1))\n","  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (6.0.2)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (0.32.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.28.0->-r requirements.txt (line 2)) (0.5.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 4)) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 4)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 4)) (4.58.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 4)) (1.4.8)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 4)) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 4)) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.0->-r requirements.txt (line 4)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r requirements.txt (line 6)) (2025.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit_learn==1.2.2->-r requirements.txt (line 7)) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn==1.2.2->-r requirements.txt (line 7)) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0->-r requirements.txt (line 11)) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.31.0->-r requirements.txt (line 11)) (2.32.3)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0->-r requirements.txt (line 11))\n","  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting hjson (from deepspeed==0.14.0->-r requirements.txt (line 12))\n","  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n","Collecting ninja (from deepspeed==0.14.0->-r requirements.txt (line 12))\n","  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.14.0->-r requirements.txt (line 12)) (9.0.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.14.0->-r requirements.txt (line 12)) (2.11.5)\n","Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from deepspeed==0.14.0->-r requirements.txt (line 12)) (12.0.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->-r requirements.txt (line 1)) (12.5.82)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->accelerate==0.28.0->-r requirements.txt (line 2)) (1.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.0->-r requirements.txt (line 4)) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2->-r requirements.txt (line 1)) (3.0.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 12)) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 12)) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->deepspeed==0.14.0->-r requirements.txt (line 12)) (0.4.1)\n","Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->deepspeed==0.14.0->-r requirements.txt (line 12)) (12.575.51)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.31.0->-r requirements.txt (line 11)) (2025.4.26)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2->-r requirements.txt (line 1)) (1.3.0)\n","Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading peft-0.4.0-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.14.0-py3-none-any.whl size=1400387 sha256=4586a51a5acf4783fb2c6723ad95a2ae8904e8fc938dc118ae215a4871e9120f\n","  Stored in directory: /root/.cache/pip/wheels/21/93/10/aca4f9f9390297a80a58fb8db0fcdcf1f41499d1afe922a513\n","Successfully built deepspeed\n","Installing collected packages: tokenizers, hjson, triton, tqdm, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, einops, scipy, pandas, nvidia-cusolver-cu12, nvidia-cudnn-cu12, transformers, torch, scikit_learn, matplotlib, deepspeed, accelerate, peft\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.1\n","    Uninstalling tokenizers-0.21.1:\n","      Successfully uninstalled tokenizers-0.21.1\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.67.1\n","    Uninstalling tqdm-4.67.1:\n","      Successfully uninstalled tqdm-4.67.1\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.21.5\n","    Uninstalling nvidia-nccl-cu12-2.21.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: einops\n","    Found existing installation: einops 0.8.1\n","    Uninstalling einops-0.8.1:\n","      Successfully uninstalled einops-0.8.1\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.15.3\n","    Uninstalling scipy-1.15.3:\n","      Successfully uninstalled scipy-1.15.3\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.52.4\n","    Uninstalling transformers-4.52.4:\n","      Successfully uninstalled transformers-4.52.4\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: scikit_learn\n","    Found existing installation: scikit-learn 1.6.1\n","    Uninstalling scikit-learn-1.6.1:\n","      Successfully uninstalled scikit-learn-1.6.1\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.10.0\n","    Uninstalling matplotlib-3.10.0:\n","      Successfully uninstalled matplotlib-3.10.0\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 1.7.0\n","    Uninstalling accelerate-1.7.0:\n","      Successfully uninstalled accelerate-1.7.0\n","  Attempting uninstall: peft\n","    Found existing installation: peft 0.15.2\n","    Uninstalling peft-0.15.2:\n","      Successfully uninstalled peft-0.15.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n","dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n","xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n","albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n","scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\n","chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n","db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","dataproc-spark-connect 0.7.5 requires tqdm>=4.67, but you have tqdm 4.65.0 which is incompatible.\n","imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n","imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n","albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\n","torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\n","bigframes 2.5.0 requires matplotlib>=3.7.1, but you have matplotlib 3.7.0 which is incompatible.\n","bigframes 2.5.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n","plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.0 which is incompatible.\n","plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n","jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","blosc2 3.3.4 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n","xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.28.0 deepspeed-0.14.0 einops-0.7.0 hjson-3.1.0 matplotlib-3.7.0 ninja-1.11.1.4 numpy-1.23.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 pandas-1.5.3 peft-0.4.0 scikit_learn-1.2.2 scipy-1.12.0 tokenizers-0.13.3 torch-2.2.2 tqdm-4.65.0 transformers-4.31.0 triton-2.2.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["matplotlib","mpl_toolkits","numpy"]},"id":"4a526a7d4c3946fb9470ebb579947a52"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.31.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (0.28.0)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.2.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (1.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.46.0\n","Collecting numpy==1.24.4\n","  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n","dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n","xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n","sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\n","imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n","mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n","pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n","torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\n","tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\n","bigframes 2.5.0 requires matplotlib>=3.7.1, but you have matplotlib 3.7.0 which is incompatible.\n","cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n","plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.0 which is incompatible.\n","plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n","jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n","blosc2 3.3.4 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n","xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n","treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.24.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"5adf727b86ca4c8994cf2ff9d96941c0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Found existing installation: jax 0.5.2\n","Uninstalling jax-0.5.2:\n","  Successfully uninstalled jax-0.5.2\n","Found existing installation: jaxlib 0.5.1\n","Uninstalling jaxlib-0.5.1:\n","  Successfully uninstalled jaxlib-0.5.1\n"]}]},{"cell_type":"code","source":["!pip install transformers==4.38.2\n","!pip install mpi4py\n","!pip install deepspeed"],"metadata":{"id":"Gl2nloOUOIW1","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1749774903145,"user_tz":-540,"elapsed":127584,"user":{"displayName":"이정현","userId":"05894115669961020676"}},"outputId":"22a4403c-4102-48fb-cb48-26e0fc49d7d5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.38.2\n","  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.32.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (1.24.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (2.32.3)\n","Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n","  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.2) (4.65.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (1.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.2) (2025.4.26)\n","Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.3\n","    Uninstalling tokenizers-0.13.3:\n","      Successfully uninstalled tokenizers-0.13.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.31.0\n","    Uninstalling transformers-4.31.0:\n","      Successfully uninstalled transformers-4.31.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.38.2\n","Collecting mpi4py\n","  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: mpi4py\n","  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4442044 sha256=0c9a0f62aa9ce406bad405891108d4b83692d7d177f684105d376d1bff7d6b48\n","  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n","Successfully built mpi4py\n","Installing collected packages: mpi4py\n","Successfully installed mpi4py-4.0.3\n","Requirement already satisfied: deepspeed in /usr/local/lib/python3.11/dist-packages (0.14.0)\n","Requirement already satisfied: hjson in /usr/local/lib/python3.11/dist-packages (from deepspeed) (3.1.0)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.11.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from deepspeed) (1.24.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from deepspeed) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from deepspeed) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from deepspeed) (9.0.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.11.5)\n","Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from deepspeed) (12.0.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from deepspeed) (2.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deepspeed) (4.65.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->deepspeed) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->deepspeed) (2.33.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->deepspeed) (4.14.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->deepspeed) (0.4.1)\n","Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->deepspeed) (12.575.51)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.18.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->deepspeed) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.5.82)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->deepspeed) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n"]}]},{"cell_type":"markdown","source":["# 텍스트 실험"],"metadata":{"id":"_ljuTmw_idIc"}},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"STG8ENfFicXX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749775780227,"user_tz":-540,"elapsed":206,"user":{"displayName":"이정현","userId":"05894115669961020676"}},"outputId":"deba46da-18fa-4ed9-9e6e-eadfb06756f1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Jun 13 00:49:40 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0             50W /  400W |       0MiB /  40960MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# prompt 내 min, max 변경 학습\n","\n","!python run_main.py \\\n","  --task_name long_term_forecast \\\n","  --is_training 1 \\\n","  --model_id ETTh1_text1_10epochs \\\n","  --model TimeLLM \\\n","  --data ETTh1 \\\n","  --root_path ./dataset/ETT-small \\\n","  --data_path ETTh1.csv \\\n","  --llm_model LLAMA \\\n","  --train_epochs 10 \\\n","  --model_comment text_10epochs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bK1N5vyyC5Ti","executionInfo":{"status":"ok","timestamp":1749781928625,"user_tz":-540,"elapsed":6069778,"user":{"displayName":"이정현","userId":"05894115669961020676"}},"outputId":"51188252-a321-4107-8771-c2d4bfba8c5d","collapsed":true},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\rconfig.json:   0% 0.00/594 [00:00<?, ?B/s]\rconfig.json: 100% 594/594 [00:00<00:00, 4.93MB/s]\n","Local model files not found. Attempting to download...\n","model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 18.8MB/s]\n","Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n","model-00001-of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n","model-00001-of-00002.safetensors:   0% 10.5M/9.98G [00:00<01:52, 88.5MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   0% 21.0M/9.98G [00:00<01:43, 96.6MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   1% 52.4M/9.98G [00:00<01:00, 165MB/s] \u001b[A\n","model-00001-of-00002.safetensors:   1% 83.9M/9.98G [00:00<00:47, 207MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   1% 115M/9.98G [00:00<00:44, 223MB/s] \u001b[A\n","model-00001-of-00002.safetensors:   1% 147M/9.98G [00:00<00:43, 226MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   2% 178M/9.98G [00:00<00:40, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   2% 210M/9.98G [00:00<00:40, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   2% 241M/9.98G [00:01<00:39, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   3% 273M/9.98G [00:01<00:38, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   3% 304M/9.98G [00:01<00:37, 259MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   3% 336M/9.98G [00:01<00:36, 263MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   4% 367M/9.98G [00:01<00:36, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   4% 398M/9.98G [00:01<00:36, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   4% 430M/9.98G [00:01<00:35, 272MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   5% 461M/9.98G [00:01<00:34, 277MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   5% 493M/9.98G [00:02<00:34, 272MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   5% 524M/9.98G [00:02<00:35, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   6% 556M/9.98G [00:02<00:35, 267MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   6% 587M/9.98G [00:02<00:35, 267MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   6% 619M/9.98G [00:02<00:35, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   7% 650M/9.98G [00:02<00:34, 267MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   7% 682M/9.98G [00:02<00:34, 268MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   7% 713M/9.98G [00:02<00:35, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   7% 744M/9.98G [00:02<00:35, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   8% 776M/9.98G [00:03<00:34, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   8% 807M/9.98G [00:03<00:33, 272MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   8% 839M/9.98G [00:03<00:32, 277MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   9% 870M/9.98G [00:03<00:33, 274MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   9% 902M/9.98G [00:03<00:32, 277MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   9% 933M/9.98G [00:03<00:33, 268MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  10% 965M/9.98G [00:03<00:34, 263MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  10% 996M/9.98G [00:03<00:34, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  10% 1.03G/9.98G [00:04<00:34, 258MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  11% 1.06G/9.98G [00:04<00:35, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  11% 1.09G/9.98G [00:04<00:35, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  11% 1.12G/9.98G [00:04<00:35, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  12% 1.15G/9.98G [00:04<00:34, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  12% 1.18G/9.98G [00:04<00:34, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  12% 1.22G/9.98G [00:04<00:35, 249MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  13% 1.25G/9.98G [00:04<00:35, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  13% 1.28G/9.98G [00:05<00:35, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  13% 1.31G/9.98G [00:05<00:35, 245MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  13% 1.34G/9.98G [00:05<00:35, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  14% 1.37G/9.98G [00:05<00:35, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  14% 1.41G/9.98G [00:05<00:34, 245MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  14% 1.44G/9.98G [00:05<00:34, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  15% 1.47G/9.98G [00:05<00:34, 249MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  15% 1.50G/9.98G [00:05<00:33, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  15% 1.53G/9.98G [00:06<00:33, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  16% 1.56G/9.98G [00:06<00:33, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  16% 1.59G/9.98G [00:06<00:32, 257MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  16% 1.63G/9.98G [00:06<00:32, 260MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  17% 1.66G/9.98G [00:06<00:31, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  17% 1.69G/9.98G [00:06<00:31, 260MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  17% 1.72G/9.98G [00:06<00:31, 263MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  18% 1.75G/9.98G [00:06<00:31, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  18% 1.78G/9.98G [00:07<00:32, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  18% 1.81G/9.98G [00:07<00:56, 144MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  18% 1.85G/9.98G [00:07<00:49, 165MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  19% 1.88G/9.98G [00:07<00:44, 181MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  19% 1.91G/9.98G [00:07<00:40, 197MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  19% 1.94G/9.98G [00:07<00:38, 211MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  20% 1.97G/9.98G [00:08<00:36, 220MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  20% 2.00G/9.98G [00:08<00:34, 228MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  20% 2.03G/9.98G [00:08<00:33, 238MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  21% 2.07G/9.98G [00:08<00:32, 242MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  21% 2.10G/9.98G [00:08<00:31, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  21% 2.13G/9.98G [00:08<00:31, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  22% 2.16G/9.98G [00:08<00:30, 255MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  22% 2.19G/9.98G [00:08<00:30, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  22% 2.22G/9.98G [00:09<00:30, 250MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  23% 2.25G/9.98G [00:09<00:30, 256MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  23% 2.29G/9.98G [00:09<00:30, 256MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  23% 2.32G/9.98G [00:09<00:30, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  24% 2.35G/9.98G [00:09<00:29, 258MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  24% 2.38G/9.98G [00:09<00:29, 257MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  24% 2.41G/9.98G [00:09<00:29, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  24% 2.44G/9.98G [00:09<00:29, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  25% 2.47G/9.98G [00:10<00:29, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  25% 2.51G/9.98G [00:10<00:29, 255MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  25% 2.54G/9.98G [00:10<00:44, 167MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  26% 2.57G/9.98G [00:10<00:39, 189MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  26% 2.60G/9.98G [00:10<00:35, 210MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  26% 2.63G/9.98G [00:10<00:32, 229MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  27% 2.66G/9.98G [00:11<00:30, 238MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  27% 2.69G/9.98G [00:11<00:28, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  27% 2.73G/9.98G [00:11<00:27, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 2.76G/9.98G [00:11<00:26, 269MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 2.79G/9.98G [00:11<00:26, 275MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 2.82G/9.98G [00:11<00:25, 278MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  29% 2.85G/9.98G [00:11<00:25, 279MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  29% 2.88G/9.98G [00:11<00:25, 282MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  29% 2.92G/9.98G [00:11<00:25, 282MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  30% 2.95G/9.98G [00:11<00:24, 283MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  30% 2.98G/9.98G [00:12<00:24, 285MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  30% 3.01G/9.98G [00:12<00:24, 286MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  30% 3.04G/9.98G [00:12<00:24, 286MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  31% 3.07G/9.98G [00:12<00:24, 283MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  31% 3.10G/9.98G [00:12<00:24, 283MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  31% 3.14G/9.98G [00:12<00:24, 276MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  32% 3.17G/9.98G [00:12<00:25, 269MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  32% 3.20G/9.98G [00:12<00:24, 273MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  32% 3.23G/9.98G [00:13<00:24, 274MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33% 3.26G/9.98G [00:13<00:24, 273MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33% 3.29G/9.98G [00:13<00:25, 264MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33% 3.32G/9.98G [00:13<00:25, 264MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  34% 3.36G/9.98G [00:13<00:25, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  34% 3.39G/9.98G [00:13<00:25, 255MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  34% 3.42G/9.98G [00:13<00:26, 252MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  35% 3.45G/9.98G [00:13<00:25, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  35% 3.48G/9.98G [00:14<00:26, 249MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  35% 3.51G/9.98G [00:14<00:26, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  36% 3.54G/9.98G [00:14<00:26, 245MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  36% 3.58G/9.98G [00:14<00:26, 243MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  36% 3.61G/9.98G [00:14<00:26, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  36% 3.64G/9.98G [00:14<00:25, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  37% 3.67G/9.98G [00:14<00:24, 260MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  37% 3.70G/9.98G [00:14<00:23, 264MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  37% 3.73G/9.98G [00:14<00:23, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  38% 3.76G/9.98G [00:15<00:23, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  38% 3.80G/9.98G [00:15<00:23, 260MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  38% 3.83G/9.98G [00:15<00:23, 259MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  39% 3.86G/9.98G [00:15<00:23, 260MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  39% 3.89G/9.98G [00:15<00:23, 263MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  39% 3.92G/9.98G [00:15<00:23, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  40% 3.95G/9.98G [00:15<00:23, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  40% 3.98G/9.98G [00:15<00:22, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  40% 4.02G/9.98G [00:16<00:22, 260MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  41% 4.05G/9.98G [00:16<00:22, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  41% 4.08G/9.98G [00:16<00:22, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  41% 4.11G/9.98G [00:16<00:21, 268MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  42% 4.14G/9.98G [00:16<00:21, 272MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  42% 4.17G/9.98G [00:16<00:20, 276MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  42% 4.20G/9.98G [00:16<00:20, 277MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  42% 4.24G/9.98G [00:16<00:20, 274MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  43% 4.27G/9.98G [00:16<00:20, 275MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  43% 4.30G/9.98G [00:17<00:20, 273MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  43% 4.33G/9.98G [00:17<00:20, 270MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  44% 4.36G/9.98G [00:17<00:20, 270MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  44% 4.39G/9.98G [00:17<00:20, 273MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  44% 4.42G/9.98G [00:17<00:19, 278MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  45% 4.46G/9.98G [00:17<00:19, 280MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  45% 4.49G/9.98G [00:17<00:19, 283MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  45% 4.52G/9.98G [00:17<00:19, 274MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  46% 4.55G/9.98G [00:18<00:20, 265MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  46% 4.58G/9.98G [00:18<00:20, 264MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  46% 4.61G/9.98G [00:18<00:20, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  47% 4.65G/9.98G [00:18<00:19, 268MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  47% 4.68G/9.98G [00:18<00:20, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  47% 4.71G/9.98G [00:18<00:20, 259MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  48% 4.74G/9.98G [00:18<00:20, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  48% 4.77G/9.98G [00:18<00:20, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  48% 4.80G/9.98G [00:19<00:20, 249MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  48% 4.83G/9.98G [00:19<00:20, 249MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  49% 4.87G/9.98G [00:19<00:20, 249MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  49% 4.90G/9.98G [00:19<00:20, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  49% 4.93G/9.98G [00:19<00:20, 250MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  50% 4.96G/9.98G [00:19<00:20, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  50% 4.99G/9.98G [00:19<00:20, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  50% 5.02G/9.98G [00:19<00:20, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  51% 5.05G/9.98G [00:20<00:20, 245MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  51% 5.09G/9.98G [00:20<00:20, 236MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  51% 5.12G/9.98G [00:20<00:20, 237MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  52% 5.15G/9.98G [00:20<00:20, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  52% 5.18G/9.98G [00:20<00:19, 245MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  52% 5.21G/9.98G [00:20<00:22, 213MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  53% 5.24G/9.98G [00:20<00:21, 223MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  53% 5.27G/9.98G [00:21<00:20, 228MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  53% 5.31G/9.98G [00:21<00:19, 236MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  53% 5.34G/9.98G [00:21<00:19, 240MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  54% 5.37G/9.98G [00:21<00:18, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  54% 5.40G/9.98G [00:21<00:18, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  54% 5.43G/9.98G [00:21<00:18, 245MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  55% 5.46G/9.98G [00:21<00:18, 248MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  55% 5.49G/9.98G [00:21<00:18, 248MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  55% 5.53G/9.98G [00:22<00:18, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  56% 5.56G/9.98G [00:22<00:17, 252MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  56% 5.59G/9.98G [00:22<00:16, 258MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  56% 5.62G/9.98G [00:22<00:16, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  57% 5.65G/9.98G [00:22<00:17, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  57% 5.68G/9.98G [00:22<00:16, 258MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  57% 5.71G/9.98G [00:22<00:16, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  58% 5.75G/9.98G [00:23<00:25, 169MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  58% 5.78G/9.98G [00:23<00:22, 190MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  58% 5.81G/9.98G [00:23<00:19, 211MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59% 5.84G/9.98G [00:23<00:18, 228MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59% 5.87G/9.98G [00:23<00:16, 243MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59% 5.90G/9.98G [00:23<00:15, 255MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59% 5.93G/9.98G [00:23<00:15, 264MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  60% 5.97G/9.98G [00:23<00:14, 270MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  60% 6.00G/9.98G [00:23<00:14, 275MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  60% 6.03G/9.98G [00:24<00:14, 278MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  61% 6.06G/9.98G [00:24<00:13, 280MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  61% 6.09G/9.98G [00:24<00:13, 281MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  61% 6.12G/9.98G [00:24<00:14, 273MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  62% 6.16G/9.98G [00:24<00:14, 269MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  62% 6.19G/9.98G [00:24<00:13, 272MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  62% 6.22G/9.98G [00:24<00:14, 267MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63% 6.25G/9.98G [00:24<00:13, 272MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63% 6.28G/9.98G [00:25<00:13, 277MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63% 6.31G/9.98G [00:25<00:13, 269MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  64% 6.34G/9.98G [00:25<00:13, 272MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  64% 6.38G/9.98G [00:25<00:13, 267MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  64% 6.41G/9.98G [00:25<00:13, 263MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  65% 6.44G/9.98G [00:25<00:13, 263MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  65% 6.47G/9.98G [00:25<00:13, 258MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  65% 6.50G/9.98G [00:25<00:13, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  65% 6.53G/9.98G [00:25<00:13, 252MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  66% 6.56G/9.98G [00:26<00:13, 252MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  66% 6.60G/9.98G [00:26<00:13, 248MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  66% 6.63G/9.98G [00:26<00:13, 250MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  67% 6.66G/9.98G [00:26<00:13, 250MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  67% 6.69G/9.98G [00:26<00:13, 249MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  67% 6.72G/9.98G [00:26<00:13, 250MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  68% 6.75G/9.98G [00:26<00:12, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  68% 6.78G/9.98G [00:27<00:12, 252MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  68% 6.82G/9.98G [00:27<00:12, 248MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  69% 6.85G/9.98G [00:27<00:13, 240MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  69% 6.88G/9.98G [00:27<00:12, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  69% 6.91G/9.98G [00:27<00:12, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  70% 6.94G/9.98G [00:27<00:11, 259MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  70% 6.97G/9.98G [00:27<00:11, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  70% 7.00G/9.98G [00:27<00:11, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  71% 7.04G/9.98G [00:28<00:11, 250MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  71% 7.07G/9.98G [00:28<00:11, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  71% 7.10G/9.98G [00:28<00:11, 245MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  71% 7.13G/9.98G [00:28<00:11, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  72% 7.16G/9.98G [00:28<00:11, 245MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  72% 7.19G/9.98G [00:28<00:11, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  72% 7.22G/9.98G [00:28<00:11, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  73% 7.26G/9.98G [00:28<00:11, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  73% 7.29G/9.98G [00:29<00:10, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  73% 7.32G/9.98G [00:29<00:10, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  74% 7.35G/9.98G [00:29<00:10, 242MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  74% 7.38G/9.98G [00:29<00:10, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  74% 7.41G/9.98G [00:29<00:10, 242MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  75% 7.44G/9.98G [00:29<00:10, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  75% 7.48G/9.98G [00:29<00:10, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  75% 7.51G/9.98G [00:29<00:10, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  76% 7.54G/9.98G [00:30<00:10, 243MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  76% 7.57G/9.98G [00:30<00:09, 246MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  76% 7.60G/9.98G [00:30<00:09, 257MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  77% 7.63G/9.98G [00:30<00:08, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  77% 7.67G/9.98G [00:30<00:08, 271MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  77% 7.70G/9.98G [00:30<00:08, 276MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  77% 7.73G/9.98G [00:30<00:08, 279MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  78% 7.76G/9.98G [00:30<00:09, 242MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  78% 7.79G/9.98G [00:31<00:08, 249MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  78% 7.82G/9.98G [00:31<00:08, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  79% 7.85G/9.98G [00:31<00:08, 256MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  79% 7.89G/9.98G [00:31<00:07, 262MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  79% 7.92G/9.98G [00:31<00:07, 270MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  80% 7.95G/9.98G [00:31<00:07, 274MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  80% 7.98G/9.98G [00:31<00:07, 272MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  80% 8.01G/9.98G [00:31<00:07, 270MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  81% 8.04G/9.98G [00:31<00:07, 270MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  81% 8.07G/9.98G [00:32<00:07, 269MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  81% 8.11G/9.98G [00:32<00:06, 269MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  82% 8.14G/9.98G [00:32<00:06, 270MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  82% 8.17G/9.98G [00:32<00:06, 269MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  82% 8.20G/9.98G [00:32<00:06, 267MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  83% 8.23G/9.98G [00:32<00:06, 267MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  83% 8.26G/9.98G [00:32<00:06, 268MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  83% 8.29G/9.98G [00:32<00:06, 268MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  83% 8.33G/9.98G [00:33<00:06, 269MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  84% 8.36G/9.98G [00:33<00:06, 266MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  84% 8.39G/9.98G [00:33<00:06, 256MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  84% 8.42G/9.98G [00:33<00:06, 256MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  85% 8.45G/9.98G [00:33<00:05, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  85% 8.48G/9.98G [00:33<00:05, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  85% 8.51G/9.98G [00:33<00:05, 260MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  86% 8.55G/9.98G [00:33<00:05, 258MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  86% 8.58G/9.98G [00:34<00:05, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  86% 8.61G/9.98G [00:34<00:05, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  87% 8.64G/9.98G [00:34<00:05, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  87% 8.67G/9.98G [00:34<00:05, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  87% 8.70G/9.98G [00:34<00:07, 165MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  88% 8.73G/9.98G [00:34<00:06, 184MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  88% 8.77G/9.98G [00:34<00:06, 200MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  88% 8.80G/9.98G [00:35<00:05, 213MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  88% 8.83G/9.98G [00:35<00:05, 220MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  89% 8.86G/9.98G [00:35<00:04, 227MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  89% 8.89G/9.98G [00:35<00:04, 236MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  89% 8.92G/9.98G [00:35<00:04, 240MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  90% 8.95G/9.98G [00:35<00:04, 236MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  90% 8.99G/9.98G [00:35<00:04, 236MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  90% 9.02G/9.98G [00:36<00:04, 238MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  91% 9.05G/9.98G [00:36<00:03, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  91% 9.08G/9.98G [00:36<00:03, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  91% 9.11G/9.98G [00:36<00:03, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  92% 9.14G/9.98G [00:36<00:03, 248MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  92% 9.18G/9.98G [00:36<00:03, 250MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  92% 9.21G/9.98G [00:36<00:03, 249MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  93% 9.24G/9.98G [00:36<00:02, 248MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  93% 9.27G/9.98G [00:37<00:02, 249MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  93% 9.30G/9.98G [00:37<00:02, 245MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  94% 9.33G/9.98G [00:37<00:02, 237MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  94% 9.36G/9.98G [00:37<00:02, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  94% 9.40G/9.98G [00:37<00:02, 238MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  94% 9.43G/9.98G [00:37<00:02, 244MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  95% 9.46G/9.98G [00:37<00:02, 251MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  95% 9.49G/9.98G [00:37<00:01, 254MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  95% 9.52G/9.98G [00:38<00:01, 253MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  96% 9.55G/9.98G [00:38<00:01, 258MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  96% 9.58G/9.98G [00:38<00:01, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  96% 9.62G/9.98G [00:38<00:01, 263MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  97% 9.65G/9.98G [00:38<00:01, 261MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  97% 9.68G/9.98G [00:38<00:01, 259MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  97% 9.71G/9.98G [00:38<00:01, 252MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  98% 9.74G/9.98G [00:38<00:00, 247MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  98% 9.77G/9.98G [00:39<00:00, 250MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  98% 9.80G/9.98G [00:39<00:00, 248MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  99% 9.84G/9.98G [00:39<00:00, 243MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  99% 9.87G/9.98G [00:39<00:00, 241MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  99% 9.90G/9.98G [00:39<00:00, 245MB/s]\u001b[A\n","model-00001-of-00002.safetensors: 100% 9.93G/9.98G [00:39<00:00, 248MB/s]\u001b[A\n","model-00001-of-00002.safetensors: 100% 9.98G/9.98G [00:39<00:00, 250MB/s]\n","Downloading shards:  50% 1/2 [00:40<00:40, 40.01s/it]\n","model-00002-of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n","model-00002-of-00002.safetensors:   0% 10.5M/3.50G [00:00<01:51, 31.4MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   1% 21.0M/3.50G [00:00<01:23, 41.8MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   1% 31.5M/3.50G [00:00<01:04, 53.5MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   1% 52.4M/3.50G [00:00<00:44, 78.2MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   2% 73.4M/3.50G [00:00<00:33, 101MB/s] \u001b[A\n","model-00002-of-00002.safetensors:   3% 94.4M/3.50G [00:01<00:28, 121MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   4% 126M/3.50G [00:01<00:21, 156MB/s] \u001b[A\n","model-00002-of-00002.safetensors:   4% 157M/3.50G [00:01<00:18, 179MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   5% 178M/3.50G [00:01<00:18, 181MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   6% 199M/3.50G [00:01<00:19, 170MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   6% 220M/3.50G [00:01<00:19, 166MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   7% 241M/3.50G [00:01<00:20, 162MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   7% 262M/3.50G [00:02<00:20, 159MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   8% 283M/3.50G [00:02<00:20, 158MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   9% 304M/3.50G [00:02<00:20, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   9% 325M/3.50G [00:02<00:20, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  10% 346M/3.50G [00:02<00:20, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  10% 367M/3.50G [00:02<00:20, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  11% 388M/3.50G [00:02<00:19, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  12% 409M/3.50G [00:02<00:20, 154MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  12% 430M/3.50G [00:03<00:20, 151MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  13% 451M/3.50G [00:03<00:20, 150MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  13% 472M/3.50G [00:03<00:20, 150MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  14% 493M/3.50G [00:03<00:20, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  15% 514M/3.50G [00:03<00:20, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  15% 535M/3.50G [00:03<00:20, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  16% 556M/3.50G [00:03<00:20, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  16% 577M/3.50G [00:04<00:19, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  17% 598M/3.50G [00:04<00:19, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  18% 619M/3.50G [00:04<00:19, 151MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  18% 640M/3.50G [00:04<00:18, 153MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  19% 661M/3.50G [00:04<00:18, 155MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  19% 682M/3.50G [00:04<00:18, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  20% 703M/3.50G [00:04<00:17, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  21% 724M/3.50G [00:05<00:17, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  21% 744M/3.50G [00:05<00:17, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  22% 765M/3.50G [00:05<00:17, 158MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  22% 786M/3.50G [00:05<00:17, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  23% 807M/3.50G [00:05<00:17, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  24% 828M/3.50G [00:05<00:16, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  24% 849M/3.50G [00:05<00:16, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  25% 870M/3.50G [00:05<00:16, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  25% 891M/3.50G [00:06<00:16, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  26% 912M/3.50G [00:06<00:16, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  27% 933M/3.50G [00:06<00:16, 158MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  27% 954M/3.50G [00:06<00:16, 158MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  28% 975M/3.50G [00:06<00:16, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  28% 996M/3.50G [00:06<00:15, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  29% 1.02G/3.50G [00:06<00:15, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  30% 1.04G/3.50G [00:07<00:15, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  30% 1.06G/3.50G [00:07<00:15, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  31% 1.08G/3.50G [00:07<00:34, 70.5MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  31% 1.10G/3.50G [00:07<00:28, 84.3MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  32% 1.12G/3.50G [00:08<00:24, 97.5MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  33% 1.14G/3.50G [00:08<00:21, 110MB/s] \u001b[A\n","model-00002-of-00002.safetensors:  33% 1.16G/3.50G [00:08<00:19, 121MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  34% 1.18G/3.50G [00:08<00:17, 130MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  34% 1.21G/3.50G [00:08<00:16, 137MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  35% 1.23G/3.50G [00:08<00:15, 143MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  36% 1.25G/3.50G [00:08<00:15, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  36% 1.27G/3.50G [00:09<00:14, 150MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  37% 1.29G/3.50G [00:09<00:14, 153MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  37% 1.31G/3.50G [00:09<00:14, 154MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  38% 1.33G/3.50G [00:09<00:13, 155MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  39% 1.35G/3.50G [00:09<00:13, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  39% 1.37G/3.50G [00:09<00:13, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  40% 1.39G/3.50G [00:09<00:13, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  40% 1.42G/3.50G [00:09<00:13, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  41% 1.44G/3.50G [00:10<00:13, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  42% 1.46G/3.50G [00:10<00:13, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  42% 1.48G/3.50G [00:10<00:14, 138MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  43% 1.50G/3.50G [00:10<00:14, 140MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  43% 1.52G/3.50G [00:10<00:16, 122MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  44% 1.54G/3.50G [00:11<00:17, 110MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  45% 1.56G/3.50G [00:11<00:16, 121MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  45% 1.58G/3.50G [00:11<00:16, 114MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  46% 1.60G/3.50G [00:11<00:15, 124MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  46% 1.63G/3.50G [00:11<00:14, 131MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  47% 1.65G/3.50G [00:11<00:15, 124MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  48% 1.67G/3.50G [00:12<00:16, 112MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  48% 1.69G/3.50G [00:12<00:17, 105MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  49% 1.71G/3.50G [00:12<00:18, 95.5MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  49% 1.72G/3.50G [00:12<00:19, 92.8MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  50% 1.74G/3.50G [00:12<00:18, 96.8MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  50% 1.75G/3.50G [00:13<00:22, 78.5MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  51% 1.77G/3.50G [00:13<00:18, 91.6MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  51% 1.79G/3.50G [00:13<00:16, 107MB/s] \u001b[A\n","model-00002-of-00002.safetensors:  52% 1.81G/3.50G [00:13<00:14, 119MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  52% 1.84G/3.50G [00:13<00:12, 129MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  53% 1.86G/3.50G [00:13<00:12, 137MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  54% 1.88G/3.50G [00:13<00:11, 142MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  54% 1.90G/3.50G [00:14<00:10, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  55% 1.92G/3.50G [00:14<00:10, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  55% 1.94G/3.50G [00:14<00:10, 151MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  56% 1.96G/3.50G [00:14<00:10, 153MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  57% 1.98G/3.50G [00:14<00:09, 154MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  57% 2.00G/3.50G [00:14<00:09, 154MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  58% 2.02G/3.50G [00:14<00:09, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  58% 2.04G/3.50G [00:15<00:09, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  59% 2.07G/3.50G [00:15<00:09, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  60% 2.09G/3.50G [00:15<00:08, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  60% 2.11G/3.50G [00:15<00:13, 105MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  61% 2.13G/3.50G [00:15<00:11, 117MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  61% 2.15G/3.50G [00:15<00:10, 127MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  62% 2.17G/3.50G [00:16<00:09, 134MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  63% 2.19G/3.50G [00:16<00:09, 140MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  63% 2.21G/3.50G [00:16<00:08, 144MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  64% 2.23G/3.50G [00:16<00:08, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  64% 2.25G/3.50G [00:16<00:08, 150MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  65% 2.28G/3.50G [00:16<00:08, 152MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  66% 2.30G/3.50G [00:16<00:07, 153MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  66% 2.32G/3.50G [00:17<00:07, 155MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  67% 2.34G/3.50G [00:17<00:07, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  67% 2.36G/3.50G [00:17<00:07, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  68% 2.38G/3.50G [00:17<00:07, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  69% 2.40G/3.50G [00:17<00:06, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  69% 2.42G/3.50G [00:17<00:06, 158MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  70% 2.44G/3.50G [00:17<00:06, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  70% 2.46G/3.50G [00:17<00:06, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  71% 2.49G/3.50G [00:18<00:06, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  72% 2.51G/3.50G [00:18<00:06, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  72% 2.53G/3.50G [00:18<00:06, 157MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  73% 2.55G/3.50G [00:18<00:06, 156MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  73% 2.57G/3.50G [00:18<00:06, 155MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  74% 2.59G/3.50G [00:18<00:05, 153MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  75% 2.61G/3.50G [00:18<00:05, 151MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  75% 2.63G/3.50G [00:19<00:05, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  76% 2.65G/3.50G [00:19<00:05, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  76% 2.67G/3.50G [00:19<00:05, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  77% 2.69G/3.50G [00:19<00:05, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  78% 2.72G/3.50G [00:19<00:05, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  78% 2.74G/3.50G [00:19<00:05, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  79% 2.76G/3.50G [00:19<00:05, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  79% 2.78G/3.50G [00:20<00:04, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  80% 2.80G/3.50G [00:20<00:04, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  81% 2.82G/3.50G [00:20<00:04, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  81% 2.84G/3.50G [00:20<00:04, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  82% 2.86G/3.50G [00:20<00:04, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  82% 2.88G/3.50G [00:20<00:04, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  83% 2.90G/3.50G [00:20<00:04, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  84% 2.93G/3.50G [00:21<00:03, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  84% 2.95G/3.50G [00:21<00:03, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  85% 2.97G/3.50G [00:21<00:03, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  85% 2.99G/3.50G [00:21<00:03, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  86% 3.01G/3.50G [00:21<00:03, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  87% 3.03G/3.50G [00:21<00:03, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  87% 3.05G/3.50G [00:21<00:03, 146MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  88% 3.07G/3.50G [00:22<00:02, 146MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  88% 3.09G/3.50G [00:22<00:02, 146MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  89% 3.11G/3.50G [00:22<00:02, 145MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  90% 3.14G/3.50G [00:22<00:02, 146MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  90% 3.16G/3.50G [00:22<00:02, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  91% 3.18G/3.50G [00:22<00:02, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  91% 3.20G/3.50G [00:22<00:02, 147MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  92% 3.22G/3.50G [00:23<00:01, 145MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  93% 3.24G/3.50G [00:23<00:01, 149MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  93% 3.26G/3.50G [00:23<00:01, 151MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  94% 3.28G/3.50G [00:23<00:01, 148MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  94% 3.30G/3.50G [00:23<00:01, 150MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  95% 3.32G/3.50G [00:23<00:01, 152MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  96% 3.34G/3.50G [00:24<00:01, 111MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  96% 3.37G/3.50G [00:24<00:01, 122MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  97% 3.39G/3.50G [00:24<00:00, 130MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  97% 3.41G/3.50G [00:24<00:00, 137MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  98% 3.43G/3.50G [00:24<00:00, 143MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  99% 3.45G/3.50G [00:24<00:00, 143MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  99% 3.47G/3.50G [00:24<00:00, 131MB/s]\u001b[A\n","model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:25<00:00, 139MB/s]\n","Downloading shards: 100% 2/2 [01:05<00:00, 32.66s/it]\n","Loading checkpoint shards: 100% 2/2 [00:00<00:00,  2.06it/s]\n","Local tokenizer files not found. Atempting to download them..\n","tokenizer_config.json: 100% 2.28k/2.28k [00:00<00:00, 25.0MB/s]\n","tokenizer.model: 100% 500k/500k [00:00<00:00, 180MB/s]\n","special_tokens_map.json: 100% 411/411 [00:00<00:00, 5.00MB/s]\n","tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 9.30MB/s]\n","[2025-06-13 00:52:20,784] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2025-06-13 00:52:21,289] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n","[2025-06-13 00:52:21,289] [INFO] [comm.py:637:init_distributed] cdb=None\n","[2025-06-13 00:52:21,289] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n","[2025-06-13 00:52:21,934] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.28.0.12, master_port=29500\n","[2025-06-13 00:52:21,934] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n","[2025-06-13 00:52:23,643] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n","[2025-06-13 00:52:23,644] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n","[2025-06-13 00:52:23,644] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n","[2025-06-13 00:52:23,645] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n","[2025-06-13 00:52:23,645] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n","[2025-06-13 00:52:23,645] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n","[2025-06-13 00:52:23,645] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n","[2025-06-13 00:52:23,645] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n","[2025-06-13 00:52:23,645] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n","[2025-06-13 00:52:23,645] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n","[2025-06-13 00:52:23,884] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n","[2025-06-13 00:52:23,885] [INFO] [utils.py:801:see_memory_usage] MA 2.71 GB         Max_MA 2.78 GB         CA 2.8 GB         Max_CA 3 GB \n","[2025-06-13 00:52:23,885] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.49 GB, percent = 4.2%\n","[2025-06-13 00:52:24,004] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n","[2025-06-13 00:52:24,004] [INFO] [utils.py:801:see_memory_usage] MA 2.71 GB         Max_MA 2.85 GB         CA 2.93 GB         Max_CA 3 GB \n","[2025-06-13 00:52:24,005] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.49 GB, percent = 4.2%\n","[2025-06-13 00:52:24,005] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n","[2025-06-13 00:52:24,116] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n","[2025-06-13 00:52:24,116] [INFO] [utils.py:801:see_memory_usage] MA 2.71 GB         Max_MA 2.71 GB         CA 2.93 GB         Max_CA 3 GB \n","[2025-06-13 00:52:24,117] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.5 GB, percent = 4.2%\n","[2025-06-13 00:52:24,117] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n","[2025-06-13 00:52:24,117] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n","[2025-06-13 00:52:24,117] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n","[2025-06-13 00:52:24,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4.000000000000002e-06], mom=[(0.95, 0.999)]\n","[2025-06-13 00:52:24,118] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n","[2025-06-13 00:52:24,118] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n","    \"partition_activations\": false, \n","    \"contiguous_memory_optimization\": false, \n","    \"cpu_checkpointing\": false, \n","    \"number_checkpoints\": null, \n","    \"synchronize_checkpoint_boundary\": false, \n","    \"profile\": false\n","}\n","[2025-06-13 00:52:24,118] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n","[2025-06-13 00:52:24,118] [INFO] [config.py:1000:print]   amp_enabled .................. False\n","[2025-06-13 00:52:24,118] [INFO] [config.py:1000:print]   amp_params ................... False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   autotuning_config ............ {\n","    \"enabled\": false, \n","    \"start_step\": null, \n","    \"end_step\": null, \n","    \"metric_path\": null, \n","    \"arg_mappings\": null, \n","    \"metric\": \"throughput\", \n","    \"model_info\": null, \n","    \"results_dir\": \"autotuning_results\", \n","    \"exps_dir\": \"autotuning_exps\", \n","    \"overwrite\": true, \n","    \"fast\": true, \n","    \"start_profile_step\": 3, \n","    \"end_profile_step\": 5, \n","    \"tuner_type\": \"gridsearch\", \n","    \"tuner_early_stopping\": 5, \n","    \"tuner_num_trials\": 50, \n","    \"model_info_path\": null, \n","    \"mp_size\": 1, \n","    \"max_train_batch_size\": null, \n","    \"min_train_batch_size\": 1, \n","    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n","    \"min_train_micro_batch_size_per_gpu\": 1, \n","    \"num_tuning_micro_batch_sizes\": 3\n","}\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x788b15c3c610>\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   communication_data_type ...... None\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   disable_allgather ............ False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   dump_state ................... False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n","    \"enabled\": false, \n","    \"recompute_fwd_factor\": 0.0, \n","    \"profile_step\": 1, \n","    \"module_depth\": -1, \n","    \"top_modules\": 1, \n","    \"detailed\": true, \n","    \"output_file\": null\n","}\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   global_rank .................. 0\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n","[2025-06-13 00:52:24,119] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   nebula_config ................ {\n","    \"enabled\": false, \n","    \"persistent_storage_path\": null, \n","    \"persistent_time_interval\": 100, \n","    \"num_of_version_in_retention\": 2, \n","    \"enable_nebula_load\": true, \n","    \"load_path\": null\n","}\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   optimizer_name ............... None\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   optimizer_params ............. None\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   pld_enabled .................. False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   pld_params ................... False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   scheduler_name ............... None\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   scheduler_params ............. None\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   sparse_attention ............. None\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   world_size ................... 1\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   zero_enabled ................. True\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n","[2025-06-13 00:52:24,120] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n","[2025-06-13 00:52:24,120] [INFO] [config.py:986:print_user_config]   json = {\n","    \"bf16\": {\n","        \"enabled\": true, \n","        \"auto_cast\": true\n","    }, \n","    \"zero_optimization\": {\n","        \"stage\": 2, \n","        \"allgather_partitions\": true, \n","        \"allgather_bucket_size\": 2.000000e+08, \n","        \"overlap_comm\": true, \n","        \"reduce_scatter\": true, \n","        \"reduce_bucket_size\": 2.000000e+08, \n","        \"contiguous_gradients\": true, \n","        \"sub_group_size\": 1.000000e+09\n","    }, \n","    \"gradient_accumulation_steps\": 1, \n","    \"train_batch_size\": 32, \n","    \"train_micro_batch_size_per_gpu\": 32, \n","    \"steps_per_print\": inf, \n","    \"wall_clock_breakdown\": false, \n","    \"fp16\": {\n","        \"enabled\": false\n","    }, \n","    \"zero_allow_untested_optimizer\": true\n","}\n","99it [00:24,  4.19it/s]\titers: 100, epoch: 1 | loss: 0.4602680\n","\tspeed: 0.2820s/iter; left time: 5183.5695s\n","199it [00:48,  4.18it/s]\titers: 200, epoch: 1 | loss: 0.4982061\n","\tspeed: 0.2388s/iter; left time: 4365.9508s\n","299it [01:12,  4.20it/s]\titers: 300, epoch: 1 | loss: 0.5662640\n","\tspeed: 0.2385s/iter; left time: 4336.3609s\n","399it [01:36,  4.19it/s]\titers: 400, epoch: 1 | loss: 0.5794842\n","\tspeed: 0.2393s/iter; left time: 4326.5898s\n","499it [02:00,  4.18it/s]\titers: 500, epoch: 1 | loss: 0.5691562\n","\tspeed: 0.2391s/iter; left time: 4299.6641s\n","599it [02:24,  4.17it/s]\titers: 600, epoch: 1 | loss: 0.6605886\n","\tspeed: 0.2392s/iter; left time: 4277.2891s\n","699it [02:47,  4.17it/s]\titers: 700, epoch: 1 | loss: 0.4534400\n","\tspeed: 0.2391s/iter; left time: 4251.5831s\n","799it [03:11,  4.20it/s]\titers: 800, epoch: 1 | loss: 0.5038550\n","\tspeed: 0.2389s/iter; left time: 4223.6577s\n","899it [03:35,  4.17it/s]\titers: 900, epoch: 1 | loss: 0.4516759\n","\tspeed: 0.2389s/iter; left time: 4200.6482s\n","999it [03:59,  4.16it/s]\titers: 1000, epoch: 1 | loss: 0.6017337\n","\tspeed: 0.2399s/iter; left time: 4194.0706s\n","1099it [04:23,  4.17it/s]\titers: 1100, epoch: 1 | loss: 0.6577663\n","\tspeed: 0.2395s/iter; left time: 4162.8408s\n","1199it [04:47,  4.17it/s]\titers: 1200, epoch: 1 | loss: 0.4581606\n","\tspeed: 0.2399s/iter; left time: 4146.2719s\n","1299it [05:11,  4.18it/s]\titers: 1300, epoch: 1 | loss: 0.6935350\n","\tspeed: 0.2395s/iter; left time: 4114.3915s\n","1399it [05:35,  4.16it/s]\titers: 1400, epoch: 1 | loss: 0.4583389\n","\tspeed: 0.2394s/iter; left time: 4089.3271s\n","1499it [05:59,  4.16it/s]\titers: 1500, epoch: 1 | loss: 0.4220032\n","\tspeed: 0.2395s/iter; left time: 4067.3187s\n","1599it [06:23,  4.19it/s]\titers: 1600, epoch: 1 | loss: 0.3298918\n","\tspeed: 0.2395s/iter; left time: 4043.2631s\n","1699it [06:47,  4.18it/s]\titers: 1700, epoch: 1 | loss: 0.7771272\n","\tspeed: 0.2395s/iter; left time: 4019.1566s\n","1799it [07:11,  4.17it/s]\titers: 1800, epoch: 1 | loss: 0.3614914\n","\tspeed: 0.2395s/iter; left time: 3994.6900s\n","1848it [07:23,  4.17it/s]\n","Epoch: 1 cost time: 443.17285227775574\n","609it [01:13,  8.27it/s]\n","609it [01:12,  8.37it/s]\n","Epoch: 1 | Train Loss: 0.5176588 Vali Loss: 0.8614996 Test Loss: 0.4947344 MAE Loss: 0.4677206\n","lr = 0.0000040000\n","Updating learning rate to 4.000000000000002e-06\n","99it [00:24,  4.19it/s]\titers: 100, epoch: 2 | loss: 0.3772892\n","\tspeed: 1.8914s/iter; left time: 31269.7209s\n","199it [00:48,  4.18it/s]\titers: 200, epoch: 2 | loss: 0.5647764\n","\tspeed: 0.2397s/iter; left time: 3939.1845s\n","299it [01:11,  4.16it/s]\titers: 300, epoch: 2 | loss: 0.3818729\n","\tspeed: 0.2392s/iter; left time: 3907.6401s\n","399it [01:35,  4.17it/s]\titers: 400, epoch: 2 | loss: 0.3600408\n","\tspeed: 0.2392s/iter; left time: 3883.0211s\n","499it [01:59,  4.18it/s]\titers: 500, epoch: 2 | loss: 0.3520272\n","\tspeed: 0.2391s/iter; left time: 3857.4767s\n","599it [02:23,  4.18it/s]\titers: 600, epoch: 2 | loss: 0.3986740\n","\tspeed: 0.2390s/iter; left time: 3832.1205s\n","699it [02:47,  4.19it/s]\titers: 700, epoch: 2 | loss: 0.4056012\n","\tspeed: 0.2391s/iter; left time: 3809.3796s\n","799it [03:11,  4.19it/s]\titers: 800, epoch: 2 | loss: 0.3207473\n","\tspeed: 0.2389s/iter; left time: 3782.8518s\n","899it [03:35,  4.16it/s]\titers: 900, epoch: 2 | loss: 0.4986942\n","\tspeed: 0.2399s/iter; left time: 3773.9172s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 2 | loss: 0.2412154\n","\tspeed: 0.2398s/iter; left time: 3749.3431s\n","1099it [04:23,  4.17it/s]\titers: 1100, epoch: 2 | loss: 0.3648738\n","\tspeed: 0.2396s/iter; left time: 3722.1733s\n","1199it [04:47,  4.19it/s]\titers: 1200, epoch: 2 | loss: 0.3170519\n","\tspeed: 0.2395s/iter; left time: 3696.4864s\n","1299it [05:11,  4.15it/s]\titers: 1300, epoch: 2 | loss: 0.6267492\n","\tspeed: 0.2397s/iter; left time: 3674.8563s\n","1399it [05:35,  4.16it/s]\titers: 1400, epoch: 2 | loss: 0.3390426\n","\tspeed: 0.2398s/iter; left time: 3653.4982s\n","1499it [05:59,  4.16it/s]\titers: 1500, epoch: 2 | loss: 0.4147116\n","\tspeed: 0.2396s/iter; left time: 3626.5073s\n","1599it [06:23,  4.18it/s]\titers: 1600, epoch: 2 | loss: 0.4015031\n","\tspeed: 0.2399s/iter; left time: 3605.8866s\n","1699it [06:47,  4.17it/s]\titers: 1700, epoch: 2 | loss: 0.3597496\n","\tspeed: 0.2400s/iter; left time: 3584.4667s\n","1799it [07:11,  4.19it/s]\titers: 1800, epoch: 2 | loss: 0.3431927\n","\tspeed: 0.2395s/iter; left time: 3552.4349s\n","1848it [07:22,  4.17it/s]\n","Epoch: 2 cost time: 442.99053406715393\n","609it [01:13,  8.27it/s]\n","609it [01:12,  8.36it/s]\n","Epoch: 2 | Train Loss: 0.4283623 Vali Loss: 0.7854621 Test Loss: 0.4437478 MAE Loss: 0.4409274\n","Updating learning rate to 2.000000000000001e-06\n","99it [00:23,  4.17it/s]\titers: 100, epoch: 3 | loss: 0.3893317\n","\tspeed: 1.9259s/iter; left time: 28281.9632s\n","199it [00:47,  4.18it/s]\titers: 200, epoch: 3 | loss: 0.3494448\n","\tspeed: 0.2393s/iter; left time: 3489.5396s\n","299it [01:11,  4.15it/s]\titers: 300, epoch: 3 | loss: 0.3942264\n","\tspeed: 0.2390s/iter; left time: 3462.2672s\n","399it [01:35,  4.16it/s]\titers: 400, epoch: 3 | loss: 0.4507183\n","\tspeed: 0.2391s/iter; left time: 3440.0676s\n","499it [01:59,  4.17it/s]\titers: 500, epoch: 3 | loss: 0.3681732\n","\tspeed: 0.2393s/iter; left time: 3417.8334s\n","599it [02:23,  4.15it/s]\titers: 600, epoch: 3 | loss: 0.3333884\n","\tspeed: 0.2396s/iter; left time: 3398.0286s\n","699it [02:47,  4.18it/s]\titers: 700, epoch: 3 | loss: 0.5557258\n","\tspeed: 0.2394s/iter; left time: 3372.0398s\n","799it [03:11,  4.16it/s]\titers: 800, epoch: 3 | loss: 0.3780043\n","\tspeed: 0.2398s/iter; left time: 3353.8533s\n","899it [03:35,  4.16it/s]\titers: 900, epoch: 3 | loss: 0.4392630\n","\tspeed: 0.2396s/iter; left time: 3326.8742s\n","999it [03:59,  4.15it/s]\titers: 1000, epoch: 3 | loss: 0.4397654\n","\tspeed: 0.2399s/iter; left time: 3307.5802s\n","1099it [04:23,  4.19it/s]\titers: 1100, epoch: 3 | loss: 0.3878861\n","\tspeed: 0.2399s/iter; left time: 3282.5176s\n","1199it [04:47,  4.22it/s]\titers: 1200, epoch: 3 | loss: 0.3661336\n","\tspeed: 0.2391s/iter; left time: 3248.6074s\n","1299it [05:11,  4.19it/s]\titers: 1300, epoch: 3 | loss: 0.4101710\n","\tspeed: 0.2395s/iter; left time: 3229.4965s\n","1399it [05:35,  4.16it/s]\titers: 1400, epoch: 3 | loss: 0.4416329\n","\tspeed: 0.2392s/iter; left time: 3201.3183s\n","1499it [05:59,  4.17it/s]\titers: 1500, epoch: 3 | loss: 0.2952866\n","\tspeed: 0.2397s/iter; left time: 3184.5443s\n","1599it [06:23,  4.18it/s]\titers: 1600, epoch: 3 | loss: 0.3046585\n","\tspeed: 0.2395s/iter; left time: 3157.8217s\n","1699it [06:47,  4.18it/s]\titers: 1700, epoch: 3 | loss: 0.4498687\n","\tspeed: 0.2394s/iter; left time: 3132.1937s\n","1799it [07:11,  4.19it/s]\titers: 1800, epoch: 3 | loss: 0.3247207\n","\tspeed: 0.2394s/iter; left time: 3108.3567s\n","1848it [07:22,  4.17it/s]\n","Epoch: 3 cost time: 442.8012034893036\n","609it [01:13,  8.27it/s]\n","609it [01:12,  8.37it/s]\n","Epoch: 3 | Train Loss: 0.4113828 Vali Loss: 0.7609681 Test Loss: 0.4293789 MAE Loss: 0.4324923\n","Updating learning rate to 1.0000000000000006e-06\n","99it [00:23,  4.17it/s]\titers: 100, epoch: 4 | loss: 0.4706872\n","\tspeed: 1.9177s/iter; left time: 24617.7860s\n","199it [00:47,  4.17it/s]\titers: 200, epoch: 4 | loss: 0.4917010\n","\tspeed: 0.2392s/iter; left time: 3046.2573s\n","299it [01:11,  4.18it/s]\titers: 300, epoch: 4 | loss: 0.4466396\n","\tspeed: 0.2396s/iter; left time: 3027.5568s\n","399it [01:35,  4.18it/s]\titers: 400, epoch: 4 | loss: 0.4284153\n","\tspeed: 0.2397s/iter; left time: 3004.6461s\n","499it [01:59,  4.18it/s]\titers: 500, epoch: 4 | loss: 0.2411139\n","\tspeed: 0.2396s/iter; left time: 2979.8614s\n","599it [02:23,  4.18it/s]\titers: 600, epoch: 4 | loss: 0.4745750\n","\tspeed: 0.2394s/iter; left time: 2953.6498s\n","699it [02:47,  4.19it/s]\titers: 700, epoch: 4 | loss: 0.4147091\n","\tspeed: 0.2393s/iter; left time: 2928.6236s\n","799it [03:11,  4.22it/s]\titers: 800, epoch: 4 | loss: 0.3768745\n","\tspeed: 0.2393s/iter; left time: 2904.1566s\n","899it [03:35,  4.18it/s]\titers: 900, epoch: 4 | loss: 0.3394865\n","\tspeed: 0.2395s/iter; left time: 2882.8608s\n","999it [03:59,  4.16it/s]\titers: 1000, epoch: 4 | loss: 0.5041615\n","\tspeed: 0.2397s/iter; left time: 2860.9493s\n","1099it [04:23,  4.16it/s]\titers: 1100, epoch: 4 | loss: 0.4478853\n","\tspeed: 0.2398s/iter; left time: 2838.5490s\n","1199it [04:47,  4.20it/s]\titers: 1200, epoch: 4 | loss: 0.3174102\n","\tspeed: 0.2396s/iter; left time: 2812.1722s\n","1299it [05:11,  4.18it/s]\titers: 1300, epoch: 4 | loss: 0.4492579\n","\tspeed: 0.2393s/iter; left time: 2785.2907s\n","1399it [05:35,  4.18it/s]\titers: 1400, epoch: 4 | loss: 0.3348320\n","\tspeed: 0.2393s/iter; left time: 2760.4448s\n","1499it [05:59,  4.18it/s]\titers: 1500, epoch: 4 | loss: 0.3518969\n","\tspeed: 0.2397s/iter; left time: 2741.0507s\n","1599it [06:23,  4.22it/s]\titers: 1600, epoch: 4 | loss: 0.4312828\n","\tspeed: 0.2390s/iter; left time: 2709.8365s\n","1699it [06:47,  4.18it/s]\titers: 1700, epoch: 4 | loss: 0.5296266\n","\tspeed: 0.2394s/iter; left time: 2689.5976s\n","1799it [07:11,  4.16it/s]\titers: 1800, epoch: 4 | loss: 0.3486902\n","\tspeed: 0.2396s/iter; left time: 2668.1663s\n","1848it [07:22,  4.17it/s]\n","Epoch: 4 cost time: 442.84811568260193\n","609it [01:13,  8.27it/s]\n","609it [01:12,  8.36it/s]\n","Epoch: 4 | Train Loss: 0.4061983 Vali Loss: 0.7561915 Test Loss: 0.4238772 MAE Loss: 0.4286783\n","Updating learning rate to 5.000000000000003e-07\n","99it [00:23,  4.17it/s]\titers: 100, epoch: 5 | loss: 0.3367414\n","\tspeed: 1.8957s/iter; left time: 20831.3523s\n","199it [00:47,  4.18it/s]\titers: 200, epoch: 5 | loss: 0.3572818\n","\tspeed: 0.2390s/iter; left time: 2602.9490s\n","299it [01:11,  4.19it/s]\titers: 300, epoch: 5 | loss: 0.3893987\n","\tspeed: 0.2394s/iter; left time: 2583.2739s\n","399it [01:35,  4.17it/s]\titers: 400, epoch: 5 | loss: 0.3244760\n","\tspeed: 0.2395s/iter; left time: 2559.8296s\n","499it [01:59,  4.17it/s]\titers: 500, epoch: 5 | loss: 0.4391889\n","\tspeed: 0.2396s/iter; left time: 2536.9604s\n","599it [02:23,  4.18it/s]\titers: 600, epoch: 5 | loss: 0.5527092\n","\tspeed: 0.2398s/iter; left time: 2515.2408s\n","699it [02:47,  4.19it/s]\titers: 700, epoch: 5 | loss: 0.3220291\n","\tspeed: 0.2392s/iter; left time: 2485.4084s\n","799it [03:11,  4.18it/s]\titers: 800, epoch: 5 | loss: 0.4799531\n","\tspeed: 0.2394s/iter; left time: 2462.7527s\n","899it [03:35,  4.17it/s]\titers: 900, epoch: 5 | loss: 0.4462479\n","\tspeed: 0.2396s/iter; left time: 2441.0755s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 5 | loss: 0.3847208\n","\tspeed: 0.2395s/iter; left time: 2416.2529s\n","1099it [04:23,  4.16it/s]\titers: 1100, epoch: 5 | loss: 0.3386521\n","\tspeed: 0.2397s/iter; left time: 2394.7238s\n","1199it [04:47,  4.17it/s]\titers: 1200, epoch: 5 | loss: 0.2980430\n","\tspeed: 0.2399s/iter; left time: 2371.9881s\n","1299it [05:11,  4.16it/s]\titers: 1300, epoch: 5 | loss: 0.2966785\n","\tspeed: 0.2396s/iter; left time: 2345.6148s\n","1399it [05:35,  4.19it/s]\titers: 1400, epoch: 5 | loss: 0.4109753\n","\tspeed: 0.2392s/iter; left time: 2317.6625s\n","1499it [05:59,  4.19it/s]\titers: 1500, epoch: 5 | loss: 0.4014834\n","\tspeed: 0.2392s/iter; left time: 2293.8193s\n","1599it [06:23,  4.17it/s]\titers: 1600, epoch: 5 | loss: 0.4768956\n","\tspeed: 0.2396s/iter; left time: 2273.9183s\n","1699it [06:47,  4.17it/s]\titers: 1700, epoch: 5 | loss: 0.3584184\n","\tspeed: 0.2398s/iter; left time: 2251.1175s\n","1799it [07:11,  4.19it/s]\titers: 1800, epoch: 5 | loss: 0.2730301\n","\tspeed: 0.2390s/iter; left time: 2219.7681s\n","1848it [07:22,  4.17it/s]\n","Epoch: 5 cost time: 442.8220672607422\n","609it [01:13,  8.26it/s]\n","609it [01:12,  8.36it/s]\n","Epoch: 5 | Train Loss: 0.4037145 Vali Loss: 0.7530263 Test Loss: 0.4216893 MAE Loss: 0.4273503\n","Updating learning rate to 2.5000000000000015e-07\n","99it [00:24,  4.16it/s]\titers: 100, epoch: 6 | loss: 0.5228713\n","\tspeed: 1.9015s/iter; left time: 17381.2743s\n","199it [00:47,  4.16it/s]\titers: 200, epoch: 6 | loss: 0.4797573\n","\tspeed: 0.2396s/iter; left time: 2166.2146s\n","299it [01:11,  4.18it/s]\titers: 300, epoch: 6 | loss: 0.4043958\n","\tspeed: 0.2394s/iter; left time: 2140.1148s\n","399it [01:35,  4.16it/s]\titers: 400, epoch: 6 | loss: 0.5019395\n","\tspeed: 0.2395s/iter; left time: 2117.2574s\n","499it [01:59,  4.16it/s]\titers: 500, epoch: 6 | loss: 0.4110904\n","\tspeed: 0.2393s/iter; left time: 2092.1508s\n","599it [02:23,  4.16it/s]\titers: 600, epoch: 6 | loss: 0.3643371\n","\tspeed: 0.2393s/iter; left time: 2068.1825s\n","699it [02:47,  4.18it/s]\titers: 700, epoch: 6 | loss: 0.6866902\n","\tspeed: 0.2393s/iter; left time: 2043.6801s\n","799it [03:11,  4.18it/s]\titers: 800, epoch: 6 | loss: 0.5466545\n","\tspeed: 0.2387s/iter; left time: 2014.9602s\n","899it [03:35,  4.18it/s]\titers: 900, epoch: 6 | loss: 0.6826733\n","\tspeed: 0.2392s/iter; left time: 1994.7761s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 6 | loss: 0.3287301\n","\tspeed: 0.2394s/iter; left time: 1973.0062s\n","1099it [04:23,  4.19it/s]\titers: 1100, epoch: 6 | loss: 0.3492117\n","\tspeed: 0.2390s/iter; left time: 1945.7605s\n","1199it [04:47,  4.18it/s]\titers: 1200, epoch: 6 | loss: 0.3090428\n","\tspeed: 0.2394s/iter; left time: 1924.9369s\n","1299it [05:11,  4.19it/s]\titers: 1300, epoch: 6 | loss: 0.4881675\n","\tspeed: 0.2394s/iter; left time: 1900.8181s\n","1399it [05:35,  4.20it/s]\titers: 1400, epoch: 6 | loss: 0.3566531\n","\tspeed: 0.2399s/iter; left time: 1880.7980s\n","1499it [05:59,  4.19it/s]\titers: 1500, epoch: 6 | loss: 0.2884500\n","\tspeed: 0.2394s/iter; left time: 1853.5805s\n","1599it [06:23,  4.18it/s]\titers: 1600, epoch: 6 | loss: 0.4293990\n","\tspeed: 0.2394s/iter; left time: 1828.9025s\n","1699it [06:46,  4.17it/s]\titers: 1700, epoch: 6 | loss: 0.3469238\n","\tspeed: 0.2395s/iter; left time: 1806.1753s\n","1799it [07:10,  4.20it/s]\titers: 1800, epoch: 6 | loss: 0.3247425\n","\tspeed: 0.2395s/iter; left time: 1782.3229s\n","1848it [07:22,  4.17it/s]\n","Epoch: 6 cost time: 442.77612352371216\n","609it [01:13,  8.24it/s]\n","609it [01:13,  8.33it/s]\n","Epoch: 6 | Train Loss: 0.4026121 Vali Loss: 0.7518874 Test Loss: 0.4206255 MAE Loss: 0.4266241\n","Updating learning rate to 1.2500000000000007e-07\n","99it [00:23,  4.17it/s]\titers: 100, epoch: 7 | loss: 0.3996755\n","\tspeed: 1.9077s/iter; left time: 13912.8581s\n","199it [00:47,  4.16it/s]\titers: 200, epoch: 7 | loss: 0.5044274\n","\tspeed: 0.2391s/iter; left time: 1720.1476s\n","299it [01:11,  4.14it/s]\titers: 300, epoch: 7 | loss: 0.4719524\n","\tspeed: 0.2395s/iter; left time: 1698.8553s\n","399it [01:35,  4.18it/s]\titers: 400, epoch: 7 | loss: 0.5673033\n","\tspeed: 0.2396s/iter; left time: 1675.2572s\n","499it [01:59,  4.19it/s]\titers: 500, epoch: 7 | loss: 0.5484529\n","\tspeed: 0.2395s/iter; left time: 1650.8499s\n","599it [02:23,  4.19it/s]\titers: 600, epoch: 7 | loss: 0.6362291\n","\tspeed: 0.2394s/iter; left time: 1626.5643s\n","699it [02:47,  4.19it/s]\titers: 700, epoch: 7 | loss: 0.3266998\n","\tspeed: 0.2397s/iter; left time: 1604.2080s\n","799it [03:11,  4.17it/s]\titers: 800, epoch: 7 | loss: 0.4218595\n","\tspeed: 0.2395s/iter; left time: 1578.7499s\n","899it [03:35,  4.18it/s]\titers: 900, epoch: 7 | loss: 0.2630820\n","\tspeed: 0.2396s/iter; left time: 1555.8120s\n","999it [03:59,  4.20it/s]\titers: 1000, epoch: 7 | loss: 0.5267150\n","\tspeed: 0.2396s/iter; left time: 1531.4867s\n","1099it [04:23,  4.18it/s]\titers: 1100, epoch: 7 | loss: 0.3119723\n","\tspeed: 0.2392s/iter; left time: 1505.5543s\n","1199it [04:47,  4.20it/s]\titers: 1200, epoch: 7 | loss: 0.3650766\n","\tspeed: 0.2394s/iter; left time: 1482.3282s\n","1299it [05:11,  4.18it/s]\titers: 1300, epoch: 7 | loss: 0.4466001\n","\tspeed: 0.2392s/iter; left time: 1457.6067s\n","1399it [05:35,  4.18it/s]\titers: 1400, epoch: 7 | loss: 0.2669408\n","\tspeed: 0.2393s/iter; left time: 1433.8905s\n","1499it [05:59,  4.18it/s]\titers: 1500, epoch: 7 | loss: 0.3830640\n","\tspeed: 0.2397s/iter; left time: 1412.2791s\n","1599it [06:23,  4.19it/s]\titers: 1600, epoch: 7 | loss: 0.3681036\n","\tspeed: 0.2395s/iter; left time: 1387.4209s\n","1699it [06:47,  4.20it/s]\titers: 1700, epoch: 7 | loss: 0.3609923\n","\tspeed: 0.2393s/iter; left time: 1362.3370s\n","1799it [07:11,  4.17it/s]\titers: 1800, epoch: 7 | loss: 0.6436670\n","\tspeed: 0.2396s/iter; left time: 1339.8558s\n","1848it [07:22,  4.17it/s]\n","Epoch: 7 cost time: 442.8436896800995\n","609it [01:13,  8.25it/s]\n","609it [01:12,  8.36it/s]\n","Epoch: 7 | Train Loss: 0.4021487 Vali Loss: 0.7495851 Test Loss: 0.4197715 MAE Loss: 0.4261201\n","Updating learning rate to 6.250000000000004e-08\n","99it [00:23,  4.18it/s]\titers: 100, epoch: 8 | loss: 0.3990889\n","\tspeed: 1.9243s/iter; left time: 10477.6765s\n","199it [00:47,  4.18it/s]\titers: 200, epoch: 8 | loss: 0.3532961\n","\tspeed: 0.2389s/iter; left time: 1276.9172s\n","299it [01:11,  4.20it/s]\titers: 300, epoch: 8 | loss: 0.3403639\n","\tspeed: 0.2398s/iter; left time: 1257.5045s\n","399it [01:35,  4.17it/s]\titers: 400, epoch: 8 | loss: 0.3996358\n","\tspeed: 0.2390s/iter; left time: 1229.6265s\n","499it [01:59,  4.20it/s]\titers: 500, epoch: 8 | loss: 0.4318857\n","\tspeed: 0.2396s/iter; left time: 1208.7004s\n","599it [02:23,  4.17it/s]\titers: 600, epoch: 8 | loss: 0.3118017\n","\tspeed: 0.2391s/iter; left time: 1182.3016s\n","699it [02:47,  4.17it/s]\titers: 700, epoch: 8 | loss: 0.5295662\n","\tspeed: 0.2392s/iter; left time: 1158.8840s\n","799it [03:11,  4.17it/s]\titers: 800, epoch: 8 | loss: 0.4023373\n","\tspeed: 0.2396s/iter; left time: 1136.7093s\n","899it [03:35,  4.17it/s]\titers: 900, epoch: 8 | loss: 0.3815150\n","\tspeed: 0.2395s/iter; left time: 1112.6529s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 8 | loss: 0.2815600\n","\tspeed: 0.2397s/iter; left time: 1089.4116s\n","1099it [04:23,  4.17it/s]\titers: 1100, epoch: 8 | loss: 0.5016863\n","\tspeed: 0.2394s/iter; left time: 1064.1323s\n","1199it [04:47,  4.18it/s]\titers: 1200, epoch: 8 | loss: 0.4170123\n","\tspeed: 0.2389s/iter; left time: 1037.9180s\n","1299it [05:11,  4.17it/s]\titers: 1300, epoch: 8 | loss: 0.6347127\n","\tspeed: 0.2400s/iter; left time: 1018.8896s\n","1399it [05:35,  4.16it/s]\titers: 1400, epoch: 8 | loss: 0.4121493\n","\tspeed: 0.2398s/iter; left time: 993.9852s\n","1499it [05:59,  4.19it/s]\titers: 1500, epoch: 8 | loss: 0.4045982\n","\tspeed: 0.2398s/iter; left time: 970.1154s\n","1599it [06:23,  4.17it/s]\titers: 1600, epoch: 8 | loss: 0.4507030\n","\tspeed: 0.2394s/iter; left time: 944.2507s\n","1699it [06:47,  4.18it/s]\titers: 1700, epoch: 8 | loss: 0.4696283\n","\tspeed: 0.2393s/iter; left time: 920.2354s\n","1799it [07:10,  4.18it/s]\titers: 1800, epoch: 8 | loss: 0.5306222\n","\tspeed: 0.2393s/iter; left time: 896.2496s\n","1848it [07:22,  4.17it/s]\n","Epoch: 8 cost time: 442.7145006656647\n","609it [01:13,  8.27it/s]\n","609it [01:12,  8.37it/s]\n","Epoch: 8 | Train Loss: 0.4020704 Vali Loss: 0.7504125 Test Loss: 0.4197189 MAE Loss: 0.4260788\n","EarlyStopping counter: 1 out of 10\n","Updating learning rate to 3.125000000000002e-08\n","99it [00:23,  4.15it/s]\titers: 100, epoch: 9 | loss: 0.3034001\n","\tspeed: 1.8219s/iter; left time: 6553.3171s\n","199it [00:48,  4.17it/s]\titers: 200, epoch: 9 | loss: 0.5694621\n","\tspeed: 0.2400s/iter; left time: 839.2859s\n","299it [01:11,  4.16it/s]\titers: 300, epoch: 9 | loss: 0.4640650\n","\tspeed: 0.2396s/iter; left time: 813.8521s\n","399it [01:35,  4.18it/s]\titers: 400, epoch: 9 | loss: 0.3844441\n","\tspeed: 0.2397s/iter; left time: 790.1375s\n","499it [01:59,  4.18it/s]\titers: 500, epoch: 9 | loss: 0.3548033\n","\tspeed: 0.2395s/iter; left time: 765.5585s\n","599it [02:23,  4.18it/s]\titers: 600, epoch: 9 | loss: 0.4968048\n","\tspeed: 0.2388s/iter; left time: 739.6320s\n","699it [02:47,  4.18it/s]\titers: 700, epoch: 9 | loss: 0.5209019\n","\tspeed: 0.2395s/iter; left time: 717.7546s\n","799it [03:11,  4.16it/s]\titers: 800, epoch: 9 | loss: 0.4399791\n","\tspeed: 0.2400s/iter; left time: 695.2116s\n","899it [03:35,  4.17it/s]\titers: 900, epoch: 9 | loss: 0.2748003\n","\tspeed: 0.2399s/iter; left time: 670.9406s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 9 | loss: 0.5478202\n","\tspeed: 0.2401s/iter; left time: 647.5249s\n","1099it [04:23,  4.16it/s]\titers: 1100, epoch: 9 | loss: 0.3333875\n","\tspeed: 0.2398s/iter; left time: 622.7765s\n","1199it [04:47,  4.16it/s]\titers: 1200, epoch: 9 | loss: 0.3381943\n","\tspeed: 0.2396s/iter; left time: 598.1681s\n","1299it [05:11,  4.17it/s]\titers: 1300, epoch: 9 | loss: 0.3305587\n","\tspeed: 0.2398s/iter; left time: 574.7796s\n","1399it [05:35,  4.17it/s]\titers: 1400, epoch: 9 | loss: 0.3334582\n","\tspeed: 0.2396s/iter; left time: 550.4152s\n","1499it [05:59,  4.18it/s]\titers: 1500, epoch: 9 | loss: 0.4347071\n","\tspeed: 0.2397s/iter; left time: 526.5872s\n","1599it [06:23,  4.18it/s]\titers: 1600, epoch: 9 | loss: 0.4317172\n","\tspeed: 0.2399s/iter; left time: 503.1404s\n","1699it [06:47,  4.17it/s]\titers: 1700, epoch: 9 | loss: 0.3403867\n","\tspeed: 0.2396s/iter; left time: 478.4056s\n","1799it [07:11,  4.17it/s]\titers: 1800, epoch: 9 | loss: 0.4226144\n","\tspeed: 0.2397s/iter; left time: 454.6315s\n","1848it [07:23,  4.17it/s]\n","Epoch: 9 cost time: 443.26177644729614\n","609it [01:13,  8.27it/s]\n","609it [01:12,  8.34it/s]\n","Epoch: 9 | Train Loss: 0.4019766 Vali Loss: 0.7496591 Test Loss: 0.4195799 MAE Loss: 0.4259957\n","EarlyStopping counter: 2 out of 10\n","Updating learning rate to 1.562500000000001e-08\n","99it [00:23,  4.17it/s]\titers: 100, epoch: 10 | loss: 0.2655375\n","\tspeed: 1.8241s/iter; left time: 3190.3891s\n","199it [00:47,  4.17it/s]\titers: 200, epoch: 10 | loss: 0.4928824\n","\tspeed: 0.2396s/iter; left time: 395.1132s\n","299it [01:11,  4.18it/s]\titers: 300, epoch: 10 | loss: 0.5319400\n","\tspeed: 0.2397s/iter; left time: 371.2889s\n","399it [01:35,  4.16it/s]\titers: 400, epoch: 10 | loss: 0.4315034\n","\tspeed: 0.2397s/iter; left time: 347.3924s\n","499it [01:59,  4.16it/s]\titers: 500, epoch: 10 | loss: 0.4158064\n","\tspeed: 0.2398s/iter; left time: 323.5051s\n","599it [02:23,  4.17it/s]\titers: 600, epoch: 10 | loss: 0.2975205\n","\tspeed: 0.2397s/iter; left time: 299.3229s\n","699it [02:47,  4.18it/s]\titers: 700, epoch: 10 | loss: 0.6864713\n","\tspeed: 0.2399s/iter; left time: 275.6148s\n","799it [03:11,  4.17it/s]\titers: 800, epoch: 10 | loss: 0.3291725\n","\tspeed: 0.2397s/iter; left time: 251.4941s\n","899it [03:35,  4.17it/s]\titers: 900, epoch: 10 | loss: 0.2912889\n","\tspeed: 0.2398s/iter; left time: 227.6040s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 10 | loss: 0.3867148\n","\tspeed: 0.2399s/iter; left time: 203.6479s\n","1099it [04:23,  4.18it/s]\titers: 1100, epoch: 10 | loss: 0.3354457\n","\tspeed: 0.2398s/iter; left time: 179.6137s\n","1199it [04:47,  4.17it/s]\titers: 1200, epoch: 10 | loss: 0.2826012\n","\tspeed: 0.2400s/iter; left time: 155.7282s\n","1299it [05:11,  4.18it/s]\titers: 1300, epoch: 10 | loss: 0.7343339\n","\tspeed: 0.2396s/iter; left time: 131.5166s\n","1399it [05:35,  4.18it/s]\titers: 1400, epoch: 10 | loss: 0.3111386\n","\tspeed: 0.2396s/iter; left time: 107.5877s\n","1499it [05:59,  4.17it/s]\titers: 1500, epoch: 10 | loss: 0.3062710\n","\tspeed: 0.2396s/iter; left time: 83.6210s\n","1599it [06:23,  4.17it/s]\titers: 1600, epoch: 10 | loss: 0.4971572\n","\tspeed: 0.2397s/iter; left time: 59.6787s\n","1699it [06:47,  4.19it/s]\titers: 1700, epoch: 10 | loss: 0.4277972\n","\tspeed: 0.2395s/iter; left time: 35.6808s\n","1799it [07:11,  4.17it/s]\titers: 1800, epoch: 10 | loss: 0.3471547\n","\tspeed: 0.2398s/iter; left time: 11.7523s\n","1848it [07:23,  4.17it/s]\n","Epoch: 10 cost time: 443.270870923996\n","609it [01:13,  8.26it/s]\n","609it [01:13,  8.34it/s]\n","Epoch: 10 | Train Loss: 0.4019577 Vali Loss: 0.7503120 Test Loss: 0.4195107 MAE Loss: 0.4259318\n","EarlyStopping counter: 3 out of 10\n","Updating learning rate to 7.812500000000005e-09\n","success delete checkpoints\n"]}]},{"cell_type":"code","source":["# 원본 prompt\n","\n","!python run_main.py \\\n","  --task_name long_term_forecast \\\n","  --is_training 1 \\\n","  --model_id ETTh1_text_original_10epochs \\\n","  --model TimeLLM \\\n","  --data ETTh1 \\\n","  --root_path ./dataset/ETT-small \\\n","  --data_path ETTh1.csv \\\n","  --llm_model LLAMA \\\n","  --train_epochs 10 \\\n","  --model_comment text_original_10epochs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"72ZohAe9N2Um","executionInfo":{"status":"ok","timestamp":1749789409941,"user_tz":-540,"elapsed":5963493,"user":{"displayName":"이정현","userId":"05894115669961020676"}},"outputId":"3c066115-5c94-4b2e-a7cf-3490009291c0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading checkpoint shards: 100% 2/2 [00:00<00:00,  2.01it/s]\n","[2025-06-13 02:57:32,290] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","[2025-06-13 02:57:32,546] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n","[2025-06-13 02:57:32,546] [INFO] [comm.py:637:init_distributed] cdb=None\n","[2025-06-13 02:57:32,546] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n","[2025-06-13 02:57:32,988] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.28.0.12, master_port=29500\n","[2025-06-13 02:57:32,988] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n","[2025-06-13 02:57:34,650] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n","[2025-06-13 02:57:34,651] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n","[2025-06-13 02:57:34,651] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n","[2025-06-13 02:57:34,652] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n","[2025-06-13 02:57:34,652] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n","[2025-06-13 02:57:34,652] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n","[2025-06-13 02:57:34,652] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n","[2025-06-13 02:57:34,652] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n","[2025-06-13 02:57:34,652] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n","[2025-06-13 02:57:34,652] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n","[2025-06-13 02:57:34,890] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n","[2025-06-13 02:57:34,890] [INFO] [utils.py:801:see_memory_usage] MA 2.71 GB         Max_MA 2.78 GB         CA 2.8 GB         Max_CA 3 GB \n","[2025-06-13 02:57:34,891] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.73 GB, percent = 4.5%\n","[2025-06-13 02:57:35,016] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n","[2025-06-13 02:57:35,016] [INFO] [utils.py:801:see_memory_usage] MA 2.71 GB         Max_MA 2.85 GB         CA 2.93 GB         Max_CA 3 GB \n","[2025-06-13 02:57:35,017] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.73 GB, percent = 4.5%\n","[2025-06-13 02:57:35,017] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n","[2025-06-13 02:57:35,129] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n","[2025-06-13 02:57:35,130] [INFO] [utils.py:801:see_memory_usage] MA 2.71 GB         Max_MA 2.71 GB         CA 2.93 GB         Max_CA 3 GB \n","[2025-06-13 02:57:35,130] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.73 GB, percent = 4.5%\n","[2025-06-13 02:57:35,131] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n","[2025-06-13 02:57:35,131] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n","[2025-06-13 02:57:35,131] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n","[2025-06-13 02:57:35,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[4.000000000000002e-06], mom=[(0.95, 0.999)]\n","[2025-06-13 02:57:35,131] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n","    \"partition_activations\": false, \n","    \"contiguous_memory_optimization\": false, \n","    \"cpu_checkpointing\": false, \n","    \"number_checkpoints\": null, \n","    \"synchronize_checkpoint_boundary\": false, \n","    \"profile\": false\n","}\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   amp_enabled .................. False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   amp_params ................... False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   autotuning_config ............ {\n","    \"enabled\": false, \n","    \"start_step\": null, \n","    \"end_step\": null, \n","    \"metric_path\": null, \n","    \"arg_mappings\": null, \n","    \"metric\": \"throughput\", \n","    \"model_info\": null, \n","    \"results_dir\": \"autotuning_results\", \n","    \"exps_dir\": \"autotuning_exps\", \n","    \"overwrite\": true, \n","    \"fast\": true, \n","    \"start_profile_step\": 3, \n","    \"end_profile_step\": 5, \n","    \"tuner_type\": \"gridsearch\", \n","    \"tuner_early_stopping\": 5, \n","    \"tuner_num_trials\": 50, \n","    \"model_info_path\": null, \n","    \"mp_size\": 1, \n","    \"max_train_batch_size\": null, \n","    \"min_train_batch_size\": 1, \n","    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n","    \"min_train_micro_batch_size_per_gpu\": 1, \n","    \"num_tuning_micro_batch_sizes\": 3\n","}\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x795ba3f843d0>\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   communication_data_type ...... None\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   disable_allgather ............ False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   dump_state ................... False\n","[2025-06-13 02:57:35,132] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n","    \"enabled\": false, \n","    \"recompute_fwd_factor\": 0.0, \n","    \"profile_step\": 1, \n","    \"module_depth\": -1, \n","    \"top_modules\": 1, \n","    \"detailed\": true, \n","    \"output_file\": null\n","}\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   global_rank .................. 0\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   nebula_config ................ {\n","    \"enabled\": false, \n","    \"persistent_storage_path\": null, \n","    \"persistent_time_interval\": 100, \n","    \"num_of_version_in_retention\": 2, \n","    \"enable_nebula_load\": true, \n","    \"load_path\": null\n","}\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   optimizer_name ............... None\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   optimizer_params ............. None\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   pld_enabled .................. False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   pld_params ................... False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   scheduler_name ............... None\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   scheduler_params ............. None\n","[2025-06-13 02:57:35,133] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   sparse_attention ............. None\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   train_batch_size ............. 32\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  32\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   world_size ................... 1\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   zero_enabled ................. True\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n","[2025-06-13 02:57:35,134] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n","[2025-06-13 02:57:35,134] [INFO] [config.py:986:print_user_config]   json = {\n","    \"bf16\": {\n","        \"enabled\": true, \n","        \"auto_cast\": true\n","    }, \n","    \"zero_optimization\": {\n","        \"stage\": 2, \n","        \"allgather_partitions\": true, \n","        \"allgather_bucket_size\": 2.000000e+08, \n","        \"overlap_comm\": true, \n","        \"reduce_scatter\": true, \n","        \"reduce_bucket_size\": 2.000000e+08, \n","        \"contiguous_gradients\": true, \n","        \"sub_group_size\": 1.000000e+09\n","    }, \n","    \"gradient_accumulation_steps\": 1, \n","    \"train_batch_size\": 32, \n","    \"train_micro_batch_size_per_gpu\": 32, \n","    \"steps_per_print\": inf, \n","    \"wall_clock_breakdown\": false, \n","    \"fp16\": {\n","        \"enabled\": false\n","    }, \n","    \"zero_allow_untested_optimizer\": true\n","}\n","99it [00:24,  4.20it/s]\titers: 100, epoch: 1 | loss: 0.4607160\n","\tspeed: 0.2753s/iter; left time: 5059.7684s\n","199it [00:48,  4.18it/s]\titers: 200, epoch: 1 | loss: 0.4986204\n","\tspeed: 0.2391s/iter; left time: 4371.1120s\n","299it [01:12,  4.19it/s]\titers: 300, epoch: 1 | loss: 0.5668359\n","\tspeed: 0.2389s/iter; left time: 4343.5349s\n","399it [01:36,  4.19it/s]\titers: 400, epoch: 1 | loss: 0.5791938\n","\tspeed: 0.2391s/iter; left time: 4323.7115s\n","499it [02:00,  4.17it/s]\titers: 500, epoch: 1 | loss: 0.5695208\n","\tspeed: 0.2395s/iter; left time: 4307.1936s\n","599it [02:24,  4.16it/s]\titers: 600, epoch: 1 | loss: 0.6604478\n","\tspeed: 0.2399s/iter; left time: 4289.0745s\n","699it [02:48,  4.16it/s]\titers: 700, epoch: 1 | loss: 0.4540353\n","\tspeed: 0.2395s/iter; left time: 4257.7879s\n","799it [03:11,  4.18it/s]\titers: 800, epoch: 1 | loss: 0.5036864\n","\tspeed: 0.2396s/iter; left time: 4235.6472s\n","899it [03:35,  4.18it/s]\titers: 900, epoch: 1 | loss: 0.4503397\n","\tspeed: 0.2395s/iter; left time: 4210.8408s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 1 | loss: 0.6007888\n","\tspeed: 0.2394s/iter; left time: 4185.7610s\n","1099it [04:23,  4.17it/s]\titers: 1100, epoch: 1 | loss: 0.6570219\n","\tspeed: 0.2397s/iter; left time: 4166.7952s\n","1199it [04:47,  4.16it/s]\titers: 1200, epoch: 1 | loss: 0.4582040\n","\tspeed: 0.2401s/iter; left time: 4148.9429s\n","1299it [05:11,  4.18it/s]\titers: 1300, epoch: 1 | loss: 0.6926413\n","\tspeed: 0.2401s/iter; left time: 4125.8067s\n","1399it [05:35,  4.16it/s]\titers: 1400, epoch: 1 | loss: 0.4587602\n","\tspeed: 0.2399s/iter; left time: 4097.8425s\n","1499it [05:59,  4.15it/s]\titers: 1500, epoch: 1 | loss: 0.4217348\n","\tspeed: 0.2400s/iter; left time: 4075.1714s\n","1599it [06:23,  4.18it/s]\titers: 1600, epoch: 1 | loss: 0.3296473\n","\tspeed: 0.2399s/iter; left time: 4050.4588s\n","1699it [06:47,  4.18it/s]\titers: 1700, epoch: 1 | loss: 0.7769405\n","\tspeed: 0.2398s/iter; left time: 4023.9290s\n","1799it [07:11,  4.15it/s]\titers: 1800, epoch: 1 | loss: 0.3610145\n","\tspeed: 0.2398s/iter; left time: 4000.5420s\n","1848it [07:23,  4.17it/s]\n","Epoch: 1 cost time: 443.58869671821594\n","609it [01:13,  8.24it/s]\n","609it [01:12,  8.35it/s]\n","Epoch: 1 | Train Loss: 0.5173959 Vali Loss: 0.8602856 Test Loss: 0.4936895 MAE Loss: 0.4672162\n","lr = 0.0000040000\n","Updating learning rate to 4.000000000000002e-06\n","99it [00:24,  4.18it/s]\titers: 100, epoch: 2 | loss: 0.3775672\n","\tspeed: 1.8970s/iter; left time: 31362.9909s\n","199it [00:48,  4.20it/s]\titers: 200, epoch: 2 | loss: 0.5634032\n","\tspeed: 0.2400s/iter; left time: 3944.5249s\n","299it [01:12,  4.18it/s]\titers: 300, epoch: 2 | loss: 0.3815935\n","\tspeed: 0.2391s/iter; left time: 3905.2119s\n","399it [01:35,  4.17it/s]\titers: 400, epoch: 2 | loss: 0.3601033\n","\tspeed: 0.2390s/iter; left time: 3880.0565s\n","499it [01:59,  4.18it/s]\titers: 500, epoch: 2 | loss: 0.3519591\n","\tspeed: 0.2391s/iter; left time: 3857.4239s\n","599it [02:23,  4.18it/s]\titers: 600, epoch: 2 | loss: 0.3977116\n","\tspeed: 0.2393s/iter; left time: 3836.4400s\n","699it [02:47,  4.19it/s]\titers: 700, epoch: 2 | loss: 0.4048805\n","\tspeed: 0.2397s/iter; left time: 3819.7141s\n","799it [03:11,  4.18it/s]\titers: 800, epoch: 2 | loss: 0.3210537\n","\tspeed: 0.2389s/iter; left time: 3782.0514s\n","899it [03:35,  4.16it/s]\titers: 900, epoch: 2 | loss: 0.4982674\n","\tspeed: 0.2396s/iter; left time: 3769.8811s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 2 | loss: 0.2409918\n","\tspeed: 0.2392s/iter; left time: 3739.5573s\n","1099it [04:23,  4.18it/s]\titers: 1100, epoch: 2 | loss: 0.3641656\n","\tspeed: 0.2392s/iter; left time: 3715.4884s\n","1199it [04:47,  4.18it/s]\titers: 1200, epoch: 2 | loss: 0.3164320\n","\tspeed: 0.2394s/iter; left time: 3694.8363s\n","1299it [05:11,  4.15it/s]\titers: 1300, epoch: 2 | loss: 0.6237082\n","\tspeed: 0.2400s/iter; left time: 3679.3487s\n","1399it [05:35,  4.18it/s]\titers: 1400, epoch: 2 | loss: 0.3386836\n","\tspeed: 0.2400s/iter; left time: 3655.5878s\n","1499it [05:59,  4.17it/s]\titers: 1500, epoch: 2 | loss: 0.4145009\n","\tspeed: 0.2393s/iter; left time: 3621.7624s\n","1599it [06:23,  4.19it/s]\titers: 1600, epoch: 2 | loss: 0.4012722\n","\tspeed: 0.2393s/iter; left time: 3597.2282s\n","1699it [06:47,  4.18it/s]\titers: 1700, epoch: 2 | loss: 0.3590620\n","\tspeed: 0.2394s/iter; left time: 3575.6366s\n","1799it [07:11,  4.19it/s]\titers: 1800, epoch: 2 | loss: 0.3423380\n","\tspeed: 0.2393s/iter; left time: 3549.1726s\n","1848it [07:22,  4.17it/s]\n","Epoch: 2 cost time: 442.8636565208435\n","609it [01:13,  8.26it/s]\n","609it [01:12,  8.34it/s]\n","Epoch: 2 | Train Loss: 0.4280328 Vali Loss: 0.7850058 Test Loss: 0.4430850 MAE Loss: 0.4404798\n","Updating learning rate to 2.000000000000001e-06\n","99it [00:24,  4.16it/s]\titers: 100, epoch: 3 | loss: 0.3891416\n","\tspeed: 1.9035s/iter; left time: 27953.0538s\n","199it [00:48,  4.18it/s]\titers: 200, epoch: 3 | loss: 0.3499447\n","\tspeed: 0.2395s/iter; left time: 3493.6762s\n","299it [01:12,  4.16it/s]\titers: 300, epoch: 3 | loss: 0.3941354\n","\tspeed: 0.2395s/iter; left time: 3468.5349s\n","399it [01:36,  4.17it/s]\titers: 400, epoch: 3 | loss: 0.4501319\n","\tspeed: 0.2392s/iter; left time: 3440.2331s\n","499it [02:00,  4.18it/s]\titers: 500, epoch: 3 | loss: 0.3683445\n","\tspeed: 0.2391s/iter; left time: 3415.7094s\n","599it [02:24,  4.17it/s]\titers: 600, epoch: 3 | loss: 0.3329226\n","\tspeed: 0.2396s/iter; left time: 3398.3101s\n","699it [02:47,  4.19it/s]\titers: 700, epoch: 3 | loss: 0.5560014\n","\tspeed: 0.2391s/iter; left time: 3368.4252s\n","799it [03:11,  4.17it/s]\titers: 800, epoch: 3 | loss: 0.3771634\n","\tspeed: 0.2392s/iter; left time: 3344.7873s\n","899it [03:35,  4.18it/s]\titers: 900, epoch: 3 | loss: 0.4386800\n","\tspeed: 0.2396s/iter; left time: 3326.1995s\n","999it [03:59,  4.20it/s]\titers: 1000, epoch: 3 | loss: 0.4386729\n","\tspeed: 0.2395s/iter; left time: 3300.9204s\n","1099it [04:23,  4.18it/s]\titers: 1100, epoch: 3 | loss: 0.3873808\n","\tspeed: 0.2395s/iter; left time: 3277.1592s\n","1199it [04:47,  4.21it/s]\titers: 1200, epoch: 3 | loss: 0.3655177\n","\tspeed: 0.2391s/iter; left time: 3247.9303s\n","1299it [05:11,  4.17it/s]\titers: 1300, epoch: 3 | loss: 0.4106690\n","\tspeed: 0.2396s/iter; left time: 3231.1464s\n","1399it [05:35,  4.18it/s]\titers: 1400, epoch: 3 | loss: 0.4409610\n","\tspeed: 0.2394s/iter; left time: 3204.6480s\n","1499it [05:59,  4.18it/s]\titers: 1500, epoch: 3 | loss: 0.2955779\n","\tspeed: 0.2393s/iter; left time: 3178.7745s\n","1599it [06:23,  4.19it/s]\titers: 1600, epoch: 3 | loss: 0.3044609\n","\tspeed: 0.2394s/iter; left time: 3156.6937s\n","1699it [06:47,  4.17it/s]\titers: 1700, epoch: 3 | loss: 0.4504944\n","\tspeed: 0.2394s/iter; left time: 3133.1808s\n","1799it [07:11,  4.18it/s]\titers: 1800, epoch: 3 | loss: 0.3246818\n","\tspeed: 0.2394s/iter; left time: 3108.6485s\n","1848it [07:23,  4.17it/s]\n","Epoch: 3 cost time: 443.07316493988037\n","609it [01:13,  8.26it/s]\n","609it [01:13,  8.34it/s]\n","Epoch: 3 | Train Loss: 0.4111520 Vali Loss: 0.7607160 Test Loss: 0.4290476 MAE Loss: 0.4322067\n","Updating learning rate to 1.0000000000000006e-06\n","99it [00:24,  4.18it/s]\titers: 100, epoch: 4 | loss: 0.4700673\n","\tspeed: 1.8984s/iter; left time: 24369.4017s\n","199it [00:48,  4.17it/s]\titers: 200, epoch: 4 | loss: 0.4909724\n","\tspeed: 0.2400s/iter; left time: 3056.5199s\n","299it [01:12,  4.18it/s]\titers: 300, epoch: 4 | loss: 0.4480528\n","\tspeed: 0.2396s/iter; left time: 3027.4907s\n","399it [01:35,  4.18it/s]\titers: 400, epoch: 4 | loss: 0.4278170\n","\tspeed: 0.2393s/iter; left time: 3000.1198s\n","499it [01:59,  4.18it/s]\titers: 500, epoch: 4 | loss: 0.2407879\n","\tspeed: 0.2397s/iter; left time: 2981.4872s\n","599it [02:23,  4.18it/s]\titers: 600, epoch: 4 | loss: 0.4746686\n","\tspeed: 0.2400s/iter; left time: 2961.4453s\n","699it [02:47,  4.19it/s]\titers: 700, epoch: 4 | loss: 0.4163427\n","\tspeed: 0.2394s/iter; left time: 2929.4978s\n","799it [03:11,  4.24it/s]\titers: 800, epoch: 4 | loss: 0.3762532\n","\tspeed: 0.2393s/iter; left time: 2904.8568s\n","899it [03:35,  4.18it/s]\titers: 900, epoch: 4 | loss: 0.3398970\n","\tspeed: 0.2396s/iter; left time: 2884.5814s\n","999it [03:59,  4.15it/s]\titers: 1000, epoch: 4 | loss: 0.5036775\n","\tspeed: 0.2400s/iter; left time: 2865.2955s\n","1099it [04:23,  4.17it/s]\titers: 1100, epoch: 4 | loss: 0.4481446\n","\tspeed: 0.2397s/iter; left time: 2837.1287s\n","1199it [04:47,  4.19it/s]\titers: 1200, epoch: 4 | loss: 0.3179393\n","\tspeed: 0.2394s/iter; left time: 2809.8398s\n","1299it [05:11,  4.18it/s]\titers: 1300, epoch: 4 | loss: 0.4483325\n","\tspeed: 0.2395s/iter; left time: 2787.4893s\n","1399it [05:35,  4.17it/s]\titers: 1400, epoch: 4 | loss: 0.3347028\n","\tspeed: 0.2398s/iter; left time: 2766.6673s\n","1499it [05:59,  4.17it/s]\titers: 1500, epoch: 4 | loss: 0.3513381\n","\tspeed: 0.2393s/iter; left time: 2736.8724s\n","1599it [06:23,  4.19it/s]\titers: 1600, epoch: 4 | loss: 0.4322861\n","\tspeed: 0.2396s/iter; left time: 2716.7312s\n","1699it [06:47,  4.17it/s]\titers: 1700, epoch: 4 | loss: 0.5293696\n","\tspeed: 0.2394s/iter; left time: 2690.1916s\n","1799it [07:11,  4.15it/s]\titers: 1800, epoch: 4 | loss: 0.3486186\n","\tspeed: 0.2399s/iter; left time: 2672.2400s\n","1848it [07:23,  4.17it/s]\n","Epoch: 4 cost time: 443.23645401000977\n","609it [01:13,  8.24it/s]\n","609it [01:12,  8.35it/s]\n","Epoch: 4 | Train Loss: 0.4060286 Vali Loss: 0.7559492 Test Loss: 0.4235308 MAE Loss: 0.4283927\n","Updating learning rate to 5.000000000000003e-07\n","99it [00:24,  4.15it/s]\titers: 100, epoch: 5 | loss: 0.3364123\n","\tspeed: 1.9012s/iter; left time: 20892.7764s\n","199it [00:48,  4.17it/s]\titers: 200, epoch: 5 | loss: 0.3573887\n","\tspeed: 0.2397s/iter; left time: 2610.5258s\n","299it [01:11,  4.18it/s]\titers: 300, epoch: 5 | loss: 0.3886049\n","\tspeed: 0.2392s/iter; left time: 2580.6504s\n","399it [01:35,  4.17it/s]\titers: 400, epoch: 5 | loss: 0.3247955\n","\tspeed: 0.2391s/iter; left time: 2555.8338s\n","499it [01:59,  4.18it/s]\titers: 500, epoch: 5 | loss: 0.4388586\n","\tspeed: 0.2391s/iter; left time: 2531.5760s\n","599it [02:23,  4.17it/s]\titers: 600, epoch: 5 | loss: 0.5525755\n","\tspeed: 0.2402s/iter; left time: 2519.1413s\n","699it [02:47,  4.19it/s]\titers: 700, epoch: 5 | loss: 0.3218255\n","\tspeed: 0.2396s/iter; left time: 2489.6998s\n","799it [03:11,  4.17it/s]\titers: 800, epoch: 5 | loss: 0.4795036\n","\tspeed: 0.2396s/iter; left time: 2465.5253s\n","899it [03:35,  4.17it/s]\titers: 900, epoch: 5 | loss: 0.4464006\n","\tspeed: 0.2394s/iter; left time: 2438.9395s\n","999it [03:59,  4.14it/s]\titers: 1000, epoch: 5 | loss: 0.3842030\n","\tspeed: 0.2397s/iter; left time: 2418.4875s\n","1099it [04:23,  4.17it/s]\titers: 1100, epoch: 5 | loss: 0.3384363\n","\tspeed: 0.2400s/iter; left time: 2396.9239s\n","1199it [04:47,  4.19it/s]\titers: 1200, epoch: 5 | loss: 0.2976488\n","\tspeed: 0.2395s/iter; left time: 2368.7081s\n","1299it [05:11,  4.15it/s]\titers: 1300, epoch: 5 | loss: 0.2966227\n","\tspeed: 0.2401s/iter; left time: 2350.6006s\n","1399it [05:35,  4.19it/s]\titers: 1400, epoch: 5 | loss: 0.4112066\n","\tspeed: 0.2400s/iter; left time: 2325.2481s\n","1499it [05:59,  4.17it/s]\titers: 1500, epoch: 5 | loss: 0.4008974\n","\tspeed: 0.2395s/iter; left time: 2296.1687s\n","1599it [06:23,  4.18it/s]\titers: 1600, epoch: 5 | loss: 0.4779234\n","\tspeed: 0.2398s/iter; left time: 2275.5557s\n","1699it [06:47,  4.16it/s]\titers: 1700, epoch: 5 | loss: 0.3580923\n","\tspeed: 0.2396s/iter; left time: 2249.9043s\n","1799it [07:11,  4.18it/s]\titers: 1800, epoch: 5 | loss: 0.2727641\n","\tspeed: 0.2394s/iter; left time: 2224.1661s\n","1848it [07:23,  4.17it/s]\n","Epoch: 5 cost time: 443.2528464794159\n","609it [01:13,  8.24it/s]\n","609it [01:12,  8.35it/s]\n","Epoch: 5 | Train Loss: 0.4035597 Vali Loss: 0.7528719 Test Loss: 0.4213863 MAE Loss: 0.4270966\n","Updating learning rate to 2.5000000000000015e-07\n","99it [00:24,  4.16it/s]\titers: 100, epoch: 6 | loss: 0.5218090\n","\tspeed: 1.9042s/iter; left time: 17405.9881s\n","199it [00:48,  4.17it/s]\titers: 200, epoch: 6 | loss: 0.4802207\n","\tspeed: 0.2402s/iter; left time: 2171.4810s\n","299it [01:12,  4.16it/s]\titers: 300, epoch: 6 | loss: 0.4043599\n","\tspeed: 0.2396s/iter; left time: 2142.6575s\n","399it [01:36,  4.16it/s]\titers: 400, epoch: 6 | loss: 0.5030083\n","\tspeed: 0.2401s/iter; left time: 2122.5710s\n","499it [02:00,  4.15it/s]\titers: 500, epoch: 6 | loss: 0.4108268\n","\tspeed: 0.2400s/iter; left time: 2098.0611s\n","599it [02:24,  4.17it/s]\titers: 600, epoch: 6 | loss: 0.3640161\n","\tspeed: 0.2395s/iter; left time: 2069.4426s\n","699it [02:47,  4.17it/s]\titers: 700, epoch: 6 | loss: 0.6858840\n","\tspeed: 0.2396s/iter; left time: 2046.8197s\n","799it [03:11,  4.17it/s]\titers: 800, epoch: 6 | loss: 0.5483286\n","\tspeed: 0.2395s/iter; left time: 2021.9264s\n","899it [03:35,  4.19it/s]\titers: 900, epoch: 6 | loss: 0.6839887\n","\tspeed: 0.2396s/iter; left time: 1998.1227s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 6 | loss: 0.3286130\n","\tspeed: 0.2397s/iter; left time: 1975.0087s\n","1099it [04:23,  4.18it/s]\titers: 1100, epoch: 6 | loss: 0.3492576\n","\tspeed: 0.2396s/iter; left time: 1950.8279s\n","1199it [04:47,  4.16it/s]\titers: 1200, epoch: 6 | loss: 0.3089710\n","\tspeed: 0.2400s/iter; left time: 1929.6445s\n","1299it [05:11,  4.18it/s]\titers: 1300, epoch: 6 | loss: 0.4883668\n","\tspeed: 0.2394s/iter; left time: 1901.3098s\n","1399it [05:35,  4.19it/s]\titers: 1400, epoch: 6 | loss: 0.3555909\n","\tspeed: 0.2396s/iter; left time: 1878.7278s\n","1499it [05:59,  4.19it/s]\titers: 1500, epoch: 6 | loss: 0.2875938\n","\tspeed: 0.2393s/iter; left time: 1852.4888s\n","1599it [06:23,  4.19it/s]\titers: 1600, epoch: 6 | loss: 0.4288565\n","\tspeed: 0.2395s/iter; left time: 1829.7169s\n","1699it [06:47,  4.16it/s]\titers: 1700, epoch: 6 | loss: 0.3467430\n","\tspeed: 0.2396s/iter; left time: 1807.1923s\n","1799it [07:11,  4.19it/s]\titers: 1800, epoch: 6 | loss: 0.3245171\n","\tspeed: 0.2400s/iter; left time: 1786.1425s\n","1848it [07:23,  4.17it/s]\n","Epoch: 6 cost time: 443.36744356155396\n","609it [01:13,  8.25it/s]\n","609it [01:13,  8.34it/s]\n","Epoch: 6 | Train Loss: 0.4024705 Vali Loss: 0.7517270 Test Loss: 0.4203273 MAE Loss: 0.4263856\n","Updating learning rate to 1.2500000000000007e-07\n","99it [00:24,  4.16it/s]\titers: 100, epoch: 7 | loss: 0.3999298\n","\tspeed: 1.9083s/iter; left time: 13916.8927s\n","199it [00:47,  4.16it/s]\titers: 200, epoch: 7 | loss: 0.5049080\n","\tspeed: 0.2390s/iter; left time: 1718.8558s\n","299it [01:11,  4.16it/s]\titers: 300, epoch: 7 | loss: 0.4720000\n","\tspeed: 0.2393s/iter; left time: 1697.0980s\n","399it [01:35,  4.18it/s]\titers: 400, epoch: 7 | loss: 0.5681987\n","\tspeed: 0.2395s/iter; left time: 1674.9113s\n","499it [01:59,  4.19it/s]\titers: 500, epoch: 7 | loss: 0.5480515\n","\tspeed: 0.2393s/iter; left time: 1649.4945s\n","599it [02:23,  4.18it/s]\titers: 600, epoch: 7 | loss: 0.6366967\n","\tspeed: 0.2395s/iter; left time: 1626.8812s\n","699it [02:47,  4.19it/s]\titers: 700, epoch: 7 | loss: 0.3261909\n","\tspeed: 0.2394s/iter; left time: 1602.1901s\n","799it [03:11,  4.15it/s]\titers: 800, epoch: 7 | loss: 0.4220667\n","\tspeed: 0.2393s/iter; left time: 1578.0222s\n","899it [03:35,  4.17it/s]\titers: 900, epoch: 7 | loss: 0.2631044\n","\tspeed: 0.2398s/iter; left time: 1556.8178s\n","999it [03:59,  4.19it/s]\titers: 1000, epoch: 7 | loss: 0.5269231\n","\tspeed: 0.2399s/iter; left time: 1533.9170s\n","1099it [04:23,  4.17it/s]\titers: 1100, epoch: 7 | loss: 0.3119529\n","\tspeed: 0.2400s/iter; left time: 1510.1303s\n","1199it [04:47,  4.19it/s]\titers: 1200, epoch: 7 | loss: 0.3650596\n","\tspeed: 0.2397s/iter; left time: 1484.3319s\n","1299it [05:11,  4.16it/s]\titers: 1300, epoch: 7 | loss: 0.4471049\n","\tspeed: 0.2399s/iter; left time: 1461.4367s\n","1399it [05:35,  4.17it/s]\titers: 1400, epoch: 7 | loss: 0.2670789\n","\tspeed: 0.2397s/iter; left time: 1436.2696s\n","1499it [05:59,  4.19it/s]\titers: 1500, epoch: 7 | loss: 0.3834206\n","\tspeed: 0.2399s/iter; left time: 1413.9190s\n","1599it [06:23,  4.17it/s]\titers: 1600, epoch: 7 | loss: 0.3684320\n","\tspeed: 0.2398s/iter; left time: 1388.8885s\n","1699it [06:47,  4.18it/s]\titers: 1700, epoch: 7 | loss: 0.3612802\n","\tspeed: 0.2397s/iter; left time: 1364.6304s\n","1799it [07:11,  4.17it/s]\titers: 1800, epoch: 7 | loss: 0.6437570\n","\tspeed: 0.2394s/iter; left time: 1338.7275s\n","1848it [07:23,  4.17it/s]\n","Epoch: 7 cost time: 443.17586636543274\n","609it [01:13,  8.24it/s]\n","609it [01:12,  8.36it/s]\n","Epoch: 7 | Train Loss: 0.4020110 Vali Loss: 0.7494386 Test Loss: 0.4194884 MAE Loss: 0.4258878\n","Updating learning rate to 6.250000000000004e-08\n","99it [00:24,  4.18it/s]\titers: 100, epoch: 8 | loss: 0.3981338\n","\tspeed: 1.9145s/iter; left time: 10424.6779s\n","199it [00:47,  4.19it/s]\titers: 200, epoch: 8 | loss: 0.3538156\n","\tspeed: 0.2390s/iter; left time: 1277.3792s\n","299it [01:11,  4.19it/s]\titers: 300, epoch: 8 | loss: 0.3406813\n","\tspeed: 0.2395s/iter; left time: 1256.1703s\n","399it [01:35,  4.19it/s]\titers: 400, epoch: 8 | loss: 0.3987370\n","\tspeed: 0.2396s/iter; left time: 1232.6200s\n","499it [01:59,  4.17it/s]\titers: 500, epoch: 8 | loss: 0.4321980\n","\tspeed: 0.2397s/iter; left time: 1209.3709s\n","599it [02:23,  4.15it/s]\titers: 600, epoch: 8 | loss: 0.3124383\n","\tspeed: 0.2404s/iter; left time: 1188.9720s\n","699it [02:47,  4.16it/s]\titers: 700, epoch: 8 | loss: 0.5284512\n","\tspeed: 0.2400s/iter; left time: 1162.6547s\n","799it [03:11,  4.17it/s]\titers: 800, epoch: 8 | loss: 0.4026185\n","\tspeed: 0.2395s/iter; left time: 1136.6099s\n","899it [03:35,  4.17it/s]\titers: 900, epoch: 8 | loss: 0.3809780\n","\tspeed: 0.2393s/iter; left time: 1111.3644s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 8 | loss: 0.2814762\n","\tspeed: 0.2396s/iter; left time: 1088.9094s\n","1099it [04:23,  4.17it/s]\titers: 1100, epoch: 8 | loss: 0.5009845\n","\tspeed: 0.2403s/iter; left time: 1068.1526s\n","1199it [04:47,  4.19it/s]\titers: 1200, epoch: 8 | loss: 0.4171024\n","\tspeed: 0.2396s/iter; left time: 1040.9088s\n","1299it [05:11,  4.18it/s]\titers: 1300, epoch: 8 | loss: 0.6347873\n","\tspeed: 0.2399s/iter; left time: 1018.3412s\n","1399it [05:35,  4.17it/s]\titers: 1400, epoch: 8 | loss: 0.4111916\n","\tspeed: 0.2394s/iter; left time: 992.4297s\n","1499it [05:59,  4.18it/s]\titers: 1500, epoch: 8 | loss: 0.4054230\n","\tspeed: 0.2397s/iter; left time: 969.6800s\n","1599it [06:23,  4.19it/s]\titers: 1600, epoch: 8 | loss: 0.4504849\n","\tspeed: 0.2395s/iter; left time: 944.7487s\n","1699it [06:47,  4.16it/s]\titers: 1700, epoch: 8 | loss: 0.4700018\n","\tspeed: 0.2399s/iter; left time: 922.5083s\n","1799it [07:11,  4.15it/s]\titers: 1800, epoch: 8 | loss: 0.5310990\n","\tspeed: 0.2398s/iter; left time: 898.1950s\n","1848it [07:23,  4.17it/s]\n","Epoch: 8 cost time: 443.33317589759827\n","609it [01:13,  8.27it/s]\n","609it [01:12,  8.35it/s]\n","Epoch: 8 | Train Loss: 0.4019276 Vali Loss: 0.7502560 Test Loss: 0.4194101 MAE Loss: 0.4258375\n","EarlyStopping counter: 1 out of 10\n","Updating learning rate to 3.125000000000002e-08\n","99it [00:24,  4.17it/s]\titers: 100, epoch: 9 | loss: 0.3028636\n","\tspeed: 1.8236s/iter; left time: 6559.4064s\n","199it [00:47,  4.16it/s]\titers: 200, epoch: 9 | loss: 0.5697665\n","\tspeed: 0.2395s/iter; left time: 837.6313s\n","299it [01:11,  4.17it/s]\titers: 300, epoch: 9 | loss: 0.4642414\n","\tspeed: 0.2396s/iter; left time: 813.8312s\n","399it [01:35,  4.18it/s]\titers: 400, epoch: 9 | loss: 0.3843475\n","\tspeed: 0.2396s/iter; left time: 789.9272s\n","499it [01:59,  4.17it/s]\titers: 500, epoch: 9 | loss: 0.3549610\n","\tspeed: 0.2397s/iter; left time: 766.4003s\n","599it [02:23,  4.19it/s]\titers: 600, epoch: 9 | loss: 0.4966097\n","\tspeed: 0.2392s/iter; left time: 740.6529s\n","699it [02:47,  4.17it/s]\titers: 700, epoch: 9 | loss: 0.5209507\n","\tspeed: 0.2399s/iter; left time: 718.8631s\n","799it [03:11,  4.18it/s]\titers: 800, epoch: 9 | loss: 0.4400246\n","\tspeed: 0.2395s/iter; left time: 693.8285s\n","899it [03:35,  4.19it/s]\titers: 900, epoch: 9 | loss: 0.2752059\n","\tspeed: 0.2394s/iter; left time: 669.7321s\n","999it [03:59,  4.18it/s]\titers: 1000, epoch: 9 | loss: 0.5482849\n","\tspeed: 0.2394s/iter; left time: 645.5816s\n","1099it [04:23,  4.17it/s]\titers: 1100, epoch: 9 | loss: 0.3339377\n","\tspeed: 0.2394s/iter; left time: 621.6573s\n","1199it [04:47,  4.18it/s]\titers: 1200, epoch: 9 | loss: 0.3378697\n","\tspeed: 0.2395s/iter; left time: 598.0944s\n","1299it [05:11,  4.17it/s]\titers: 1300, epoch: 9 | loss: 0.3303331\n","\tspeed: 0.2394s/iter; left time: 573.8717s\n","1399it [05:35,  4.18it/s]\titers: 1400, epoch: 9 | loss: 0.3329524\n","\tspeed: 0.2397s/iter; left time: 550.5978s\n","1499it [05:59,  4.18it/s]\titers: 1500, epoch: 9 | loss: 0.4339311\n","\tspeed: 0.2394s/iter; left time: 525.9386s\n","1599it [06:23,  4.17it/s]\titers: 1600, epoch: 9 | loss: 0.4308795\n","\tspeed: 0.2395s/iter; left time: 502.2186s\n","1699it [06:47,  4.18it/s]\titers: 1700, epoch: 9 | loss: 0.3403246\n","\tspeed: 0.2397s/iter; left time: 478.6057s\n","1799it [07:11,  4.17it/s]\titers: 1800, epoch: 9 | loss: 0.4222464\n","\tspeed: 0.2396s/iter; left time: 454.5786s\n","1848it [07:22,  4.17it/s]\n","Epoch: 9 cost time: 442.99441623687744\n","609it [01:13,  8.24it/s]\n","609it [01:12,  8.35it/s]\n","Epoch: 9 | Train Loss: 0.4018320 Vali Loss: 0.7495004 Test Loss: 0.4192857 MAE Loss: 0.4257513\n","EarlyStopping counter: 2 out of 10\n","Updating learning rate to 1.562500000000001e-08\n","99it [00:24,  4.18it/s]\titers: 100, epoch: 10 | loss: 0.2649153\n","\tspeed: 1.8263s/iter; left time: 3194.2437s\n","199it [00:47,  4.17it/s]\titers: 200, epoch: 10 | loss: 0.4926116\n","\tspeed: 0.2394s/iter; left time: 394.8431s\n","299it [01:11,  4.19it/s]\titers: 300, epoch: 10 | loss: 0.5318952\n","\tspeed: 0.2395s/iter; left time: 371.0124s\n","399it [01:35,  4.16it/s]\titers: 400, epoch: 10 | loss: 0.4316034\n","\tspeed: 0.2398s/iter; left time: 347.4112s\n","499it [01:59,  4.16it/s]\titers: 500, epoch: 10 | loss: 0.4158139\n","\tspeed: 0.2395s/iter; left time: 323.1170s\n","599it [02:23,  4.17it/s]\titers: 600, epoch: 10 | loss: 0.2973584\n","\tspeed: 0.2393s/iter; left time: 298.8745s\n","699it [02:47,  4.18it/s]\titers: 700, epoch: 10 | loss: 0.6877204\n","\tspeed: 0.2400s/iter; left time: 275.7436s\n","799it [03:11,  4.16it/s]\titers: 800, epoch: 10 | loss: 0.3295881\n","\tspeed: 0.2394s/iter; left time: 251.1164s\n","899it [03:35,  4.18it/s]\titers: 900, epoch: 10 | loss: 0.2915771\n","\tspeed: 0.2395s/iter; left time: 227.2580s\n","999it [03:59,  4.19it/s]\titers: 1000, epoch: 10 | loss: 0.3867836\n","\tspeed: 0.2394s/iter; left time: 203.2447s\n","1099it [04:23,  4.18it/s]\titers: 1100, epoch: 10 | loss: 0.3348719\n","\tspeed: 0.2398s/iter; left time: 179.6360s\n","1199it [04:47,  4.17it/s]\titers: 1200, epoch: 10 | loss: 0.2833432\n","\tspeed: 0.2396s/iter; left time: 155.5147s\n","1299it [05:11,  4.17it/s]\titers: 1300, epoch: 10 | loss: 0.7334654\n","\tspeed: 0.2395s/iter; left time: 131.5073s\n","1399it [05:35,  4.17it/s]\titers: 1400, epoch: 10 | loss: 0.3106686\n","\tspeed: 0.2399s/iter; left time: 107.7098s\n","1499it [05:59,  4.20it/s]\titers: 1500, epoch: 10 | loss: 0.3069558\n","\tspeed: 0.2396s/iter; left time: 83.6228s\n","1599it [06:23,  4.18it/s]\titers: 1600, epoch: 10 | loss: 0.4968265\n","\tspeed: 0.2395s/iter; left time: 59.6284s\n","1699it [06:47,  4.19it/s]\titers: 1700, epoch: 10 | loss: 0.4266654\n","\tspeed: 0.2395s/iter; left time: 35.6900s\n","1799it [07:11,  4.17it/s]\titers: 1800, epoch: 10 | loss: 0.3473840\n","\tspeed: 0.2394s/iter; left time: 11.7298s\n","1848it [07:23,  4.17it/s]\n","Epoch: 10 cost time: 443.0823712348938\n","609it [01:13,  8.25it/s]\n","609it [01:12,  8.34it/s]\n","Epoch: 10 | Train Loss: 0.4018184 Vali Loss: 0.7501418 Test Loss: 0.4192130 MAE Loss: 0.4256890\n","EarlyStopping counter: 3 out of 10\n","Updating learning rate to 7.812500000000005e-09\n","success delete checkpoints\n"]}]}]}